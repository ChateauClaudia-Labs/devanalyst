{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import os, sys, io\n",
    "from io import StringIO\n",
    "import pkg_resources\n",
    "import importlib\n",
    "from colorama import Fore, Back, Style \n",
    "import ast\n",
    "from datetime import datetime #, date, timedelta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Link Code to be Tested</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Reloads the devanalyst modules used in this notebook. Useful to refresh after code changes in the devanalyst modules\n",
    "def reload():\n",
    "    global timecard\n",
    "    timecard = importlib.import_module('devanalyst.simulation.GenerateTimecards')\n",
    "    timecard = importlib.reload(timecard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\GenerateTimecards.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\GenerateTimecards.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Re-run this each time the source code changes, so that we have the latest version\n",
    "reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.GenerateTimecards as timecard\n",
    "from devanalyst.simulation.GenerateTimecards import UserStoriesRepo, UserStory, UserStoryStatus, \\\n",
    "Ticket, TicketsRepo, ScrumTeam, ScrumTeamsRepo, IdCounter, WorkAssignments, \\\n",
    "ModelsConfig, DefaultCostModel, GreedyAllocationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Common Context Across Tests</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Immutable Globals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#globals used in this test. After they are initialized in 'loadTestResources()', they don't change.\n",
    "PM_DF = None\n",
    "DEV_DF = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mutable Globals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Global variable used to have a single counter for user stories id as they get generated in multiple calls. \n",
    "# Mutable state\n",
    "NEXT_USER_STORY_ID = None\n",
    "\n",
    "# Dictioaries of test results. Each key is the string name of a test and the value is the output of that test,\n",
    "# expected or actual\n",
    "EXPECTED = {}\n",
    "ACTUAL = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Utilities</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def initTestData(developers_df, productManagers_df, releaseDuration, sprintDuration, modelsConfig):\n",
    "# Test the code for creating user stories by constructing them and arranging them in a dataframe to display and evidence\n",
    "# visually that the code works as it should\n",
    "\n",
    "    global NEXT_USER_STORY_ID\n",
    "    storiesRepo = UserStoriesRepo([])\n",
    "    teamsRepo = ScrumTeamsRepo([])\n",
    "    ticketsRepo = TicketsRepo([])\n",
    "    NEXT_USER_STORY_ID = IdCounter()\n",
    "    #cols = ['User Story Id','Scrum Team', 'Product Manager', 'Developer', 'Estimate',]\n",
    "    userStoryId_vals = []\n",
    "    scrumTeam_vals = []\n",
    "    developer_vals = []\n",
    "    productManager_vals = []\n",
    "    estimate_vals = []\n",
    "    \n",
    "    teams_df = timecard.createTeamsDF(developers_df, productManagers_df)\n",
    "    \n",
    "    for team in teams_df['Scrum Team']:\n",
    "        stories, backlog = timecard.createUserStoryBacklog(team, releaseDuration, sprintDuration, \n",
    "                                                           NEXT_USER_STORY_ID, modelsConfig)\n",
    "        storiesRepo.stories.extend(stories)\n",
    "        teamsRepo.teams.append(team)\n",
    "        for story in stories:\n",
    "            scrumTeam_vals.append(team.teamId)\n",
    "            userStoryId_vals.append(story.userStoryId)\n",
    "            developer_vals.append(story.developer)\n",
    "            productManager_vals.append(story.productManager)\n",
    "            estimate_vals.append(story.originalEstimate)\n",
    "        team.backlog = backlog\n",
    "    stories_dict = {'User Story Id': userStoryId_vals, 'Scrum Team': scrumTeam_vals, 'Product Manager':productManager_vals, \\\n",
    "                'Developer': developer_vals, 'Estimate': estimate_vals}\n",
    "    return teams_df, pd.DataFrame(stories_dict), teamsRepo, storiesRepo, ticketsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loadTestResources():\n",
    "    global PM_DF\n",
    "    global DEV_DF\n",
    "    resource_path = '/'.join(('Resources', 'Simulation', 'Team.xlsx'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    PM_DF = pd.read_excel(fullpath, 'PMs')\n",
    "    DEV_DF = pd.read_excel(fullpath, 'Dev')\n",
    "\n",
    "loadTestResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def createExpectedOutput(expected_df, testName):\n",
    "    resource_path = '/'.join(('Resources', 'Tests','Simulation', testName + '_EXPECTED.csv'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    return expected_df.to_csv(fullpath)\n",
    "\n",
    "def loadExpectedOutput(testName, literal_cols=[]):\n",
    "    resource_path = '/'.join(('Resources', 'Tests','Simulation', testName + '_EXPECTED.csv'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    df = pd.read_csv(fullpath)\n",
    "    \n",
    "    spurious_col = 'Unnamed: 0'\n",
    "    if spurious_col in df.columns:\n",
    "        df = df.drop([spurious_col], axis = 'columns') # Remove spurious index column, if any\n",
    "    \n",
    "    # Now for each column in list_cols, we have to treat it like it is a list so replace the contents of that\n",
    "    # column with a parsing of the string value to to a list value\n",
    "    for col in literal_cols:\n",
    "        df[col] = df[col].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def matches(actual, expected):\n",
    "    if isinstance(actual, str):\n",
    "        return actual==expected\n",
    "    else:\n",
    "        return actual.equals(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def testOK(testname):\n",
    "    return matches(ACTUAL[testname], EXPECTED[testname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_uss</h1>\n",
    "<p>Test User Story Status</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     3,
     10,
     16,
     23
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "# Helper methods to the test\n",
    "def format_ticket(ticket):\n",
    "    return ('Ticket:' + ticket.ticketId \\\n",
    "            + ',\\n\\t\\t storyId=' + ticket.userStoryId \\\n",
    "            + ',\\n\\t\\t costToFix=' + str(ticket.costToFix) \\\n",
    "            + ',\\n\\t\\t effortToDate=' + str(ticket.effortToDate) \\\n",
    "            + ',\\n\\t\\t percentAchieved=' + str(ticket.percentAchieved))\n",
    "                \n",
    "def format_tickets(tickets):\n",
    "    output = ''\n",
    "    for ticket in tickets:\n",
    "        output = output + '\\n\\t\\t{' + format_ticket(ticket) + '}'\n",
    "    return output\n",
    "            \n",
    "def format_uss(uss):\n",
    "    return ('\\n *** USS:' + uss.userStoryId \\\n",
    "            + '\\n\\t achieved=' + str(uss.percentAchieved) \\\n",
    "            + ',\\n\\t planned=' + str(uss.planned) \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(uss.sprintPlanned) \\\n",
    "            + ',\\n\\t tickets=' + format_tickets(uss.pendingTickets))\n",
    "\n",
    "def format_item(item, item_label, sprint, timeInSprint):\n",
    "    return ('\\n *** ' + item_label + ' at ' + timeInSprint + ' of sprint' + str(sprint) + ': ' \\\n",
    "            + '\\n\\t userStoryId=' + str(item.userStoryId) \\\n",
    "            + ',\\n\\t taskType=' + str(item.taskType) \\\n",
    "            + ',\\n\\t ticketId=' + str(item.ticketId) \\\n",
    "            + ',\\n\\t estimate=' + '{0:.2f}'.format(item.estimate) \\\n",
    "            + ',\\n\\t percentAchieved=' + str(item.percentAchieved)  \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(item.sprintPlanned))\n",
    "\n",
    "# Test logic\n",
    "def test_uss():\n",
    "    output = '' \n",
    "    repo = UserStoriesRepo([UserStory('Story A', 25, 'Joe Developer', 'Amy PM'), \\\n",
    "                            UserStory('Story B', 17, 'Alex Developer', 'Kate PM')])\n",
    "    uss = UserStoryStatus('Story B', 0.0)\n",
    "    uss.planned = True\n",
    "    uss.sprintPlanned = 1\n",
    "    output = output + (format_uss(uss))\n",
    "    item = uss.generateWorkItems(repo)[0]\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'start'))\n",
    "    item.percentAchieved = 0.7\n",
    "    newTickets = [Ticket('Bug 100','Story B', 4), Ticket('Bug 101','Story B', 1.5)]\n",
    "    bugRepo = TicketsRepo(newTickets)\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'end'))\n",
    "    uss.updateStatus([item], newTickets, bugRepo)\n",
    "    uss.sprintPlanned = 2\n",
    "    output = output + (format_uss(uss))\n",
    "\n",
    "    items = uss.generateWorkItems(repo)\n",
    "    item=items[0]\n",
    "    output = output + (format_item(item, 'Item#1', 2, 'start'))\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'start'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'start'))\n",
    "    items[0].percentAchieved = 0.9\n",
    "    items[1].percentAchieved = 1.0\n",
    "    items[2].percentAchieved = 0.5\n",
    "    item=items[0]\n",
    "    item=items[0]\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'end'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'end'))\n",
    "    uss.updateStatus(items, [], bugRepo)\n",
    "    output = output + (format_uss(uss))\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_uss_ACTUAL = test_uss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to print a string output one can copy and paste into test_uss_EXPECTED\n",
    "#test_uss_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_uss_EXPECTED = '\\n *** USS:Story B\\n\\t achieved=0.0,\\n\\t planned=True,\\n\\t sprintPlanned=1,\\n\\t tickets=\\n *** Item#1 at start of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=1\\n *** Item#1 at end of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.7,\\n\\t sprintPlanned=1\\n *** USS:Story B\\n\\t achieved=0.7,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 100,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=4,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n *** Item#1 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=5.10,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#3 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=1.0,\\n\\t sprintPlanned=2\\n *** Item#3 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.5,\\n\\t sprintPlanned=2\\n *** USS:Story B\\n\\t achieved=0.97,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=2.0,\\n\\t\\t percentAchieved=0.5}'\n",
    "\n",
    "EXPECTED['uss'] = test_uss_EXPECTED\n",
    "ACTUAL['uss'] = test_uss_ACTUAL\n",
    "testOK('uss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44m\u001b[37m--------------------- ACTUAL ------------------------- \u001b[49m\u001b[34m\n",
      "\n",
      " *** USS:Story B\n",
      "\t achieved=0.0,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=1,\n",
      "\t tickets=\n",
      " *** Item#1 at start of sprint1: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=17.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=1\n",
      " *** Item#1 at end of sprint1: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=17.00,\n",
      "\t percentAchieved=0.7,\n",
      "\t sprintPlanned=1\n",
      " *** USS:Story B\n",
      "\t achieved=0.7,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=2,\n",
      "\t tickets=\n",
      "\t\t{Ticket:Bug 100,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=4,\n",
      "\t\t effortToDate=0.0,\n",
      "\t\t percentAchieved=0.0}\n",
      "\t\t{Ticket:Bug 101,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=1.5,\n",
      "\t\t effortToDate=0.0,\n",
      "\t\t percentAchieved=0.0}\n",
      " *** Item#1 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=5.10,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#2 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 100,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#3 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 101,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#2 at end of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 100,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=1.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#3 at end of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 101,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.5,\n",
      "\t sprintPlanned=2\n",
      " *** USS:Story B\n",
      "\t achieved=0.97,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=2,\n",
      "\t tickets=\n",
      "\t\t{Ticket:Bug 101,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=1.5,\n",
      "\t\t effortToDate=2.0,\n",
      "\t\t percentAchieved=0.5}\n"
     ]
    }
   ],
   "source": [
    "# Print ACTUAL output\n",
    "print(Back.BLUE + Fore.WHITE + '--------------------- ACTUAL -------------------------', \\\n",
    "      Back.RESET + Fore.BLUE + '\\n' + test_uss_ACTUAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[37m--------------------- EXPECTED ----------------------- \u001b[49m\u001b[32m\n",
      "\n",
      " *** USS:Story B\n",
      "\t achieved=0.0,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=1,\n",
      "\t tickets=\n",
      " *** Item#1 at start of sprint1: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=17.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=1\n",
      " *** Item#1 at end of sprint1: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=17.00,\n",
      "\t percentAchieved=0.7,\n",
      "\t sprintPlanned=1\n",
      " *** USS:Story B\n",
      "\t achieved=0.7,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=2,\n",
      "\t tickets=\n",
      "\t\t{Ticket:Bug 100,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=4,\n",
      "\t\t effortToDate=0.0,\n",
      "\t\t percentAchieved=0.0}\n",
      "\t\t{Ticket:Bug 101,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=1.5,\n",
      "\t\t effortToDate=0.0,\n",
      "\t\t percentAchieved=0.0}\n",
      " *** Item#1 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=UNFINISHED_STORIES,\n",
      "\t ticketId=None,\n",
      "\t estimate=5.10,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#2 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 100,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#3 at start of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 101,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#2 at end of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 100,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=1.0,\n",
      "\t sprintPlanned=2\n",
      " *** Item#3 at end of sprint2: \n",
      "\t userStoryId=Story B,\n",
      "\t taskType=BUGS_ON_UNFINISHED_STORIES,\n",
      "\t ticketId=Bug 101,\n",
      "\t estimate=2.00,\n",
      "\t percentAchieved=0.5,\n",
      "\t sprintPlanned=2\n",
      " *** USS:Story B\n",
      "\t achieved=0.97,\n",
      "\t planned=True,\n",
      "\t sprintPlanned=2,\n",
      "\t tickets=\n",
      "\t\t{Ticket:Bug 101,\n",
      "\t\t storyId=Story B,\n",
      "\t\t costToFix=1.5,\n",
      "\t\t effortToDate=2.0,\n",
      "\t\t percentAchieved=0.5}\n"
     ]
    }
   ],
   "source": [
    "# Print EXPECTED output\n",
    "print(Back.GREEN + Fore.WHITE + '--------------------- EXPECTED -----------------------', \\\n",
    "      Back.RESET + Fore.GREEN + '\\n' + test_uss_EXPECTED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_createTeams</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "teams_df = timecard.createTeamsDF(DEV_DF, PM_DF)\n",
    "\n",
    "object_cols = ['Scrum Team'] # Need to drop these since they print always-chaning memory address + can't save it in EXPECTED\n",
    "test_createTeams_ACTUAL = teams_df.drop(object_cols, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "#createExpectedOutput(test_createTeams_ACTUAL, 'test_createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols = ['Developers', 'Product Managers', 'Areas of Responsibility']\n",
    "test_createTeams_EXPECTED = loadExpectedOutput('test_createTeams', list_cols)\n",
    "\n",
    "EXPECTED['createTeams'] = test_createTeams_EXPECTED\n",
    "ACTUAL['createTeams'] = test_createTeams_ACTUAL\n",
    "testOK('createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Id</th>\n",
       "      <th>Developers</th>\n",
       "      <th>Product Managers</th>\n",
       "      <th>Areas of Responsibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Team A</td>\n",
       "      <td>[Anton Easterday, Beau Hockensmith, Bruno Stud...</td>\n",
       "      <td>[Sherlyn Cordle, Edgar Hibbler]</td>\n",
       "      <td>[Doctor, Patient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Team B</td>\n",
       "      <td>[Hyun Jaffe, Isaura Casterline, Jacinto Immel,...</td>\n",
       "      <td>[Spencer Venezia]</td>\n",
       "      <td>[Ministry of Health]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Team C</td>\n",
       "      <td>[Lonnie Belz, Lorriane Demmer, Margorie Bering...</td>\n",
       "      <td>[Jamie Addington]</td>\n",
       "      <td>[Hospital Administration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Team D</td>\n",
       "      <td>[Mohammad Tineo, Nohemi Santini, Olevia Haymak...</td>\n",
       "      <td>[Georgine Roan]</td>\n",
       "      <td>[Insurance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team Id                                         Developers  \\\n",
       "0  Team A  [Anton Easterday, Beau Hockensmith, Bruno Stud...   \n",
       "1  Team B  [Hyun Jaffe, Isaura Casterline, Jacinto Immel,...   \n",
       "2  Team C  [Lonnie Belz, Lorriane Demmer, Margorie Bering...   \n",
       "3  Team D  [Mohammad Tineo, Nohemi Santini, Olevia Haymak...   \n",
       "\n",
       "                  Product Managers    Areas of Responsibility  \n",
       "0  [Sherlyn Cordle, Edgar Hibbler]          [Doctor, Patient]  \n",
       "1                [Spencer Venezia]       [Ministry of Health]  \n",
       "2                [Jamie Addington]  [Hospital Administration]  \n",
       "3                  [Georgine Roan]                [Insurance]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_createTeams_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Id</th>\n",
       "      <th>Developers</th>\n",
       "      <th>Product Managers</th>\n",
       "      <th>Areas of Responsibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Team A</td>\n",
       "      <td>[Anton Easterday, Beau Hockensmith, Bruno Stud...</td>\n",
       "      <td>[Sherlyn Cordle, Edgar Hibbler]</td>\n",
       "      <td>[Doctor, Patient]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Team B</td>\n",
       "      <td>[Hyun Jaffe, Isaura Casterline, Jacinto Immel,...</td>\n",
       "      <td>[Spencer Venezia]</td>\n",
       "      <td>[Ministry of Health]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Team C</td>\n",
       "      <td>[Lonnie Belz, Lorriane Demmer, Margorie Bering...</td>\n",
       "      <td>[Jamie Addington]</td>\n",
       "      <td>[Hospital Administration]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Team D</td>\n",
       "      <td>[Mohammad Tineo, Nohemi Santini, Olevia Haymak...</td>\n",
       "      <td>[Georgine Roan]</td>\n",
       "      <td>[Insurance]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Team Id                                         Developers  \\\n",
       "0  Team A  [Anton Easterday, Beau Hockensmith, Bruno Stud...   \n",
       "1  Team B  [Hyun Jaffe, Isaura Casterline, Jacinto Immel,...   \n",
       "2  Team C  [Lonnie Belz, Lorriane Demmer, Margorie Bering...   \n",
       "3  Team D  [Mohammad Tineo, Nohemi Santini, Olevia Haymak...   \n",
       "\n",
       "                  Product Managers    Areas of Responsibility  \n",
       "0  [Sherlyn Cordle, Edgar Hibbler]          [Doctor, Patient]  \n",
       "1                [Spencer Venezia]       [Ministry of Health]  \n",
       "2                [Jamie Addington]  [Hospital Administration]  \n",
       "3                  [Georgine Roan]                [Insurance]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_createTeams_EXPECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_userStoryCreate</h1>\n",
    "<p>This test has multiple views, each of which is checked separately</p>\n",
    "<li>test_userStoryCreate_stories\n",
    "<li>test_userStoryCreate_estimates\n",
    "<li>test_userStoryCreate_crossCheck\n",
    "<li>test_userStoryCreate_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_userStoryCreate():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "        \n",
    "    grouped_estimates_df = stories_df.groupby([ 'Scrum Team', 'Developer'])['Estimate'].sum()\n",
    "    workload_df = stories_df.groupby([ 'Scrum Team'])['User Story Id'].count()\n",
    "    #workload_count_df = workload_df.count()\n",
    "    \n",
    "    avg_estimates_df = grouped_estimates_df.unstack().apply(lambda x: x.mean(), axis='columns')\n",
    "\n",
    "    # Reset index to match the way how EXPECTED will be saved as a CSV file\n",
    "    estimates_df      = grouped_estimates_df.reset_index()\n",
    "    workload_df       = workload_df.reset_index()\n",
    "    avg_estimates_df= avg_estimates_df.reset_index()\n",
    "    \n",
    "    # The unstacking above created an column with a number as the column name, 0. That will not match once expected output is saved\n",
    "    # and reloaded, as it will come back as the string '0'. So rename that column to avoid spurious test failures\n",
    "    avg_estimates_df = avg_estimates_df.rename(index=str, columns={0: 'Avg'})\n",
    "    \n",
    "    # Because of the manipulations, the index has changed and that will cause mistaches with the EXPECTED loaded from \n",
    "    # CSV. So re-index\n",
    "    avg_estimates_df.index = pd.RangeIndex(start=0, stop=avg_estimates_df.index.size, step=1)\n",
    "    \n",
    "    crosscheck = [len(teams_df['Scrum Team'][0].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][1].backlog.pendingUserStories),\n",
    "                  len(teams_df['Scrum Team'][2].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][3].backlog.pendingUserStories)]\n",
    "    crosscheck_df = pd.DataFrame({'Team idx': [0,1,2,3], 'Backlog size': crosscheck})\n",
    "    \n",
    "    output['stories_df'] = stories_df\n",
    "    output['estimates_df'] = estimates_df\n",
    "    output['workload_df'] = workload_df\n",
    "    output['crosscheck_df'] = crosscheck_df\n",
    "    output['avg_estimates_df'] = avg_estimates_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_userStoryCreate_ACTUAL = test_userStoryCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_userStoryCreate_EXPECTED():\n",
    "    createExpectedOutput(test_userStoryCreate_ACTUAL['stories_df'],        'test_userStoryCreate.stories_df')\n",
    "    createExpectedOutput(test_userStoryCreate_ACTUAL['estimates_df'],      'test_userStoryCreate.estimates_df')\n",
    "    createExpectedOutput(test_userStoryCreate_ACTUAL['workload_df'],       'test_userStoryCreate.workload_df')\n",
    "    createExpectedOutput(test_userStoryCreate_ACTUAL['crosscheck_df'],     'test_userStoryCreate.crosscheck_df')\n",
    "    createExpectedOutput(test_userStoryCreate_ACTUAL['avg_estimates_df'],  'test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_userStoryCreate_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "\n",
    "test_userStoryCreate_EXPECTED = {}\n",
    "\n",
    "test_userStoryCreate_EXPECTED['stories_df']         = loadExpectedOutput('test_userStoryCreate.stories_df')\n",
    "test_userStoryCreate_EXPECTED['estimates_df']       = loadExpectedOutput('test_userStoryCreate.estimates_df')\n",
    "test_userStoryCreate_EXPECTED['workload_df']        = loadExpectedOutput('test_userStoryCreate.workload_df')\n",
    "test_userStoryCreate_EXPECTED['crosscheck_df']      = loadExpectedOutput('test_userStoryCreate.crosscheck_df')\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']   = loadExpectedOutput('test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for the sensitive fields\n",
    "rounded = test_userStoryCreate_EXPECTED['avg_estimates_df']['Avg'].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']['Avg'] = rounded\n",
    "rounded = test_userStoryCreate_ACTUAL['avg_estimates_df']['Avg'].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "test_userStoryCreate_ACTUAL['avg_estimates_df']['Avg'] = rounded\n",
    "\n",
    "EXPECTED['test_userStoryCreate.stories_df']         = test_userStoryCreate_EXPECTED['stories_df']\n",
    "EXPECTED['test_userStoryCreate.estimates_df']       = test_userStoryCreate_EXPECTED['estimates_df']\n",
    "EXPECTED['test_userStoryCreate.workload_df']        = test_userStoryCreate_EXPECTED['workload_df']\n",
    "EXPECTED['test_userStoryCreate.crosscheck_df']      = test_userStoryCreate_EXPECTED['crosscheck_df']\n",
    "EXPECTED['test_userStoryCreate.avg_estimates_df']   = test_userStoryCreate_EXPECTED['avg_estimates_df']\n",
    "\n",
    "ACTUAL['test_userStoryCreate.stories_df']           = test_userStoryCreate_ACTUAL['stories_df']\n",
    "ACTUAL['test_userStoryCreate.estimates_df']         = test_userStoryCreate_ACTUAL['estimates_df']\n",
    "ACTUAL['test_userStoryCreate.workload_df']          = test_userStoryCreate_ACTUAL['workload_df']\n",
    "ACTUAL['test_userStoryCreate.crosscheck_df']        = test_userStoryCreate_ACTUAL['crosscheck_df']\n",
    "ACTUAL['test_userStoryCreate.avg_estimates_df']     = test_userStoryCreate_ACTUAL['avg_estimates_df']\n",
    "\n",
    "testOK('test_userStoryCreate.stories_df'), \\\n",
    "testOK('test_userStoryCreate.estimates_df'), \\\n",
    "testOK('test_userStoryCreate.workload_df'), \\\n",
    "testOK('test_userStoryCreate.crosscheck_df'), \\\n",
    "testOK('test_userStoryCreate.avg_estimates_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_WorkAssignments</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_WorkAssignments():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    #NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    \n",
    "    #timecard.RANDOM.reset(271) # Set seed so output is the same even if logic invokes random methods\n",
    "    #teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, 125)\n",
    "\n",
    "    teamId0 = teams_df['Scrum Team'][0].teamId\n",
    "    work = WorkAssignments(teamId0, teamsRepo, storiesRepo, sprint=1)\n",
    "    initial_df = work.committedTime(10)\n",
    "    # Test re-assigning of work\n",
    "    item = work.allocations[timecard.UNPLANNED][timecard.OWNER_TBD][timecard.UNFINISHED_STORIES][25]\n",
    "    work.reAssign(item, 'Bruno Studley', timecard.CURRENT_SPRINT)\n",
    "    final_df = work.committedTime(10)\n",
    "    \n",
    "    output['Initial'] = initial_df\n",
    "    output['Final'] = final_df\n",
    "\n",
    "    return output\n",
    "    \n",
    "# Run the test\n",
    "test_WorkAssignments_ACTUAL = test_WorkAssignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_WorkAssignments_EXPECTED():\n",
    "    createExpectedOutput(test_WorkAssignments_ACTUAL['Initial'],        'test_WorkAssignments.Initial')\n",
    "    createExpectedOutput(test_WorkAssignments_ACTUAL['Final'],          'test_WorkAssignments.Final')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_WorkAssignments_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_WorkAssignments_EXPECTED = {}\n",
    "\n",
    "test_WorkAssignments_EXPECTED['Initial']         = loadExpectedOutput('test_WorkAssignments.Initial')\n",
    "test_WorkAssignments_EXPECTED['Final']       = loadExpectedOutput('test_WorkAssignments.Final')\n",
    "\n",
    "EXPECTED['test_WorkAssignments.Initial']         = test_WorkAssignments_EXPECTED['Initial']\n",
    "EXPECTED['test_WorkAssignments.Final']           = test_WorkAssignments_EXPECTED['Final']\n",
    "\n",
    "ACTUAL['test_WorkAssignments.Initial']           = test_WorkAssignments_ACTUAL['Initial']\n",
    "ACTUAL['test_WorkAssignments.Final']             = test_WorkAssignments_ACTUAL['Final']\n",
    "\n",
    "testOK('test_WorkAssignments.Initial'), \\\n",
    "testOK('test_WorkAssignments.Final'), \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_greedyAllocation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_greedyAllocation():    \n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    model = GreedyAllocationModel(SPRINT_DURATION) \n",
    "    modelsConfig = ModelsConfig([], [], model)\n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "    # Choose what to work on at the start of a sprint.\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "   \n",
    "    work = WorkAssignments(teamId, teamsRepo, storiesRepo, sprint=1)\n",
    "\n",
    "    work = model.allocate(work, modelsConfig)\n",
    "    \n",
    "    committed_df = work.committedTime(SPRINT_DURATION)\n",
    "    tasks_df = work.committedTasks()\n",
    "        \n",
    "    output['Committed'] = committed_df\n",
    "    output['Tasks'] = tasks_df\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_greedyAllocation_ACTUAL = test_greedyAllocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_greedyAllocation_EXPECTED():\n",
    "    createExpectedOutput(test_greedyAllocation_ACTUAL['Committed'],    'test_greedyAllocation.Committed')\n",
    "    createExpectedOutput(test_greedyAllocation_ACTUAL['Tasks'],        'test_greedyAllocation.Tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_greedyAllocation_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_greedyAllocation_EXPECTED = {}\n",
    "\n",
    "test_greedyAllocation_EXPECTED['Committed']      = loadExpectedOutput('test_greedyAllocation.Committed')\n",
    "test_greedyAllocation_EXPECTED['Tasks']          = loadExpectedOutput('test_greedyAllocation.Tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                    'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth']\n",
    "testlets = ['Committed']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_greedyAllocation_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_greedyAllocation_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_greedyAllocation_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_greedyAllocation_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved']\n",
    "testlets = ['Tasks']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_greedyAllocation_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_greedyAllocation_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_greedyAllocation_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_greedyAllocation_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "EXPECTED['test_greedyAllocation.Committed']      = test_greedyAllocation_EXPECTED['Committed']\n",
    "EXPECTED['test_greedyAllocation.Tasks']          = test_greedyAllocation_EXPECTED['Tasks']\n",
    "\n",
    "ACTUAL['test_greedyAllocation.Committed']        = test_greedyAllocation_ACTUAL['Committed']\n",
    "ACTUAL['test_greedyAllocation.Tasks']            = test_greedyAllocation_ACTUAL['Tasks']\n",
    "\n",
    "testOK('test_greedyAllocation.Committed'), \\\n",
    "testOK('test_greedyAllocation.Tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greedyAllocation_ACTUAL['Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greedyAllocation_EXPECTED['Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greedyAllocation_ACTUAL['Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_greedyAllocation_EXPECTED['Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_oneSprint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_oneSprint():    \n",
    "    output = {}\n",
    "\n",
    "    # Choose what to work on at the start of a sprint.\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION))\n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "\n",
    "    \n",
    "    work = timecard.chooseWhatToDoInSprint(teamId, teamsRepo, storiesRepo, SPRINT_DURATION, modelsConfig, sprint=1)\n",
    "    #work = timecard.chooseWhatToDoInSprint(teamId, teamsRepo, storiesRepo, SPRINT_DURATION, sprint=1)\n",
    "    start_committed_df = work.committedTime(SPRINT_DURATION)\n",
    "    start_tasks_df = work.committedTasks()\n",
    "    \n",
    "    # Deliver what the sprint actually accomplished, including ingest of defects arriving during sprint\n",
    "    timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, SPRINT_DURATION, modelsConfig) # mutates work\n",
    "    inflow = timecard.inflowOfTickets(teamId, teamsRepo, ticketsRepo, storiesRepo)\n",
    "    end_committed_df = work.committedTime(0) # Sprint is over, so sprint capacity parameter is 0\n",
    "    \n",
    "    # Test continued\n",
    "    timecard.updateBacklogAfterSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, inflow)\n",
    "    end_tasks_df = work.committedTasks()\n",
    "    \n",
    "    output['Start_Committed'] = start_committed_df\n",
    "    output['Start_Tasks'] = start_tasks_df\n",
    "    output['End_Committed'] = end_committed_df\n",
    "    output['End_Tasks'] = end_tasks_df\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_oneSprint_ACTUAL = test_oneSprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_oneSprint_EXPECTED():\n",
    "    createExpectedOutput(test_oneSprint_ACTUAL['Start_Committed'],    'test_oneSprint.Start_Committed')\n",
    "    createExpectedOutput(test_oneSprint_ACTUAL['Start_Tasks'],        'test_oneSprint.Start_Tasks')\n",
    "    createExpectedOutput(test_oneSprint_ACTUAL['End_Committed'],      'test_oneSprint.End_Committed')\n",
    "    createExpectedOutput(test_oneSprint_ACTUAL['End_Tasks'],          'test_oneSprint.End_Tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_oneSprint_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_oneSprint_EXPECTED = {}\n",
    "\n",
    "test_oneSprint_EXPECTED['Start_Committed']      = loadExpectedOutput('test_oneSprint.Start_Committed')\n",
    "test_oneSprint_EXPECTED['Start_Tasks']          = loadExpectedOutput('test_oneSprint.Start_Tasks')\n",
    "test_oneSprint_EXPECTED['End_Committed']        = loadExpectedOutput('test_oneSprint.End_Committed')\n",
    "test_oneSprint_EXPECTED['End_Tasks']            = loadExpectedOutput('test_oneSprint.End_Tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                    'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth']\n",
    "testlets = ['Start_Committed', 'End_Committed']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_oneSprint_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_oneSprint_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved']\n",
    "testlets = ['Start_Tasks', 'End_Tasks']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_oneSprint_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_oneSprint_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "EXPECTED['test_oneSprint.Start_Committed']      = test_oneSprint_EXPECTED['Start_Committed']\n",
    "EXPECTED['test_oneSprint.Start_Tasks']          = test_oneSprint_EXPECTED['Start_Tasks']\n",
    "EXPECTED['test_oneSprint.End_Committed']        = test_oneSprint_EXPECTED['End_Committed']\n",
    "EXPECTED['test_oneSprint.End_Tasks']            = test_oneSprint_EXPECTED['End_Tasks']\n",
    "\n",
    "ACTUAL['test_oneSprint.Start_Committed']        = test_oneSprint_ACTUAL['Start_Committed']\n",
    "ACTUAL['test_oneSprint.Start_Tasks']            = test_oneSprint_ACTUAL['Start_Tasks']\n",
    "ACTUAL['test_oneSprint.End_Committed']          = test_oneSprint_ACTUAL['End_Committed']\n",
    "ACTUAL['test_oneSprint.End_Tasks']              = test_oneSprint_ACTUAL['End_Tasks']\n",
    "\n",
    "testOK('test_oneSprint.Start_Committed'), \\\n",
    "testOK('test_oneSprint.Start_Tasks'), \\\n",
    "testOK('test_oneSprint.End_Committed'), \\\n",
    "testOK('test_oneSprint.End_Tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_multipleSprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_multipleSprints():\n",
    "    output = {}\n",
    "    # Test many sprints into the future, to see if eventually people have extra time and start using that extra time\n",
    "    # in the current sprint to get a head start on tasks for the next sprint\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 15 \n",
    "    \n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             125, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "\n",
    "\n",
    "    work = None\n",
    "    for i in range(NUMBER_OF_SPRINTS):\n",
    "        work = timecard.chooseWhatToDoInSprint(teamId, teamsRepo, storiesRepo, SPRINT_DURATION, modelsConfig, sprint=i+1)\n",
    "        if (i== NUMBER_OF_SPRINTS -1):\n",
    "            break\n",
    "        timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, SPRINT_DURATION, \\\n",
    "                              modelsConfig) # mutates 'work'\n",
    "        inflow = timecard.inflowOfTickets(teamId, teamsRepo, ticketsRepo, storiesRepo)\n",
    "        timecard.updateBacklogAfterSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, inflow) # Does not mutate 'work'\n",
    "    \n",
    "    # Work Assignments at the start of the last sprint. Should see some \"looking ahead\" tasks, i.e., tasks that would\n",
    "    # normally be done in the next sprint but are started in this sprint since we have time leftover from this sprint's \n",
    "    # deliverables\n",
    "    last = work\n",
    "    start_committed_df = last.committedTime(10)\n",
    "    start_tasks_df     = last.committedTasks()\n",
    "    \n",
    "    # Finish this last sprint and confirm we spent time in some of the deliveries for the next sprint (i.e., that we\n",
    "    # were \"looking ahead\")\n",
    "    timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, last, 10, modelsConfig)\n",
    "    end_committed_df = last.committedTime(0)\n",
    "    end_tasks_df     =last.committedTasks()\n",
    "    \n",
    "    output['start_committed']   = start_committed_df\n",
    "    output['start_tasks']       = start_tasks_df\n",
    "    output['end_committed']     = end_committed_df\n",
    "    output['end_tasks']         = end_tasks_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_multipleSprints_ACTUAL = test_multipleSprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_multipleSprints_EXPECTED():\n",
    "    createExpectedOutput(test_multipleSprints_ACTUAL['start_committed'],    'test_multipleSprints.start_committed')\n",
    "    createExpectedOutput(test_multipleSprints_ACTUAL['start_tasks'],        'test_multipleSprints.start_tasks')\n",
    "    createExpectedOutput(test_multipleSprints_ACTUAL['end_committed'],      'test_multipleSprints.end_committed')\n",
    "    createExpectedOutput(test_multipleSprints_ACTUAL['end_tasks'],          'test_multipleSprints.end_tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_multipleSprints_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_multipleSprints_EXPECTED = {}\n",
    "\n",
    "test_multipleSprints_EXPECTED['start_committed']    = loadExpectedOutput('test_multipleSprints.start_committed')\n",
    "test_multipleSprints_EXPECTED['start_tasks']        = loadExpectedOutput('test_multipleSprints.start_tasks')\n",
    "test_multipleSprints_EXPECTED['end_committed']      = loadExpectedOutput('test_multipleSprints.end_committed')\n",
    "test_multipleSprints_EXPECTED['end_tasks']          = loadExpectedOutput('test_multipleSprints.end_tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                    'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth']\n",
    "testlets = ['start_committed', 'end_committed']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_multipleSprints_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_multipleSprints_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved']\n",
    "testlets = ['start_tasks', 'end_tasks']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_multipleSprints_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_multipleSprints_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "EXPECTED['test_multipleSprints.start_committed']    = test_multipleSprints_EXPECTED['start_committed']\n",
    "EXPECTED['test_multipleSprints.start_tasks']        = test_multipleSprints_EXPECTED['start_tasks']\n",
    "EXPECTED['test_multipleSprints.end_committed']      = test_multipleSprints_EXPECTED['end_committed']\n",
    "EXPECTED['test_multipleSprints.end_tasks']          = test_multipleSprints_EXPECTED['end_tasks']\n",
    "\n",
    "ACTUAL['test_multipleSprints.start_committed']      = test_multipleSprints_ACTUAL['start_committed']\n",
    "ACTUAL['test_multipleSprints.start_tasks']          = test_multipleSprints_ACTUAL['start_tasks']\n",
    "ACTUAL['test_multipleSprints.end_committed']        = test_multipleSprints_ACTUAL['end_committed']\n",
    "ACTUAL['test_multipleSprints.end_tasks']            = test_multipleSprints_ACTUAL['end_tasks']\n",
    "\n",
    "testOK('test_multipleSprints.start_committed'), \\\n",
    "testOK('test_multipleSprints.start_tasks'), \\\n",
    "testOK('test_multipleSprints.end_committed'), \\\n",
    "testOK('test_multipleSprints.end_tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_releaseCycle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "# Test logic\n",
    "def test_releaseCycle():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "\n",
    "\n",
    "    entries_df, worksheets = timecard.runReleaseCycle(teamsRepo, ticketsRepo, storiesRepo, datetime(2018, 1, 15), \\\n",
    "                                                   SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)    \n",
    "    \n",
    "    anton_df = entries_df[entries_df['Developer'] == 'Anton Easterday']\n",
    "    glenna_df = entries_df[entries_df['Developer'] == 'Glenna Mcghie']\n",
    "        \n",
    "    burnout_df = timecard.releaseBurnout(entries_df)\n",
    "    \n",
    "    output['Entries'] = entries_df\n",
    "    output['Anton'] = anton_df\n",
    "    output['Glenna'] = glenna_df\n",
    "    output['Burnout'] = burnout_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_releaseCycle_ACTUAL = test_releaseCycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_releaseCycle_EXPECTED():\n",
    "    createExpectedOutput(test_releaseCycle_ACTUAL['Entries'],    'test_releaseCycle.Entries')\n",
    "    createExpectedOutput(test_releaseCycle_ACTUAL['Anton'],      'test_releaseCycle.Anton')\n",
    "    createExpectedOutput(test_releaseCycle_ACTUAL['Glenna'],     'test_releaseCycle.Glenna')\n",
    "    createExpectedOutput(test_releaseCycle_ACTUAL['Burnout'],    'test_releaseCycle.Burnout')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_releaseCycle_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Alex\\CodeImages\\technos\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_releaseCycle_EXPECTED = {}\n",
    "\n",
    "test_releaseCycle_EXPECTED['Entries']      = loadExpectedOutput('test_releaseCycle.Entries')\n",
    "test_releaseCycle_EXPECTED['Anton']        = loadExpectedOutput('test_releaseCycle.Anton')\n",
    "test_releaseCycle_EXPECTED['Glenna']       = loadExpectedOutput('test_releaseCycle.Glenna')\n",
    "test_releaseCycle_EXPECTED['Burnout']      = loadExpectedOutput('test_releaseCycle.Burnout')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Time Spent']\n",
    "testlets = ['Entries', 'Anton', 'Glenna']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_releaseCycle_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_releaseCycle_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Effort']\n",
    "testlets = ['Burnout']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_releaseCycle_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_releaseCycle_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_ACTUAL[testlet][field] = rounded\n",
    "        \n",
    "# Dates are loaded as strings, not pd.Timestamps, so to avoid spurious mismatches between ACTUAL (which represents dates\n",
    "# as pd.Timestamps) and EXPECTED, convert the EXPECTED dates into pd.Timestamps\n",
    "d = test_releaseCycle_EXPECTED['Entries']['Date']\n",
    "test_releaseCycle_EXPECTED['Entries']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "d = test_releaseCycle_EXPECTED['Anton']['Date']\n",
    "test_releaseCycle_EXPECTED['Anton']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "d = test_releaseCycle_EXPECTED['Glenna']['Date']\n",
    "test_releaseCycle_EXPECTED['Glenna']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "\n",
    "# For output that was filtered (i.e., a subset of rows from a dataframe), the CSV files messes up the index so fix this \n",
    "# to avoid spurious mismatches between ACTUAL and EXPECTED\n",
    "test_releaseCycle_EXPECTED['Anton'].index = test_releaseCycle_ACTUAL['Anton'].index\n",
    "test_releaseCycle_EXPECTED['Glenna'].index = test_releaseCycle_ACTUAL['Glenna'].index\n",
    "\n",
    "EXPECTED['test_releaseCycle.Entries']        = test_releaseCycle_EXPECTED['Entries']\n",
    "EXPECTED['test_releaseCycle.Anton']          = test_releaseCycle_EXPECTED['Anton']\n",
    "EXPECTED['test_releaseCycle.Glenna']         = test_releaseCycle_EXPECTED['Glenna']\n",
    "EXPECTED['test_releaseCycle.Burnout']        = test_releaseCycle_EXPECTED['Burnout']\n",
    "\n",
    "ACTUAL['test_releaseCycle.Entries']          = test_releaseCycle_ACTUAL['Entries']\n",
    "ACTUAL['test_releaseCycle.Anton']            = test_releaseCycle_ACTUAL['Anton']\n",
    "ACTUAL['test_releaseCycle.Glenna']           = test_releaseCycle_ACTUAL['Glenna']\n",
    "ACTUAL['test_releaseCycle.Burnout']          = test_releaseCycle_ACTUAL['Burnout']\n",
    "\n",
    "testOK('test_releaseCycle.Entries'), \\\n",
    "testOK('test_releaseCycle.Anton'), \\\n",
    "testOK('test_releaseCycle.Glenna'), \\\n",
    "testOK('test_releaseCycle.Burnout'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Anton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Anton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Glenna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Glenna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_FOO: template for tests that produce a DataFrame output </h1>\n",
    "<li> Copy to create a new test, and replace 'FOO' by the name of your test.\n",
    "<li> Implement test logic in test_FOO()\n",
    "<li> To create expected output once test logic is producing right output, temporarily uncomment the call to createExpectedOutput, call it, and comment it again. This saves expected output as a CSV file so subsequent runs of the test can load it and verify that the test still produces the same output as before.\n",
    "<li> Uncomment lines that load expected output and remove the dummy line next to it creating a dummy expected output.\n",
    "<li> Display the outputs for convenience in debugging when test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_FOO():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    dummyA_df = pd.DataFrame({'A': [45.123456789, 34.123456789]})\n",
    "    dummyB_df = pd.DataFrame({'B': [2333.123456789, 3984.123456789]})\n",
    "    \n",
    "    output['Dummy_A'] = dummyA_df\n",
    "    output['Dummy_B'] = dummyB_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_FOO_ACTUAL = test_FOO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_FOO_EXPECTED():\n",
    "    createExpectedOutput(test_FOO_ACTUAL['Dummy_A'],    'test_FOO.Dummy_A')\n",
    "    createExpectedOutput(test_FOO_ACTUAL['Dummy_B'],    'test_FOO.Dummy_B')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_FOO_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_FOO_EXPECTED = {}\n",
    "\n",
    "test_FOO_EXPECTED['Dummy_A']      = loadExpectedOutput('test_FOO.Dummy_A')\n",
    "test_FOO_EXPECTED['Dummy_B']      = loadExpectedOutput('test_FOO.Dummy_B')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['A']\n",
    "testlets = ['Dummy_A']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_FOO_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_FOO_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['B']\n",
    "testlets = ['Dummy_B']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_FOO_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_FOO_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "\n",
    "EXPECTED['test_FOO.Dummy_A']        = test_FOO_EXPECTED['Dummy_A']\n",
    "EXPECTED['test_FOO.Dummy_B']        = test_FOO_EXPECTED['Dummy_B']\n",
    "\n",
    "ACTUAL['test_FOO.Dummy_A']          = test_FOO_ACTUAL['Dummy_A']\n",
    "ACTUAL['test_FOO.Dummy_B']          = test_FOO_ACTUAL['Dummy_B']\n",
    "\n",
    "testOK('test_FOO.Dummy_A'), \\\n",
    "testOK('test_FOO.Dummy_B'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run all the tests in this notebook</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all the tests defined in this notebook\n",
    "#unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
