{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "from colorama import Fore, Back, Style \n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Code to be Tested</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\statics.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\businessObjects.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\simulationModels.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\generateTimecards.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.simulation.statics as S_\n",
    "\n",
    "import devanalyst.simulation.businessObjects as bo\n",
    "from devanalyst.simulation.businessObjects import UserStory, UserStoriesRepo, Ticket, TicketsRepo, WorkItem, \\\n",
    "UserStoryStatus, Backlog, ScrumTeam, ScrumTeamsRepo, ReleaseCycleContext, GlobalRepo\n",
    "\n",
    "from devanalyst.simulation.simulationModels import ModelsConfig, DefaultCostModel, GreedyAllocationModel, \\\n",
    "DistributedLagQualityModel\n",
    "\n",
    "import devanalyst.simulation.generateTimecards as timecard\n",
    "from devanalyst.simulation.generateTimecards import IdCounter, WorkAssignments, ReleaseLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\test_utils\\test_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.test_utils.test_utils as tu_\n",
    "from devanalyst.test_utils.test_utils import ExpectedOutputCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_uss</h1>\n",
    "<p>Test User Story Status</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     12,
     18,
     26,
     36
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "# Helper methods to the test\n",
    "def format_ticket(ticket):\n",
    "    return ('Ticket:' + ticket.ticketId \\\n",
    "            + ',\\n\\t\\t storyId=' + ticket.userStoryId \\\n",
    "            + ',\\n\\t\\t costToFix=' + str(ticket.estimatedCost) \\\n",
    "            + ',\\n\\t\\t sprintReported=' + str(ticket.sprintReported) \\\n",
    "            + ',\\n\\t\\t sprintFixed=' + str(ticket.sprintFixed) \\\n",
    "            + ',\\n\\t\\t effortToDate=' + str(ticket.effortToDate) \\\n",
    "            + ',\\n\\t\\t percentAchieved=' + str(ticket.percentAchieved))\n",
    "                \n",
    "def format_tickets(tickets):\n",
    "    output = ''\n",
    "    for ticket in tickets:\n",
    "        output = output + '\\n\\t\\t{' + format_ticket(ticket) + '}'\n",
    "    return output\n",
    "            \n",
    "def format_uss(uss, globalRepo):\n",
    "    openTickets = globalRepo.ticketsRepo.getOpenTickets(uss.userStoryId)        \n",
    "    return ('\\n *** USS:' + uss.userStoryId \\\n",
    "            + '\\n\\t achieved=' + str(uss.percentAchieved) \\\n",
    "            + ',\\n\\t planned=' + str(uss.planned) \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(uss.sprintPlanned) \\\n",
    "            + ',\\n\\t tickets=' + format_tickets(openTickets))\n",
    "\n",
    "def format_item(item, item_label, sprint, timeInSprint):\n",
    "    return ('\\n *** ' + item_label + ' at ' + timeInSprint + ' of sprint' + str(sprint) + ': ' \\\n",
    "            + '\\n\\t userStoryId=' + str(item.userStoryId) \\\n",
    "            + ',\\n\\t taskType=' + str(item.taskType) \\\n",
    "            + ',\\n\\t ticketId=' + str(item.ticketId) \\\n",
    "            + ',\\n\\t estimate=' + '{0:.2f}'.format(item.estimate) \\\n",
    "            + ',\\n\\t percentAchieved=' + str(item.percentAchieved)  \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(item.sprintPlanned))\n",
    "\n",
    "# Test logic\n",
    "def test_uss():\n",
    "    output = '' \n",
    "    repo = UserStoriesRepo([UserStory('Story A', 25, 'Joe Developer', 'Amy PM'), \\\n",
    "                            UserStory('Story B', 17, 'Alex Developer', 'Kate PM')])\n",
    "    # SPRINT 1\n",
    "    ctx = ReleaseCycleContext(teamId='', sprint=1, sprintDuration=None)\n",
    "    globalRepo = GlobalRepo(developersRepo=None, teamsRepo=None, storiesRepo=repo, ticketsRepo=TicketsRepo())\n",
    "    uss = UserStoryStatus('Story B', 0.0)\n",
    "    uss.planned = True\n",
    "    uss.sprintPlanned = 1\n",
    "    output = output + (format_uss(uss, globalRepo))\n",
    "    item = uss._generateWorkItems(ctx, globalRepo)[0]\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'start'))\n",
    "    item.percentAchieved = 0.7\n",
    "    \n",
    "    globalRepo.ticketsRepo.addTicket(Ticket('Bug 100','Story B', 4, 1))\n",
    "    globalRepo.ticketsRepo.addTicket(Ticket('Bug 101','Story B', 1.5, 1))\n",
    "    \n",
    "    output = output + (format_item(item, 'Item#1', 1, 'end'))\n",
    "    uss.updateStatus([item], ctx, globalRepo)\n",
    "    \n",
    "    # SPRINT 2\n",
    "    ctx.sprint = 2\n",
    "    uss.sprintPlanned = 2\n",
    "    output = output + (format_uss(uss, globalRepo))\n",
    "\n",
    "    items = uss._generateWorkItems(ctx, globalRepo)\n",
    "    item=items[0]\n",
    "    output = output + (format_item(item, 'Item#1', 2, 'start'))\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'start'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'start'))\n",
    "    items[0].percentAchieved = 0.9\n",
    "    items[0].actual = 18.3\n",
    "    items[1].percentAchieved = 1.0\n",
    "    items[1].actual = 4.32\n",
    "    items[2].percentAchieved = 0.5\n",
    "    items[2].actual = 0.86\n",
    "    item=items[0]\n",
    "    output = output + (format_item(item, 'Item#1', 2, 'end'))\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'end'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'end'))\n",
    "    uss.updateStatus(items, ctx, globalRepo)\n",
    "    output = output + (format_uss(uss, globalRepo))\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_uss_ACTUAL = test_uss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to print a string output one can copy and paste into test_uss_EXPECTED\n",
    "#test_uss_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_uss_EXPECTED = '\\n *** USS:Story B\\n\\t achieved=0.0,\\n\\t planned=True,\\n\\t sprintPlanned=1,\\n\\t tickets=\\n *** Item#1 at start of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=1\\n *** Item#1 at end of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.7,\\n\\t sprintPlanned=1\\n *** USS:Story B\\n\\t achieved=0.7,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 100,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=4,\\n\\t\\t sprintReported=1,\\n\\t\\t sprintFixed=NOT_SET,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t sprintReported=1,\\n\\t\\t sprintFixed=NOT_SET,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n *** Item#1 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=5.10,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=DEV_TIME_BUGS,\\n\\t ticketId=Bug 100,\\n\\t estimate=4.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#3 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=DEV_TIME_BUGS,\\n\\t ticketId=Bug 101,\\n\\t estimate=1.50,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#1 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=5.10,\\n\\t percentAchieved=0.9,\\n\\t sprintPlanned=2\\n *** Item#2 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=DEV_TIME_BUGS,\\n\\t ticketId=Bug 100,\\n\\t estimate=4.00,\\n\\t percentAchieved=1.0,\\n\\t sprintPlanned=2\\n *** Item#3 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=DEV_TIME_BUGS,\\n\\t ticketId=Bug 101,\\n\\t estimate=1.50,\\n\\t percentAchieved=0.5,\\n\\t sprintPlanned=2\\n *** USS:Story B\\n\\t achieved=0.97,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t sprintReported=1,\\n\\t\\t sprintFixed=NOT_SET,\\n\\t\\t effortToDate=0.86,\\n\\t\\t percentAchieved=0.5}'\n",
    "\n",
    "tu_.EXPECTED['uss'] = test_uss_EXPECTED\n",
    "tu_.ACTUAL['uss'] = test_uss_ACTUAL\n",
    "tu_.testOK('uss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print ACTUAL output\n",
    "print(Back.BLUE + Fore.WHITE + '--------------------- ACTUAL -------------------------', \\\n",
    "      Back.RESET + Fore.BLUE + '\\n' + test_uss_ACTUAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print EXPECTED output\n",
    "print(Back.GREEN + Fore.WHITE + '--------------------- EXPECTED -----------------------', \\\n",
    "      Back.RESET + Fore.GREEN + '\\n' + test_uss_EXPECTED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_createTeams</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "teams_df = bo.createTeamsDF(tu_.DEV_DF, tu_.PM_DF)\n",
    "\n",
    "object_cols = ['Scrum Team'] # Need to drop these since they print always-chaning memory address + can't save it in EXPECTED\n",
    "test_createTeams_ACTUAL = teams_df.drop(object_cols, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "#tu_.createExpectedOutput(test_createTeams_ACTUAL, 'test_createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols = ['Developers', 'Product Managers', 'Areas of Responsibility']\n",
    "test_createTeams_EXPECTED = tu_.loadExpectedOutput('test_createTeams', list_cols)\n",
    "\n",
    "tu_.EXPECTED['createTeams'] = test_createTeams_EXPECTED\n",
    "tu_.ACTUAL['createTeams'] = test_createTeams_ACTUAL\n",
    "tu_.testOK('createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_EXPECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_userStoryCreate</h1>\n",
    "<p>This test has multiple views, each of which is checked separately</p>\n",
    "<li>test_userStoryCreate_stories\n",
    "<li>test_userStoryCreate_estimates\n",
    "<li>test_userStoryCreate_crossCheck\n",
    "<li>test_userStoryCreate_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_userStoryCreate():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "        \n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    \n",
    "    grouped_estimates_df = stories_df.groupby([ 'Scrum Team', 'Developer'])['Estimate'].sum()\n",
    "    workload_df          = stories_df.groupby([ 'Scrum Team'])['User Story Id'].count()\n",
    "    \n",
    "    avg_estimates_df = grouped_estimates_df.unstack().apply(lambda x: x.mean(), axis='columns')\n",
    "\n",
    "    # Reset index to match the way how EXPECTED will be saved as a CSV file\n",
    "    estimates_df      = grouped_estimates_df.reset_index()\n",
    "    workload_df       = workload_df.reset_index()\n",
    "    avg_estimates_df  = avg_estimates_df.reset_index()\n",
    "    \n",
    "    # The unstacking above created an column with a number as the column name, 0. That will not match once expected output is saved\n",
    "    # and reloaded, as it will come back as the string '0'. So rename that column to avoid spurious test failures\n",
    "    avg_estimates_df = avg_estimates_df.rename(index=str, columns={0: 'Avg'})\n",
    "    \n",
    "    # Because of the manipulations, the index has changed and that will cause mistaches with the EXPECTED loaded from \n",
    "    # CSV. So re-index\n",
    "    avg_estimates_df.index = pd.RangeIndex(start=0, stop=avg_estimates_df.index.size, step=1)\n",
    "    \n",
    "    crosscheck = [len(teams_df['Scrum Team'][0].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][1].backlog.pendingUserStories),\n",
    "                  len(teams_df['Scrum Team'][2].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][3].backlog.pendingUserStories)]\n",
    "    crosscheck_df = pd.DataFrame({'Team idx': [0,1,2,3], 'Backlog size': crosscheck})\n",
    "    \n",
    "    output['stories_df'] = stories_df\n",
    "    output['estimates_df'] = estimates_df\n",
    "    output['workload_df'] = workload_df\n",
    "    output['crosscheck_df'] = crosscheck_df\n",
    "    output['avg_estimates_df'] = avg_estimates_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_userStoryCreate_ACTUAL = test_userStoryCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_userStoryCreate_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['stories_df'],        'test_userStoryCreate.stories_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['estimates_df'],      'test_userStoryCreate.estimates_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['workload_df'],       'test_userStoryCreate.workload_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['crosscheck_df'],     'test_userStoryCreate.crosscheck_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['avg_estimates_df'],  'test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_userStoryCreate_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "\n",
    "test_userStoryCreate_EXPECTED = {}\n",
    "\n",
    "test_userStoryCreate_EXPECTED['stories_df']         = tu_.loadExpectedOutput('test_userStoryCreate.stories_df')\n",
    "test_userStoryCreate_EXPECTED['estimates_df']       = tu_.loadExpectedOutput('test_userStoryCreate.estimates_df')\n",
    "test_userStoryCreate_EXPECTED['workload_df']        = tu_.loadExpectedOutput('test_userStoryCreate.workload_df')\n",
    "test_userStoryCreate_EXPECTED['crosscheck_df']      = tu_.loadExpectedOutput('test_userStoryCreate.crosscheck_df')\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']   = tu_.loadExpectedOutput('test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for the sensitive fields\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Avg'],\n",
    "                                        ['avg_estimates_df'],\n",
    "                                        test_userStoryCreate_EXPECTED,\n",
    "                                        test_userStoryCreate_ACTUAL)\n",
    "\n",
    "tu_.EXPECTED['test_userStoryCreate.stories_df']         = test_userStoryCreate_EXPECTED['stories_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.estimates_df']       = test_userStoryCreate_EXPECTED['estimates_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.workload_df']        = test_userStoryCreate_EXPECTED['workload_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.crosscheck_df']      = test_userStoryCreate_EXPECTED['crosscheck_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.avg_estimates_df']   = test_userStoryCreate_EXPECTED['avg_estimates_df']\n",
    "\n",
    "tu_.ACTUAL['test_userStoryCreate.stories_df']           = test_userStoryCreate_ACTUAL['stories_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.estimates_df']         = test_userStoryCreate_ACTUAL['estimates_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.workload_df']          = test_userStoryCreate_ACTUAL['workload_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.crosscheck_df']        = test_userStoryCreate_ACTUAL['crosscheck_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.avg_estimates_df']     = test_userStoryCreate_ACTUAL['avg_estimates_df']\n",
    "\n",
    "tu_.testOK('test_userStoryCreate.stories_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.estimates_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.workload_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.crosscheck_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.avg_estimates_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_WorkAssignments</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_WorkAssignments():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    SPRINT = 1\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    teamId0 = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "    modelsConfig.context = ReleaseCycleContext(teamId0, SPRINT, SPRINT_DURATION)\n",
    "        \n",
    "    work = WorkAssignments(modelsConfig.context, modelsConfig.globalRepo)\n",
    "    initial_df = work.committedTime(SPRINT_DURATION)\n",
    "    # Test re-assigning of work\n",
    "    item = work.allocations[S_.UNPLANNED][S_.OWNER_TBD][S_.UNFINISHED_STORIES][25]\n",
    "    work.reAssign(item, 'Bruno Studley', S_.CURRENT_SPRINT)\n",
    "    final_df = work.committedTime(SPRINT_DURATION)\n",
    "    \n",
    "    output['Initial'] = initial_df\n",
    "    output['Final'] = final_df\n",
    "\n",
    "    return output\n",
    "    \n",
    "# Run the test\n",
    "test_WorkAssignments_ACTUAL = test_WorkAssignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_WorkAssignments_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Initial'],        'test_WorkAssignments.Initial')\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Final'],          'test_WorkAssignments.Final')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_WorkAssignments_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_WorkAssignments_EXPECTED = {}\n",
    "\n",
    "test_WorkAssignments_EXPECTED['Initial']         = tu_.loadExpectedOutput('test_WorkAssignments.Initial')\n",
    "test_WorkAssignments_EXPECTED['Final']           = tu_.loadExpectedOutput('test_WorkAssignments.Final')\n",
    "\n",
    "tu_.EXPECTED['test_WorkAssignments.Initial']         = test_WorkAssignments_EXPECTED['Initial']\n",
    "tu_.EXPECTED['test_WorkAssignments.Final']           = test_WorkAssignments_EXPECTED['Final']\n",
    "\n",
    "tu_.ACTUAL['test_WorkAssignments.Initial']           = test_WorkAssignments_ACTUAL['Initial']\n",
    "tu_.ACTUAL['test_WorkAssignments.Final']             = test_WorkAssignments_ACTUAL['Final']\n",
    "\n",
    "tu_.testOK('test_WorkAssignments.Initial'), \\\n",
    "tu_.testOK('test_WorkAssignments.Final'), \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_oneSprint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_oneSprint():    \n",
    "    output = {}\n",
    "\n",
    "    # Choose what to work on at the start of a sprint.\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    SPRINT = 1\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel())\n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "    modelsConfig.context = ReleaseCycleContext(teamId, SPRINT, SPRINT_DURATION)\n",
    "\n",
    "    work = timecard.chooseWhatToDoInSprint(modelsConfig)\n",
    "    start_committed_df = work.committedTime(SPRINT_DURATION)\n",
    "    start_tasks_df = work.committedTasks()\n",
    "    \n",
    "    # Deliver what the sprint actually accomplished, including ingest of defects arriving during sprint\n",
    "    timecard.deliverSprint(work, modelsConfig) # mutates work\n",
    "    inflow = timecard.inflowOfTickets(modelsConfig)\n",
    "    end_committed_df = work.committedTime(0) # Sprint is over, so sprint capacity parameter is 0\n",
    "    \n",
    "    # Test continued\n",
    "    timecard.updateBacklogAfterSprint(work, modelsConfig)\n",
    "    end_tasks_df = work.committedTasks()\n",
    "    \n",
    "    output['Start_Committed'] = start_committed_df\n",
    "    output['Start_Tasks'] = start_tasks_df\n",
    "    output['End_Committed'] = end_committed_df\n",
    "    output['End_Tasks'] = end_tasks_df\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_oneSprint_ACTUAL = test_oneSprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_oneSprint_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Committed'],    'test_oneSprint.Start_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Tasks'],        'test_oneSprint.Start_Tasks')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Committed'],      'test_oneSprint.End_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Tasks'],          'test_oneSprint.End_Tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_oneSprint_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_oneSprint_EXPECTED = {}\n",
    "\n",
    "test_oneSprint_EXPECTED['Start_Committed']      = tu_.loadExpectedOutput('test_oneSprint.Start_Committed')\n",
    "test_oneSprint_EXPECTED['Start_Tasks']          = tu_.loadExpectedOutput('test_oneSprint.Start_Tasks')\n",
    "test_oneSprint_EXPECTED['End_Committed']        = tu_.loadExpectedOutput('test_oneSprint.End_Committed')\n",
    "test_oneSprint_EXPECTED['End_Tasks']            = tu_.loadExpectedOutput('test_oneSprint.End_Tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                                          'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                        ['Start_Committed', 'End_Committed'],\n",
    "                                        test_oneSprint_EXPECTED,\n",
    "                                        test_oneSprint_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent', 'Effort Remaining', \\\n",
    "                                          'Percent Achieved', '% Global Done'],\n",
    "                                        ['Start_Tasks', 'End_Tasks'],\n",
    "                                        test_oneSprint_EXPECTED,\n",
    "                                        test_oneSprint_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.destringify(['Delivered in Sprint'],\n",
    "                                 ['End_Tasks'],\n",
    "                                 test_oneSprint_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_oneSprint.Start_Committed']      = test_oneSprint_EXPECTED['Start_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.Start_Tasks']          = test_oneSprint_EXPECTED['Start_Tasks']\n",
    "tu_.EXPECTED['test_oneSprint.End_Committed']        = test_oneSprint_EXPECTED['End_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.End_Tasks']            = test_oneSprint_EXPECTED['End_Tasks']\n",
    "\n",
    "tu_.ACTUAL['test_oneSprint.Start_Committed']        = test_oneSprint_ACTUAL['Start_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.Start_Tasks']            = test_oneSprint_ACTUAL['Start_Tasks']\n",
    "tu_.ACTUAL['test_oneSprint.End_Committed']          = test_oneSprint_ACTUAL['End_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.End_Tasks']              = test_oneSprint_ACTUAL['End_Tasks']\n",
    "\n",
    "tu_.testOK('test_oneSprint.Start_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.Start_Tasks'), \\\n",
    "tu_.testOK('test_oneSprint.End_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.End_Tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Tasks'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_multipleSprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_multipleSprints():\n",
    "    output = {}\n",
    "    # Test many sprints into the future, to see if eventually people have extra time and start using that extra time\n",
    "    # in the current sprint to get a head start on tasks for the next sprint\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 15 \n",
    "    \n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        125, SPRINT_DURATION, modelsConfig)\n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "        \n",
    "\n",
    "    work = None\n",
    "    for i in range(NUMBER_OF_SPRINTS):\n",
    "        sprint = i+1\n",
    "        modelsConfig.context = ReleaseCycleContext(teamId, sprint, SPRINT_DURATION)\n",
    "        work = timecard.chooseWhatToDoInSprint(modelsConfig)\n",
    "        if (i== NUMBER_OF_SPRINTS -1):\n",
    "            break\n",
    "        timecard.deliverSprint(work, modelsConfig) # mutates 'work'\n",
    "        inflow = timecard.inflowOfTickets(modelsConfig)\n",
    "        timecard.updateBacklogAfterSprint(work, modelsConfig) # Does not mutate 'work'\n",
    "    \n",
    "    # Work Assignments at the start of the last sprint. Should see some \"looking ahead\" tasks, i.e., tasks that would\n",
    "    # normally be done in the next sprint but are started in this sprint since we have time leftover from this sprint's \n",
    "    # deliverables\n",
    "    last = work\n",
    "    start_committed_df = last.committedTime(10)\n",
    "    start_tasks_df     = last.committedTasks()\n",
    "    \n",
    "    # Finish this last sprint and confirm we spent time in some of the deliveries for the next sprint (i.e., that we\n",
    "    # were \"looking ahead\")\n",
    "    timecard.deliverSprint(last, modelsConfig)\n",
    "    inflow = timecard.inflowOfTickets(modelsConfig)\n",
    "    timecard.updateBacklogAfterSprint(work, modelsConfig) # Does not mutate 'work'\n",
    "\n",
    "    end_committed_df = last.committedTime(0)\n",
    "    end_tasks_df     =last.committedTasks()\n",
    "    \n",
    "    output['start_committed']   = start_committed_df\n",
    "    output['start_tasks']       = start_tasks_df\n",
    "    output['end_committed']     = end_committed_df\n",
    "    output['end_tasks']         = end_tasks_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_multipleSprints_ACTUAL = test_multipleSprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_multipleSprints_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_committed'],    'test_multipleSprints.start_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_tasks'],        'test_multipleSprints.start_tasks')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_committed'],      'test_multipleSprints.end_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_tasks'],          'test_multipleSprints.end_tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_multipleSprints_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_multipleSprints_EXPECTED = {}\n",
    "\n",
    "test_multipleSprints_EXPECTED['start_committed']    = tu_.loadExpectedOutput('test_multipleSprints.start_committed')\n",
    "test_multipleSprints_EXPECTED['start_tasks']        = tu_.loadExpectedOutput('test_multipleSprints.start_tasks')\n",
    "test_multipleSprints_EXPECTED['end_committed']      = tu_.loadExpectedOutput('test_multipleSprints.end_committed')\n",
    "test_multipleSprints_EXPECTED['end_tasks']          = tu_.loadExpectedOutput('test_multipleSprints.end_tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                                          'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                        ['start_committed', 'end_committed'],\n",
    "                                        test_multipleSprints_EXPECTED,\n",
    "                                        test_multipleSprints_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent', 'Effort Remaining', \\\n",
    "                                          'Percent Achieved', '% Global Done'],\n",
    "                                        ['start_tasks', 'end_tasks'],\n",
    "                                        test_multipleSprints_EXPECTED,\n",
    "                                        test_multipleSprints_ACTUAL)\n",
    "ExpectedOutputCleaner.destringify(['Delivered in Sprint'], \n",
    "                                  ['start_tasks', 'end_tasks'], \n",
    "                                  test_multipleSprints_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_multipleSprints.start_committed']    = test_multipleSprints_EXPECTED['start_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.start_tasks']        = test_multipleSprints_EXPECTED['start_tasks']\n",
    "tu_.EXPECTED['test_multipleSprints.end_committed']      = test_multipleSprints_EXPECTED['end_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.end_tasks']          = test_multipleSprints_EXPECTED['end_tasks']\n",
    "\n",
    "tu_.ACTUAL['test_multipleSprints.start_committed']      = test_multipleSprints_ACTUAL['start_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.start_tasks']          = test_multipleSprints_ACTUAL['start_tasks']\n",
    "tu_.ACTUAL['test_multipleSprints.end_committed']        = test_multipleSprints_ACTUAL['end_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.end_tasks']            = test_multipleSprints_ACTUAL['end_tasks']\n",
    "\n",
    "tu_.testOK('test_multipleSprints.start_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.start_tasks'), \\\n",
    "tu_.testOK('test_multipleSprints.end_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.end_tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_committed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_tasks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_tasks'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_releaseCycle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "# Test logic\n",
    "def test_releaseCycle():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "\n",
    "\n",
    "    entries_df, log = timecard.runReleaseCycle(datetime(2018, 1, 15), SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)   \n",
    "     \n",
    "    burnout_df = timecard.releaseBurnout(entries_df)\n",
    "    \n",
    "    output['Entries'] = entries_df\n",
    "    output['Burnout'] = burnout_df\n",
    "\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        log_df = log.mergeLogs(name)\n",
    "        output[name] = log_df\n",
    "\n",
    "    output['log'] = log #Needed for visualizations\n",
    "    return output, modelsConfig\n",
    "\n",
    "# Run the test\n",
    "test_releaseCycle_ACTUAL, modelsConfig = test_releaseCycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_releaseCycle_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Entries'],    'test_releaseCycle.Entries')\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Burnout'],    'test_releaseCycle.Burnout')\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        tu_.createExpectedOutput(test_releaseCycle_ACTUAL[name],    'test_releaseCycle.' + name)\n",
    "\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_releaseCycle_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_releaseCycle_EXPECTED = {}\n",
    "\n",
    "test_releaseCycle_EXPECTED['Entries']      = tu_.loadExpectedOutput('test_releaseCycle.Entries')\n",
    "test_releaseCycle_EXPECTED['Burnout']      = tu_.loadExpectedOutput('test_releaseCycle.Burnout')\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    test_releaseCycle_EXPECTED[name]       = tu_.loadExpectedOutput('test_releaseCycle.' + name)\n",
    "\n",
    "# ReleaseLog snapshots have integer-valued columns (1, 2,3, ...), and loading the EXPECTED CSV file will convert\n",
    "# them to strings ('1', '2', '3', ...), so to avoid spurious test failures rename the columns of the\n",
    "# EXPECTED data we just loaded\n",
    "cols_to_align = ReleaseLog.SNAPSHOTS.copy()\n",
    "# These columns don't have integer-valued columns\n",
    "cols_to_align.remove('Resourcing')\n",
    "cols_to_align.remove('Outcome')\n",
    "cols_to_align \n",
    "ExpectedOutputCleaner.alignColumns(cols_to_align,\n",
    "                                  test_releaseCycle_EXPECTED,\n",
    "                                  test_releaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Time Spent'],\n",
    "                                        ['Entries'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Effort', 'Implementation Effort', 'Debugging Effort', 'Cum % Completion'],\n",
    "                                        ['Burnout'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "                                       \n",
    "testlets = ReleaseLog.SNAPSHOTS.copy()\n",
    "# All log snapshots except 'Resourcing' and 'Outcome' have the same sensitive fields\n",
    "testlets.remove('Resourcing') \n",
    "testlets.remove('Outcome') \n",
    "ExpectedOutputCleaner.cleanRoundingNoise([0,1,2,3,4,5,6,7,8,9,10],\n",
    "                                        testlets,\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', \\\n",
    "                                          'Bandwidth', 'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                         ['Resourcing'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent','Effort Remaining', \\\n",
    "                                          'Percent Achieved', '% Global Done'],\n",
    "                                         ['Outcome'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "                                    \n",
    "\n",
    "ExpectedOutputCleaner.standardizeDates(['Date'],\n",
    "                                      ['Entries'],\n",
    "                                      test_releaseCycle_EXPECTED)\n",
    "\n",
    "ExpectedOutputCleaner.destringify(['Delivered in Sprint'],\n",
    "                                 ['Outcome'],\n",
    "                                 test_releaseCycle_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_releaseCycle.Entries']        = test_releaseCycle_EXPECTED['Entries']\n",
    "tu_.EXPECTED['test_releaseCycle.Burnout']        = test_releaseCycle_EXPECTED['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.EXPECTED['test_releaseCycle.' + name]        = test_releaseCycle_EXPECTED[name]\n",
    "\n",
    "tu_.ACTUAL['test_releaseCycle.Entries']          = test_releaseCycle_ACTUAL['Entries']\n",
    "tu_.ACTUAL['test_releaseCycle.Burnout']          = test_releaseCycle_ACTUAL['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.ACTUAL['test_releaseCycle.' + name]        = test_releaseCycle_ACTUAL[name]\n",
    "\n",
    "results = []\n",
    "results.append(tu_.testOK('test_releaseCycle.Entries'))\n",
    "results.append(tu_.testOK('test_releaseCycle.Burnout'))\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    results.append(tu_.testOK('test_releaseCycle.' + name))\n",
    "                   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to interactively visualize the release logs, and then comment again once interactive analysis is done. Commenting these\n",
    "# lines after interactive analysis is completed is required as test harness can't load these visualiations\n",
    "# libraries so leaving this uncommented will crash the entire test harness.\n",
    "# NB: MAY NEED TO RUN TWICE (there is a bug in Jupyter notebook, I think, so first time you call this it shows no visuals)\n",
    "#import devanalyst.simulation.visualizations.timecard_visuals as tc_visuals\n",
    "#tc_visuals.renderReleaseCycleLog(teamId = 'Team A', release_log=test_releaseCycle_ACTUAL['log'], first=1, last=17, spurious_columns=['Team Id', 'Sprint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Script to debug test failures when entries change. This was very useful when the test failed because of simply the\n",
    "# removal of spurious entries (tails of 0's), for which the tu_.taintFailuresToStop was very useful. This allows\n",
    "# detecting the spurious rows and discard them, and at that point we have the same number of rows. Any mismatches\n",
    "# after that should be just innocent rotations of some entries, for which tu_.find_mismatches(e2, a) helps find the\n",
    "# few rows that account for the mismatches, and visual inspection usually is enough to verify it is simply an innocent\n",
    "# rotation\n",
    "def spurious_errors_debug(): # return or extract pertinent lines. This function is just a template\n",
    "    a = test_releaseCycle_ACTUAL['Entries']\n",
    "    e = test_releaseCycle_EXPECTED['Entries']\n",
    "    at = tu_.taintFailuresToStop(a, 'User Story', 'Time Spent')\n",
    "    et = tu_.taintFailuresToStop(e, 'User Story', 'Time Spent')\n",
    "    discard = list(et[et['TAINTED'] == True].index)\n",
    "    e2 = e.drop(discard)\n",
    "    e2.index = a.index\n",
    "    tu_.find_mismatches(e2, a)\n",
    "    e2.loc[[1178, 1182, 1184, 1216, 1218, 1220, 1232]]\n",
    "    a.loc[[1178, 1182, 1184, 1216, 1218, 1220, 1232]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Burnout'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Burnout'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Resourcing'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Resourcing'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_buggyReleaseCycle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "# Test logic\n",
    "def test_buggyReleaseCycle():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel(0.0)], [DistributedLagQualityModel()], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    \n",
    "\n",
    "    entries_df, log = timecard.runReleaseCycle(datetime(2018, 1, 15), SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)  \n",
    "    \n",
    "    stories_df = UserStory.build_stories_df(globalRepo)\n",
    "    bugs_df    = Ticket.build_bugs_df(globalRepo.ticketsRepo.tickets) \n",
    "    burnout_df = timecard.releaseBurnout(entries_df)\n",
    "    \n",
    "    output['Entries']       = entries_df\n",
    "    output['User_Stories']  = stories_df\n",
    "    output['Tickets']       = bugs_df\n",
    "    output['Burnout']       = burnout_df\n",
    "\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        log_df = log.mergeLogs(name)\n",
    "        output[name] = log_df\n",
    "\n",
    "    output['log'] = log #Needed for visualizations\n",
    "    return output, modelsConfig\n",
    "\n",
    "# Run the test\n",
    "test_buggyReleaseCycle_ACTUAL, modelsConfig = test_buggyReleaseCycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_buggyReleaseCycle_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_buggyReleaseCycle_ACTUAL['Entries'],         'test_buggyReleaseCycle.Entries')\n",
    "    tu_.createExpectedOutput(test_buggyReleaseCycle_ACTUAL['User_Stories'],    'test_buggyReleaseCycle.User_Stories')\n",
    "    tu_.createExpectedOutput(test_buggyReleaseCycle_ACTUAL['Tickets'],         'test_buggyReleaseCycle.Tickets')\n",
    "    tu_.createExpectedOutput(test_buggyReleaseCycle_ACTUAL['Burnout'],         'test_buggyReleaseCycle.Burnout')\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        tu_.createExpectedOutput(test_buggyReleaseCycle_ACTUAL[name],          'test_buggyReleaseCycle.' + name)\n",
    "\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_buggyReleaseCycle_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_buggyReleaseCycle_EXPECTED = {}\n",
    "\n",
    "list_cols_bugs = [] # Lists are loaded as strings, so require special processing on load\n",
    "list_cols_stories = ['Open Bugs', 'Closed Bugs']\n",
    "\n",
    "test_buggyReleaseCycle_EXPECTED['Entries']      = tu_.loadExpectedOutput('test_buggyReleaseCycle.Entries')\n",
    "test_buggyReleaseCycle_EXPECTED['User_Stories'] = tu_.loadExpectedOutput('test_buggyReleaseCycle.User_Stories',\n",
    "                                                                        list_cols_stories)\n",
    "test_buggyReleaseCycle_EXPECTED['Tickets']      = tu_.loadExpectedOutput('test_buggyReleaseCycle.Tickets',\n",
    "                                                                        list_cols_bugs)\n",
    "test_buggyReleaseCycle_EXPECTED['Burnout']      = tu_.loadExpectedOutput('test_buggyReleaseCycle.Burnout')\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    test_buggyReleaseCycle_EXPECTED[name]       = tu_.loadExpectedOutput('test_buggyReleaseCycle.' + name)\n",
    "\n",
    "# ReleaseLog snapshots have integer-valued columns (1, 2,3, ...), and loading the EXPECTED CSV file will convert\n",
    "# them to strings ('1', '2', '3', ...), so to avoid spurious test failures rename the columns of the\n",
    "# EXPECTED data we just loaded\n",
    "cols_to_align = ReleaseLog.SNAPSHOTS.copy()\n",
    "# These columns don't have integer-valued columns\n",
    "cols_to_align.remove('Resourcing')\n",
    "cols_to_align.remove('Outcome') \n",
    "\n",
    "ExpectedOutputCleaner.alignColumns(cols_to_align,\n",
    "                                  test_buggyReleaseCycle_EXPECTED,\n",
    "                                  test_buggyReleaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Time Spent'],\n",
    "                                        ['Entries'],\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Estimated Cost', 'Effort to Date', 'Percent Achieved'],\n",
    "                                        ['Tickets'],\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Effort', 'Implementation Effort', 'Debugging Effort', 'Cum % Completion'],\n",
    "                                        ['Burnout'],\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "                                       \n",
    "testlets = ReleaseLog.SNAPSHOTS.copy()\n",
    "# All log snapshots except 'Resourcing' and Outcome' have the same sensitive fields\n",
    "testlets.remove('Resourcing')\n",
    "testlets.remove('Outcome') \n",
    "ExpectedOutputCleaner.cleanRoundingNoise([0,1,2,3,4,5,6,7,8,9,10],\n",
    "                                        testlets,\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', \\\n",
    "                                          'Bandwidth', 'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                         ['Resourcing'],\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent','Effort Remaining', \\\n",
    "                                          'Percent Achieved', 'Global Estimate', '% Global Done'],\n",
    "                                         ['Outcome'],\n",
    "                                        test_buggyReleaseCycle_EXPECTED,\n",
    "                                        test_buggyReleaseCycle_ACTUAL)\n",
    "                                    \n",
    "\n",
    "ExpectedOutputCleaner.standardizeDates(['Date'],\n",
    "                                      ['Entries'],\n",
    "                                      test_buggyReleaseCycle_EXPECTED)\n",
    "\n",
    "\n",
    "ExpectedOutputCleaner.destringify(['Delivered in Sprint'],\n",
    "                                 ['Outcome'],\n",
    "                                 test_buggyReleaseCycle_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_buggyReleaseCycle.Entries']        = test_buggyReleaseCycle_EXPECTED['Entries']\n",
    "tu_.EXPECTED['test_buggyReleaseCycle.User_Stories']   = test_buggyReleaseCycle_EXPECTED['User_Stories']\n",
    "tu_.EXPECTED['test_buggyReleaseCycle.Tickets']        = test_buggyReleaseCycle_EXPECTED['Tickets']\n",
    "tu_.EXPECTED['test_buggyReleaseCycle.Burnout']        = test_buggyReleaseCycle_EXPECTED['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.EXPECTED['test_buggyReleaseCycle.' + name]    = test_buggyReleaseCycle_EXPECTED[name]\n",
    "\n",
    "tu_.ACTUAL['test_buggyReleaseCycle.Entries']          = test_buggyReleaseCycle_ACTUAL['Entries']\n",
    "tu_.ACTUAL['test_buggyReleaseCycle.User_Stories']     = test_buggyReleaseCycle_ACTUAL['User_Stories']\n",
    "tu_.ACTUAL['test_buggyReleaseCycle.Tickets']          = test_buggyReleaseCycle_ACTUAL['Tickets']\n",
    "tu_.ACTUAL['test_buggyReleaseCycle.Burnout']          = test_buggyReleaseCycle_ACTUAL['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.ACTUAL['test_buggyReleaseCycle.' + name]      = test_buggyReleaseCycle_ACTUAL[name]\n",
    "\n",
    "results = []\n",
    "results.append(tu_.testOK('test_buggyReleaseCycle.Entries'))\n",
    "results.append(tu_.testOK('test_buggyReleaseCycle.User_Stories'))\n",
    "results.append(tu_.testOK('test_buggyReleaseCycle.Tickets'))\n",
    "results.append(tu_.testOK('test_buggyReleaseCycle.Burnout'))\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    results.append(tu_.testOK('test_buggyReleaseCycle.' + name))\n",
    "                   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to interactively visualize the release logs, and then comment again once interactive analysis is done. Commenting these\n",
    "# lines after interactive analysis is completed is required as test harness can't load these visualiations\n",
    "# libraries so leaving this uncommented will crash the entire test harness.\n",
    "# NB: MAY NEED TO RUN TWICE (there is a bug in Jupyter notebook, I think, so first time you call this it shows no visuals)\n",
    "#import devanalyst.simulation.visualizations.timecard_visuals as tc_visuals\n",
    "#tc_visuals.renderReleaseCycleLog(teamId = 'Team A', release_log=test_buggyReleaseCycle_ACTUAL['log'], first=1, last=19, spurious_columns=['Team Id', 'Sprint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Script to debug test failures when entries change. This was very useful when the test failed because of simply the\n",
    "# removal of spurious entries (tails of 0's), for which the tu_.taintFailuresToStop was very useful. This allows\n",
    "# detecting the spurious rows and discard them, and at that point we have the same number of rows. Any mismatches\n",
    "# after that should be just innocent rotations of some entries, for which tu_.find_mismatches(e2, a) helps find the\n",
    "# few rows that account for the mismatches, and visual inspection usually is enough to verify it is simply an innocent\n",
    "# rotation\n",
    "def spurious_errors_debug(): # return or extract pertinent lines. This function is just a template\n",
    "    a = test_buggyReleaseCycle_ACTUAL['Entries']\n",
    "    e = test_buggyReleaseCycle_EXPECTED['Entries']\n",
    "    at = tu_.taintFailuresToStop(a, 'User Story', 'Time Spent')\n",
    "    et = tu_.taintFailuresToStop(e, 'User Story', 'Time Spent')\n",
    "    discard = list(et[et['TAINTED'] == True].index)\n",
    "    e2 = e.drop(discard)\n",
    "    e2.index = a.index\n",
    "    tu_.find_mismatches(e2, a)\n",
    "    e2.loc[[1178, 1182, 1184, 1216, 1218, 1220, 1232]]\n",
    "    a.loc[[1178, 1182, 1184, 1216, 1218, 1220, 1232]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['User_Stories'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['User_Stories'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['Tickets'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['Tickets'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['Burnout'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['Burnout'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['Resourcing'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['Resourcing'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_ACTUAL['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_buggyReleaseCycle_EXPECTED['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
