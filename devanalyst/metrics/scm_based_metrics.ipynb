{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Library to measure volatility actuals</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     16,
     45,
     57
    ]
   },
   "outputs": [],
   "source": [
    "# Produces a dataframe, with one column per snapshot. A snapshot is a pair [string, dataframe] where the first entry \n",
    "# is the name of the snapshot, and the second holds the data for the snapshot. \n",
    "#\n",
    "# The resulting dataframe has 2 levels of column index:\n",
    "#       -Top level column headers: '# Files', 'K Loc', '# Authors', '# Commits'\n",
    "#       -Second level column headers: the names of the snapshots\n",
    "#\n",
    "# The resulting dataframe aggregates the data of all snapshots, based on the 'by_col' field. Thus, the resulting\n",
    "# dataframe has one row for each value of the 'by_col' column in the snapshots.\n",
    "# \n",
    "# -snapshots: list of [string, dataframe] pairs. In order to do aggregation, the dataframes are requred to have \n",
    "#             these columns: 'Filename', 'Loc', 'Author(s) e-mail', 'CommitId(s)'\n",
    "#             Additionally, all dataframes must have the a column whose name matches the 'by_col' parameter.\n",
    "# -by_col: name of the column in the snapshots' dataframes by which stats are sought. For example, if 'Repo' \n",
    "#          is used then the stats will be broken by repo, i.e. the output dataframe will have one row per repo.\n",
    "#\n",
    "def buildSnapshotStats(snapshots, by_col):\n",
    "    result_dict = {}\n",
    "    \n",
    "    level_1_cols = ['# Files', 'K Loc', '# Authors', '# Commits']\n",
    "    level_2_cols = list(snapshots.keys())\n",
    "    \n",
    "    multicol = pd.MultiIndex.from_product([level_1_cols, level_2_cols])\n",
    "    \n",
    "    row_labels = set()\n",
    "    for snapshot in level_2_cols:\n",
    "        snapshot_df = snapshots[snapshot]\n",
    "        if by_col in snapshot_df.columns:\n",
    "            row_labels = row_labels.union(set(snapshot_df[by_col].unique()))\n",
    "    \n",
    "    all_rows = []\n",
    "    for label in row_labels:\n",
    "        row = []\n",
    "        _appendMetricByCounts('Filename'        , row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricBySumK  ('Loc'           , row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricByCounts('Author(s) e-mail', row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricByCounts('CommitId(s)'     , row, level_2_cols, snapshots, by_col, label)\n",
    "        \n",
    "        all_rows.append(row)\n",
    "            \n",
    "    stats_df = pd.DataFrame(all_rows, index=row_labels, columns=multicol)\n",
    "            \n",
    "    return stats_df\n",
    "\n",
    "# Mutates 'row' by appending the information for column 'col'. \n",
    "def _appendMetricByCounts(metric_source_col, row, snapshot_names, snapshots, by_col, row_label):\n",
    "    for snapshot in snapshot_names:\n",
    "        snapshot_df = snapshots[snapshot]\n",
    "        if by_col in snapshot_df.columns and metric_source_col in snapshot_df.columns:\n",
    "            df = snapshot_df[snapshot_df[by_col] == row_label]\n",
    "            val = df[metric_source_col].unique().size\n",
    "            row.append(val)\n",
    "        else:\n",
    "            val = None\n",
    "            row.append(val)\n",
    "        \n",
    "# Mutates 'row' by appending the information for column 'col'.\n",
    "def _appendMetricBySumK(metric_source_col, row, snapshot_names, snapshots, by_col, row_label):\n",
    "    for snapshot in snapshot_names:\n",
    "        snapshot_df = snapshots[snapshot]\n",
    "        if by_col in snapshot_df.columns and metric_source_col in snapshot_df.columns:\n",
    "            df = snapshot_df[snapshot_df[by_col] == row_label]\n",
    "            val = df[metric_source_col].sum()/1000\n",
    "            row.append(val)\n",
    "        else:\n",
    "            val = None\n",
    "            row.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "# Produced a dataframe, with one column per snapshot. A snapshot is a pair [string, dataframe] where the first entry is the\n",
    "# name of the snapshot, and the second holds the data for the snapshot (i.e., a subset of the rows of the kind of dataframe\n",
    "# produced by _build_ticket_df)\n",
    "#\n",
    "# -by_col: name of the column in the snapshots by which stats are sought. For example, if 'Repo' is used then the stats\n",
    "#      will be broken by repo.\n",
    "def buildGranularSnapshotStats(full_repo_df, snapshots, by_col):\n",
    "    result_dict = {}\n",
    "    \n",
    "    level_1_cols = ['# Files', 'K Loc in Repo', 'K Loc change', 'K Loc+', 'K Loc-', '# Authors', '# Commits']\n",
    "    level_2_cols = list(snapshots.keys())\n",
    "    \n",
    "    multicol = pd.MultiIndex.from_product([level_1_cols, level_2_cols])\n",
    "    \n",
    "    row_labels = set()\n",
    "    for snapshot in level_2_cols:\n",
    "        snapshot_df = snapshots[snapshot]\n",
    "        if by_col in snapshot_df.columns:\n",
    "            row_labels = row_labels.union(set(snapshot_df[by_col].unique()))\n",
    "    \n",
    "    all_rows = []\n",
    "    for label in row_labels:\n",
    "        row = []\n",
    "        _appendMetricByCounts('Filename'        , row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendRepoLocK      (full_repo_df,       row,                          by_col, label)\n",
    "        _appendMetricBySumK  ('Loc'             , row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricBySumK  ('Loc+'            , row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricBySumK  ('Loc-'            , row, level_2_cols, snapshots, by_col, label)\n",
    "        \n",
    "        _appendMetricByCounts('Author(s) e-mail', row, level_2_cols, snapshots, by_col, label)\n",
    "        _appendMetricByCounts('CommitId(s)'     , row, level_2_cols, snapshots, by_col, label)\n",
    "        \n",
    "        all_rows.append(row)\n",
    "            \n",
    "    stats_df = pd.DataFrame(all_rows, index=row_labels, columns=multicol)\n",
    "    \n",
    "    for snapshot in level_2_cols:\n",
    "        multi_col = ('Churn', snapshot)\n",
    "        stats_df[multi_col] = stats_df[('K Loc change', snapshot)] / stats_df[('K Loc in Repo', snapshot)]\n",
    "    \n",
    "    stats_df = stats_df.replace(float('Inf'), 0)\n",
    "    return stats_df\n",
    "\n",
    "def _appendRepoLocK(full_repo_df, row, by_col, row_label):\n",
    "    source_col = 'Loc'\n",
    "    if by_col in full_repo_df.columns and source_col in full_repo_df.columns:\n",
    "        df = full_repo_df[full_repo_df[by_col] == row_label]\n",
    "        row.append(df[source_col].sum()/1000)\n",
    "    else:\n",
    "        row.append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Library to drill into ticket activity</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe where there is row for each ticket in the given 'ticket_family'. The returned dataframe\n",
    "# summarizes basic information about the activity for each ticket, such as the number of files, submodules,\n",
    "# packages, and artifact families involved in all the commits for the given ticket.\n",
    "#\n",
    "# -global_df: a dataframe of commit activity such as what is produced by Git_Analyzer.build_ticket_df\n",
    "# -ticket_family: a string, corresponding to name of the ticket family for which we seek information.\n",
    "#\n",
    "def findTicketFamilyStats(global_df, ticket_family):\n",
    "    df = global_df[global_df['Ticket(s) Family']==ticket_family]   \n",
    "    df_byticket = df.groupby('Ticket(s)').apply(_collapseByTicket)\n",
    "    df_byticket = df_byticket.sort_values(by='# Files', ascending=False)\n",
    "    df_byticket = df_byticket.reset_index()\n",
    "    df_byticket = df_byticket.drop('level_1', axis=1)\n",
    "    return df_byticket\n",
    "    \n",
    "def _collapseByTicket(df):\n",
    "    result_dict = {}\n",
    "    result_dict['Subject'] = [df['Subject'].iloc[0]]\n",
    "    result_dict['# Files'] = [df['Filename'].unique().size]\n",
    "    result_dict['Filenames'] = [list(df['Filename'].unique())]\n",
    "    result_dict['# Submodules'] = [df['Submodule'].unique().size]\n",
    "    result_dict['Artifact Families'] = [list(df['Artifact Family'].unique())]\n",
    "    result_dict['Repo(s)'] = [list(df['Repo'].unique())]\n",
    "    result_dict['Author(s) e-mail'] = [list(df['Author(s) e-mail'].unique())]\n",
    "    result_dict['# Packages'] = [df['Package'].unique().size]\n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     10,
     17
    ]
   },
   "outputs": [],
   "source": [
    "# Produces a dataframe with exactly one row per ticket in 'full_ticket_df'. The row provides aggregated information\n",
    "# about the commits associated to that ticket, such as:\n",
    "#    * the number of repos, commits, files, authors, submodules, packages, artifact families, loc, and dates\n",
    "#\n",
    "# -full_ticket_df: dataframe of the kind that is produced by the 'build_ticket_df' function. In particular, it\n",
    "#  is expected to have columns called:\n",
    "#\n",
    "#     * 'Ticket(s)', 'Repo', 'CommitId(s)', 'Filename', 'Author(s) e-mail', 'Submodule', 'Package',\n",
    "#       'Artifact Family', 'Loc', 'Loc+', 'Loc-', 'Author(s) date'\n",
    "#\n",
    "def buildUserStoryScope(full_ticket_df):\n",
    "    story_scope = full_ticket_df.groupby(['Ticket(s)']).apply(_computeStoryScope)\n",
    "    story_scope = story_scope.reset_index()\n",
    "    story_scope = story_scope.drop(['level_1'], axis=1)\n",
    "    return story_scope\n",
    "\n",
    "# Helper method invoked by the buildUserStoryScope function\n",
    "def _computeStoryScope(df):\n",
    "    result_dict = {}\n",
    "    result_dict['# Repos']               = [len(df['Repo'].unique())]\n",
    "    result_dict['# Commits']             = [len(df['CommitId(s)'].unique())]\n",
    "    result_dict['# Files']               = [len(df['Filename'].unique())]\n",
    "    result_dict['# Authors']             = [len(df['Author(s) e-mail'].unique())]\n",
    "    result_dict['# Submodules']          = [len(df['Submodule'].unique())]\n",
    "    result_dict['# Packages']            = [len(df['Package'].unique())]\n",
    "    result_dict['# Artifact Families']   = [len(df['Artifact Family'].unique())]\n",
    "    result_dict['Loc']                   = [df['Loc'].sum()]\n",
    "    result_dict['Loc+']                  = [df['Loc+'].sum()]\n",
    "    result_dict['Loc-']                  = [df['Loc-'].sum()]\n",
    "    \n",
    "    date_str_min                         = df['Author(s) date'].min()\n",
    "    date_str_max                         = df['Author(s) date'].max()\n",
    "    result_dict['First Commit']          = [date_str_min]\n",
    "    result_dict['Last Commit']           = [date_str_max]\n",
    "    \n",
    "    dt_min = datetime.strptime(date_str_min, '%Y-%m-%d')\n",
    "    dt_max = datetime.strptime(date_str_max, '%Y-%m-%d')\n",
    "    \n",
    "    duration = (dt_max - dt_min).days\n",
    "\n",
    "    result_dict['Duration (days)']     = [duration]\n",
    "    \n",
    "    return pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Library of DSM (Design Structure Matrix) utilities</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     6,
     120,
     124
    ]
   },
   "outputs": [],
   "source": [
    "#  Returns two dictionaries of 'square dataframes' (dataframes whose row and column index are the same). Each\n",
    "#  dataframe basically representing a dependency matrix (DSM) for different measures of interference. \n",
    "#  The difference between both dictionaries is that the first one has aggregated values (sums or counts)\n",
    "#  and the second one's values are drill-down information for the count-based aggregations of the first: the\n",
    "#  list of elements that were counted.\n",
    "#\n",
    "def computeInterferenceDSM(data_df, interference_col, breakout_col, \n",
    "                                   aggregate_by_sum, aggregate_by_count, filters):\n",
    "#  **** EXPLANATION OF FIRST DICTIONARY RETURNED ****\n",
    "#\n",
    "#  All dataframes inside this returned dictionary have the same row and column headers, and differ only on \n",
    "#  the meaning of the values in them.\n",
    "#  There is one dataframe for each aggregation metric in the 'aggregate_by_sum' and 'aggregate_by_count' parameters.\n",
    "#\n",
    "#  EXAMPLE: Suppose you set:\n",
    "#              breakout_col       = 'Repo'\n",
    "#              interference_col   = 'Author(s) e-mail'\n",
    "#              aggregate_by_sum   = ['Loc']\n",
    "#              aggregate_by_count = ['Filename', 'CommitId(s)']\n",
    "#              filters            = {'Artifact Type': ['java']}\n",
    "#\n",
    "# This will produce a dictionary 'dict' with 3 entries, with keys 'Loc', 'Filename', 'CommitIds' and the values\n",
    "# will be square dataframes whose row and column headers are the repos. 'dict['Loc'][x][y]' would contain\n",
    "# the number of lines of *java* code contributed to repo Y by the team allocated to repo X. I.e., it shows the \n",
    "# \"interference\" that team X has on a repo that is presumably handled by what should be a different team if\n",
    "# interference were 0. Aggregation is only for *java* code because of the 'filters' parameter.\n",
    "#     \n",
    "#  **** EXPLANATION OF SECOND DICTIONARY RETURNED ****\n",
    "# \n",
    "#  This dictionary is similar to the first, except that it only has dataframes for the 'aggregate_by_count'\n",
    "#  columns, and the values are not the counts, but a list of the elements that were counted.\n",
    "#\n",
    "#  **** PARAMETERS ****\n",
    "#\n",
    "# -data_df: a dataframe, that must include columns named the same as the 'breakout_col' and 'interference_col'\n",
    "#           parameters, as well as columns named after the strings in the 'aggregate_by_sum' and \n",
    "#           'aggregate_by_count' parameters. In the case of the columns in 'aggregate_by_sum', values in \n",
    "#           data_df must be numbers so that they can be added.\n",
    "# -breakout_col: name of column in 'data_df' used to partition data into groups (the x and y axis of the DSM)\n",
    "# -interference_col: name of column in 'data_df' whose values, if aggregated per 'breakout_col', are intersected\n",
    "#                    to determine where there is interference. \n",
    "#                    => GOTCHA ALERT: The values in this column must be either strings or lists of strings.\n",
    "# -aggregate_by_sum: a list of column names for 'data_df' that should be aggregated by adding their values.\n",
    "#                    => GOTCHA ALERT: If the values under 'interference_col' are lists, this option is disabled.\n",
    "# -aggregate_by_count: a list of column names for 'data_df' that should be aggregated by adding their values\n",
    "#                    => GOTCHA ALERT: If the values under 'interference_col' are lists, the only column for \n",
    "#                                     which aggregation is allowed is the 'interference_col' itself. Reason:\n",
    "#                                     when interfering by collections the interference is not a binary\n",
    "#                                     determination (am I in the set of interfers?) but a messy partial overlap\n",
    "#                                     (am I intersecting some interfers?)\n",
    "# -filters: a possibly empty dictionary whose keys are among the column names of 'data_df', and whose values\n",
    "#           are a list of values for such column that restrict which portion of 'data_df' to aggregate.\n",
    "#\n",
    "    #projects_df = data_df.groupby(breakout_col).apply(lambda row: row[interference_col].unique().size)\n",
    "    projects_df = data_df.groupby(breakout_col).apply(_collapseInterferenceCol, interference_col=interference_col)\n",
    "    \n",
    "    projects = list(projects_df.sort_values(ascending=False).index)\n",
    "    \n",
    "    x_ticks                     = projects\n",
    "    y_ticks                     = projects\n",
    "    main_working_dict           = {} # dict of dicts, from which a dict #1 of dataframes will be built\n",
    "    drilldown_working_dict      = {} # dict of dicts, from which a dict #2 of dataframes will be built\n",
    "    for metric in aggregate_by_sum:\n",
    "        main_working_dict[metric]         = {} # Columns will be the x_ticks and the row index will the y_ticks\n",
    "    for metric in aggregate_by_count:\n",
    "        main_working_dict[metric]         = {} # Columns will be the x_ticks and the row index will the y_ticks\n",
    "        drilldown_working_dict[metric]    = {} # Columns will be the x_ticks and the row index will the y_ticks\n",
    "    \n",
    "    result_columns = []\n",
    "    for x in x_ticks:\n",
    "        colx                                   = 'By ' + x\n",
    "        result_columns                         .append(colx)\n",
    "        for metric in aggregate_by_sum:\n",
    "            main_working_dict[metric][colx]         = []\n",
    "        for metric in aggregate_by_count:\n",
    "            main_working_dict[metric][colx]         = []\n",
    "            drilldown_working_dict[metric][colx]    = []\n",
    "\n",
    "        for y in y_ticks:\n",
    "            dfx                                = data_df[data_df[breakout_col]==x]            \n",
    "            #x_interferers                      = list(dfx[interference_col].unique())\n",
    "            x_interferers, interference_type   = _getUniqueValuesOfCol(dfx, interference_col)\n",
    "            \n",
    "            dfy                                = data_df[data_df[breakout_col]==y]\n",
    "            if filters != None:\n",
    "                for filter_col in filters.keys():\n",
    "                    dfy = dfy[dfy[filter_col].isin(filters[filter_col])]\n",
    "            \n",
    "            # Interference from x in onto y\n",
    "            #dfy_interfererx                     = dfy[dfy[interference_col].isin(x_interferers)]\n",
    "            for metric in aggregate_by_sum: \n",
    "                if interference_type == list: #This should never happen if caller read the documentation\n",
    "                    assert(0==1)\n",
    "                dfy_interfererx                     = dfy[dfy[interference_col].isin(x_interferers)]\n",
    "                main_working_dict[metric][colx]      .append(dfy_interfererx[metric].sum())\n",
    "            for metric in aggregate_by_count:   \n",
    "                if interference_type == str:\n",
    "                    dfy_interfererx                 = dfy[dfy[interference_col].isin(x_interferers)]\n",
    "                    things_to_count = list(dfy_interfererx[metric].sort_values().unique())\n",
    "                else: #Then interference_type = list and metric = interference_col\n",
    "                    if interference_type == list and metric != interference_col: \n",
    "                        #This should never happen if caller read the documentation\n",
    "                        assert(0==1)\n",
    "                    things_to_count = _get_XY_intersection(dfy, metric, x_interferers)\n",
    "                main_working_dict[metric][colx]      .append(len(things_to_count))            \n",
    "                drilldown_working_dict[metric][colx] .append(things_to_count)            \n",
    "    \n",
    "    main_dict                   = {} # dict of DataFrames \n",
    "    drilldown_dict              = {} # dict of DataFrames \n",
    "    for metric in aggregate_by_sum:   \n",
    "        main_dict[metric]       = pd.DataFrame(main_working_dict[metric],         \n",
    "                                               index=y_ticks, columns=result_columns) \n",
    "    for metric in aggregate_by_count:   \n",
    "        main_dict[metric]       = pd.DataFrame(main_working_dict[metric],         \n",
    "                                               index=y_ticks, columns=result_columns) \n",
    "        drilldown_dict[metric]  = pd.DataFrame(drilldown_working_dict[metric],         \n",
    "                                               index=y_ticks, columns=result_columns) \n",
    "\n",
    "    return main_dict, drilldown_dict\n",
    "\n",
    "def _collapseInterferenceCol(df, interference_col):\n",
    "    interference_values, interference_type = _getUniqueValuesOfCol(df, interference_col)\n",
    "    return len(interference_values)\n",
    "\n",
    "def _getUniqueValuesOfCol(df, col):\n",
    "    interference_type = str\n",
    "    values = set()\n",
    "    for index, row in df.iterrows():\n",
    "        x = row[col]\n",
    "        \n",
    "        # Clean the data in case we loaded an empty string as a nan. Treat it like an empty string, or else\n",
    "        # subsequent code will think it is a float and throw an assertion for 'Unsupported type...'\n",
    "        if type(x) == float and math.isnan(x):\n",
    "            x = ''\n",
    "        if type(x) == str:\n",
    "            values.add(x)\n",
    "        else:\n",
    "            if type(x) == list:\n",
    "                values = values.union(set(x))\n",
    "                interference_type = list\n",
    "            else: #Don't support other forms of interference, should never get here\n",
    "                assert 0 == 1, 'Unsupported type ' + str(type(x)) + ' at row index = ' + str(index)\n",
    "    return list(values), interference_type\n",
    "\n",
    "def _get_XY_intersection(dfy, col, x_interferers):\n",
    "    result = set()\n",
    "    x_set = set(x_interferers)\n",
    "    for index, row in dfy.iterrows():\n",
    "        y_set = set(row[col])\n",
    "        intersection = x_set.intersection(y_set)\n",
    "        result = result.union(intersection)\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
