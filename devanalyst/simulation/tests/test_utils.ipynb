{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, io\n",
    "from io import StringIO\n",
    "import pkg_resources\n",
    "import ast\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Link Code to be Tested</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.businessObjects as bo\n",
    "from devanalyst.simulation.businessObjects import UserStoriesRepo, TicketsRepo, ScrumTeamsRepo\n",
    "import devanalyst.simulation.generateTimecards as timecard\n",
    "from devanalyst.simulation.generateTimecards import IdCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Common Context Across Tests</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Immutable Globals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#globals used in this test. After they are initialized in 'loadTestResources()', they don't change.\n",
    "PM_DF = None\n",
    "DEV_DF = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mutable Globals</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Global variable used to have a single counter for user stories id as they get generated in multiple calls. \n",
    "# Mutable state\n",
    "NEXT_USER_STORY_ID = None\n",
    "\n",
    "\n",
    "# Dictioaries of test results. Each key is the string name of a test and the value is the output of that test,\n",
    "# expected or actual\n",
    "EXPECTED = {}\n",
    "ACTUAL = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Utilities</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def initTestData(developers_df, productManagers_df, releaseDuration, sprintDuration, modelsConfig):\n",
    "# Test the code for creating user stories by constructing them and arranging them in a dataframe to display and evidence\n",
    "# visually that the code works as it should\n",
    "\n",
    "    global NEXT_USER_STORY_ID\n",
    "    storiesRepo = UserStoriesRepo([])\n",
    "    teamsRepo = ScrumTeamsRepo([])\n",
    "    ticketsRepo = TicketsRepo()\n",
    "    NEXT_USER_STORY_ID = IdCounter()\n",
    "    #cols = ['User Story Id','Scrum Team', 'Product Manager', 'Developer', 'Estimate',]\n",
    "    userStoryId_vals = []\n",
    "    scrumTeam_vals = []\n",
    "    developer_vals = []\n",
    "    productManager_vals = []\n",
    "    estimate_vals = []\n",
    "    \n",
    "    teams_df = bo.createTeamsDF(developers_df, productManagers_df)\n",
    "    \n",
    "    for team in teams_df['Scrum Team']:\n",
    "        stories, backlog = timecard.createUserStoryBacklog(team, releaseDuration, sprintDuration, \n",
    "                                                           NEXT_USER_STORY_ID, modelsConfig)\n",
    "        storiesRepo.stories.extend(stories)\n",
    "        teamsRepo.teams.append(team)\n",
    "        for story in stories:\n",
    "            scrumTeam_vals.append(team.teamId)\n",
    "            userStoryId_vals.append(story.userStoryId)\n",
    "            developer_vals.append(story.developer)\n",
    "            productManager_vals.append(story.productManager)\n",
    "            estimate_vals.append(story.originalEstimate)\n",
    "        team.backlog = backlog\n",
    "    stories_dict = {'User Story Id': userStoryId_vals, 'Scrum Team': scrumTeam_vals, 'Product Manager':productManager_vals, \\\n",
    "                'Developer': developer_vals, 'Estimate': estimate_vals}\n",
    "    return teams_df, pd.DataFrame(stories_dict), teamsRepo, storiesRepo, ticketsRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def loadTestResources():\n",
    "    global PM_DF\n",
    "    global DEV_DF\n",
    "    resource_path = '/'.join(('Resources', 'Simulation', 'Team.xlsx'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    PM_DF = pd.read_excel(fullpath, 'PMs')\n",
    "    DEV_DF = pd.read_excel(fullpath, 'Dev')\n",
    "\n",
    "loadTestResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def createExpectedOutput(expected_df, testName):\n",
    "    resource_path = '/'.join(('Resources', 'Tests','Simulation', testName + '_EXPECTED.csv'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    return expected_df.to_csv(fullpath)\n",
    "\n",
    "def loadExpectedOutput(testName, literal_cols=[]):\n",
    "    resource_path = '/'.join(('Resources', 'Tests','Simulation', testName + '_EXPECTED.csv'))\n",
    "    resource_package = 'devanalyst'\n",
    "    fullpath = pkg_resources.resource_filename(resource_package, resource_path)\n",
    "    df = pd.read_csv(fullpath)\n",
    "    \n",
    "    spurious_col = 'Unnamed: 0'\n",
    "    if spurious_col in df.columns:\n",
    "        df = df.drop([spurious_col], axis = 'columns') # Remove spurious index column, if any\n",
    "    \n",
    "    # Now for each column in list_cols, we have to treat it like it is a list so replace the contents of that\n",
    "    # column with a parsing of the string value to to a list value\n",
    "    for col in literal_cols:\n",
    "        df[col] = df[col].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def matches(actual, expected):\n",
    "    if isinstance(actual, str):\n",
    "        return actual==expected\n",
    "    else:\n",
    "        return actual.equals(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mismatchButOK(old_df, new_df, new_cols):\n",
    "# Verifies that two dataframes are different only with regards to new columns added to the newest dataframe\n",
    "    df = new_df.copy()\n",
    "    return (not old_df.equals(new_df) and old_df.equals(df.drop(columns = new_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mismatches(df1, df2):\n",
    "# For two dataframes with equivalent indices and columns, it returns the list of row indexes in for rows that\n",
    "# are different between df1 and df2\n",
    "    bad_idx = []\n",
    "    for idx, rows in df1.iterrows():\n",
    "        if not df1.loc[idx].equals(df2.loc[idx]):\n",
    "            bad_idx.append(idx)\n",
    "    return bad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def testOK(testname):\n",
    "    return matches(ACTUAL[testname], EXPECTED[testname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     20
    ]
   },
   "outputs": [],
   "source": [
    "class ExpectedOutputCleaner:\n",
    "    # Class containing utilities to clean the expected output when it is loaded from a CSV file. The process of\n",
    "    # saving a dataframe to CSV and then loading it results in some cosmetic changes that would cause spurious\n",
    "    # test failures if they are not cleaned.\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    # Float-valued fields may have many decimal places that get truncated when saving or loading a CSV file.\n",
    "    # To avoid spurious errors, round all values to just a few decimal places that are not part of the truncation\n",
    "    def cleanRoundingNoise(sensitive_fields, testlets, test_EXPECTED, test_ACTUAL):\n",
    "        for field in sensitive_fields:\n",
    "            for testlet in testlets:\n",
    "                rounded = test_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "                test_EXPECTED[testlet][field] = rounded\n",
    "                rounded = test_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "                test_ACTUAL[testlet][field] = rounded\n",
    "                \n",
    "    # Dates are loaded as strings, not pd.Timestamps, so to avoid spurious mismatches between ACTUAL (which represents dates\n",
    "    # as pd.Timestamps) and EXPECTED, convert the EXPECTED dates into pd.Timestamps\n",
    "    def standardizeDates(date_fields, testlets, test_EXPECTED):\n",
    "        for field in date_fields:\n",
    "            for testlet in testlets:        \n",
    "                d = test_EXPECTED[testlet][field]\n",
    "                test_EXPECTED[testlet][field] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "                \n",
    "    def alignColumns(testlets, test_EXPECTED, test_ACTUAL):\n",
    "        for testlet in testlets:\n",
    "            test_EXPECTED[testlet].columns  = test_ACTUAL[testlet].columns\n",
    "            \n",
    "    def alignIndex(testlets, test_EXPECTED, test_ACTUAL):\n",
    "        for testlet in testlets:\n",
    "            test_EXPECTED[testlet].index  = test_ACTUAL[testlet].index\n",
    "            \n",
    "    # Used for columns where some values are non-negative integers, and others are strings. If the non-negative \n",
    "    # integers are represented as strings, this method will replace them with the corresponding non-negative integer\n",
    "    # value. Other values in the column are left alone\n",
    "    def destringify(fields, testlets, test_EXPECTED):\n",
    "        for field in fields:\n",
    "            for testlet in testlets:\n",
    "                original = test_EXPECTED[testlet][field]\n",
    "                test_EXPECTED[testlet][field] = original.apply(lambda x: x if not str.isdigit(x) else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects errors in a dataframe that is supposed to have a tail of 0 values for the 'finishing_line_column' when\n",
    "# grouped by the 'grouping_column'\n",
    "#\n",
    "# For example, a timecard that is grouped by user story id should not have a tail of 0's for the effort spent:\n",
    "# if timecard generation is working properly, once the user story is completed then there should not be additional\n",
    "# spurious entries for the user story, all with 0 for effort spent (since no more effort is required, since the\n",
    "# user story is completed). This is useful to detect anomalies in timecards, as may happen if rounding errors make\n",
    "# it appear that there is an epsilon amount of work still left, so the user story keeps re-appearing in the timecard\n",
    "# entries, but with 0 time entered against it. If that happens, it is a bug as timecard generation should not leave\n",
    "# spurious epsilon-sized work for the future. In those cases, this method is useful to debug because it produces a\n",
    "# a dataframe with a 'TAINTED' column which is TRUE for rows that have such spurious 0's.\n",
    "def taintFailuresToStop(original_df, grouping_column, finishing_line_column):\n",
    "    tainted_df = original_df.groupby(grouping_column).apply(_detect, finishing_line_column=finishing_line_column)\n",
    "    return tainted_df\n",
    "\n",
    "# Helper method used in taintFailuresToStop\n",
    "def _detect(df, finishing_line_column):\n",
    "    df['TAINTED'] = False\n",
    "    df['PRIOR'] = None\n",
    "    df['ACCUM'] = 0.0\n",
    "    prior = None\n",
    "    accum = 0.0\n",
    "    last_good_idx = -1\n",
    "    for idx in df.index:\n",
    "        extra = df.loc[idx, finishing_line_column]\n",
    "        if extra > 0.0:\n",
    "            last_good_idx = idx\n",
    "\n",
    "    for idx in df.index:\n",
    "        df.loc[idx, 'PRIOR'] = prior\n",
    "        if idx > last_good_idx:\n",
    "            df.loc[idx, 'TAINTED'] = True\n",
    "        prior = idx\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
