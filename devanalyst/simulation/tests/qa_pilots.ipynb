{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import math\n",
    "#from random import choices\n",
    "#import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap\n",
    "#import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.statics as S_\n",
    "import devanalyst.simulation.generateTimecards as timecard\n",
    "from devanalyst.simulation.generateTimecards import WorkAssignments\n",
    "from devanalyst.simulation.simulationModels import BalancedAllocationModel, GreedyAllocationModel, ModelsConfig, \\\n",
    "Distribution, DefaultCostModel, QualityModel, ReleaseCycleContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.tests.test_utils as tu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.visualizations.simm_visuals as simm_visuals\n",
    "import devanalyst.simulation.visualizations.timecard_visuals as tc_visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pilots</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bug inflow prototypes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedLagQualityModel(QualityModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistributedLagQualityModel, self).__init__()\n",
    "        return\n",
    "    \n",
    "    # Returns a list of Ticket instances.\n",
    "    # -context: a ReleaseCycleContext instance, providing run-time environmental conditions about the particular\n",
    "    # release cycle (e.g., which sprint, history that led to it, etc.) in which the model is being used\n",
    "    def findBugs(self, context): \n",
    "        uss_list = DistributedLagQualityModel._findFinishedStories(context)\n",
    "        \n",
    "        bugs = []\n",
    "        current_sprint = context.sprint\n",
    "        for uss in uss_list:\n",
    "            delivery_sprint = uss.sprintDelivered\n",
    "            lag = current_sprint - delivery_sprint\n",
    "            story = context.StoriesRepo.findStory(uss.userStory)\n",
    "            story_bugs = DistributedLagQualityModel._findBugsInStory(lag, story)\n",
    "            bugs.extend(story_bugs)\n",
    "        return bugs\n",
    "\n",
    "    # Returns a list of UserStoryStatus instances, corresponding to all user stories that have been finished.\n",
    "    def _findFinishedStories(context):\n",
    "        # TODO\n",
    "        return []\n",
    "    \n",
    "    # Returns a list of Ticket instances\n",
    "    def _findBugsInStory(lag, userStory):\n",
    "        # TODO\n",
    "        return []\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def genReleaseCycleSheets(modelsConfig):\n",
    "    RELEASE_DURATION = 60\n",
    "    SPRINT_DURATION = 10\n",
    "    \n",
    "    tu_.loadTestResources()\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, \\\n",
    "                                                                                           tu_.PM_DF, \\\n",
    "                                                                                           RELEASE_DURATION, \\\n",
    "                                                                                           SPRINT_DURATION, \\\n",
    "                                                                                          modelsConfig)\n",
    "    \n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    entries_df, releaseLog = timecard.runReleaseCycle(teamsRepo, ticketsRepo, storiesRepo, datetime(2018, 1, 15), \\\n",
    "                                                   SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)    \n",
    "    \n",
    "    return entries_df, releaseLog, teamsRepo, storiesRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPRINT_DURATION = 10\n",
    "modelsConfig = ModelsConfig([DefaultCostModel(0.25)], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "modelsConfig.random.reset(271)\n",
    "entries_df, releaseLog, TEAMS_REPO, STORIES_REPO = genReleaseCycleSheets(modelsConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_visuals.renderReleaseCycleLog('Team A', releaseLog, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_df = releaseLog.log['Team A'][1]['planned_End_CURRENT_SPRINT']\n",
    "end_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Burnout in release cycle - Pilot</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def genReleaseCycle(modelsConfig):\n",
    "    RELEASE_DURATION = 60\n",
    "    SPRINT_DURATION = 10\n",
    "    \n",
    "    tu_.loadTestResources()\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, \\\n",
    "                                                                                           tu_.PM_DF, \\\n",
    "                                                                                           RELEASE_DURATION, \\\n",
    "                                                                                           SPRINT_DURATION, \\\n",
    "                                                                                          modelsConfig)\n",
    "    \n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    entries_df, worksheets = timecard.runReleaseCycle(teamsRepo, ticketsRepo, storiesRepo, datetime(2018, 1, 15), \\\n",
    "                                                   SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)    \n",
    "    \n",
    "    return entries_df, worksheets, storiesRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genBurnout(entries_df, storiesRepo):\n",
    "    bystory = entries_df.groupby('User Story')\n",
    "    u = bystory.apply(storyInfo, storiesRepo)\n",
    "    \n",
    "    \n",
    "    u = u.reset_index()\n",
    "    u.drop(['level_1'], axis='columns', inplace=True)\n",
    "    s = u.groupby('Final Sprint')\n",
    "    burnout = s.apply(sprintInfo)\n",
    "    burnout = burnout.reset_index()\n",
    "    burnout.drop(['level_1'], axis='columns', inplace=True)\n",
    "    \n",
    "\n",
    "    return burnout, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storyInfo(group_df, storiesRepo):\n",
    "    info = {}\n",
    "    \n",
    "    #Go back, as min['Date'] is end of first sprint\n",
    "    info['Start'] = [timecard.subtractBusinessDays(group_df['Date'].min(), SPRINT_DURATION)] \n",
    "    info['End'] = [group_df['Date'].max()]\n",
    "    info['Elapsed Time'] = (info['End'][0] - info['Start'][0]).days\n",
    "    info['Effort'] = group_df['Time Spent'].sum()\n",
    "    info['Initial Sprint'] = [group_df['Sprint'].min()]\n",
    "    info['Final Sprint'] = [group_df['Sprint'].max()]  \n",
    "    '''\n",
    "    userStoryId = group_df['User Story']\n",
    "    userStory = storiesRepo.findStory(userStoryId)\n",
    "    info['Original estimate'] = userStory.originalEstimate    \n",
    "    '''\n",
    "    \n",
    "    return pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprintInfo(group_df):\n",
    "    info = {}\n",
    "    \n",
    "    info['Effort'] = group_df['Effort'].sum()\n",
    "    info['Avg Effort'] = group_df['Effort'].mean()\n",
    "    info['Stories Completed'] = [group_df['User Story'].count()]\n",
    "    info['Over 1 sprint'] = [group_df[group_df['Elapsed Time']==14]['User Story'].count()]\n",
    "    info['Over 2 sprint'] = [group_df[group_df['Elapsed Time']==28]['User Story'].count()]\n",
    "    info['Over 3 sprint'] = [group_df[group_df['Elapsed Time']==42]['User Story'].count()]\n",
    "    info['Over 4 sprint'] = [group_df[group_df['Elapsed Time']==56]['User Story'].count()]\n",
    " \n",
    "    \n",
    "    return pd.DataFrame(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPRINT_DURATION = 10\n",
    "modelsConfig1 = ModelsConfig([DefaultCostModel(0.0)], [], BalancedAllocationModel(SPRINT_DURATION)) \n",
    "modelsConfig1.random.reset(271)\n",
    "entries_df1, worksheets1, storiesRepo = genReleaseCycle(modelsConfig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn1, u1 = genBurnout(entries_df1, storiesRepo)\n",
    "burn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPRINT_DURATION = 10\n",
    "modelsConfig2 = ModelsConfig([DefaultCostModel(0.0)], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "modelsConfig2.random.reset(271)\n",
    "entries_df2, worksheets2, storiesRepo = genReleaseCycle(modelsConfig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn2, u2 = genBurnout(entries_df2, storiesRepo)\n",
    "burn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn1['Effort'].plot(label = 'Balanced'), burn2['Effort'].plot(label = 'Greedy'), plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn1['Avg Effort'].plot(label = 'Balanced'), burn2['Avg Effort'].plot(label = 'Greedy'), plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10 = u1[u1['Final Sprint'] == 10]\n",
    "u1_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10[u1_10['Initial Sprint'] == 10]['Effort'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10['Effort'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10[u1_10['Initial Sprint'] == 10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10[u1_10['Initial Sprint'] == 9]['Effort'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_10[u1_10['Initial Sprint'] == 9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
