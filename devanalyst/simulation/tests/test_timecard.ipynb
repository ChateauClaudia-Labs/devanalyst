{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "from colorama import Fore, Back, Style \n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Code to be Tested</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\statics.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\businessObjects.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\simulationModels.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\generateTimecards.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.simulation.statics as S_\n",
    "\n",
    "import devanalyst.simulation.businessObjects as bo\n",
    "from devanalyst.simulation.businessObjects import UserStory, UserStoriesRepo, Ticket, TicketsRepo, WorkItem, \\\n",
    "UserStoryStatus, Backlog, ScrumTeam, ScrumTeamsRepo\n",
    "\n",
    "from devanalyst.simulation.simulationModels import ModelsConfig, DefaultCostModel, GreedyAllocationModel, ReleaseCycleContext\n",
    "\n",
    "import devanalyst.simulation.generateTimecards as timecard\n",
    "from devanalyst.simulation.generateTimecards import IdCounter, WorkAssignments, ReleaseLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import devanalyst.simulation.tests.test_utils as tu_\n",
    "from devanalyst.simulation.tests.test_utils import ExpectedOutputCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_uss</h1>\n",
    "<p>Test User Story Status</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     10,
     16,
     23
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "# Helper methods to the test\n",
    "def format_ticket(ticket):\n",
    "    return ('Ticket:' + ticket.ticketId \\\n",
    "            + ',\\n\\t\\t storyId=' + ticket.userStoryId \\\n",
    "            + ',\\n\\t\\t costToFix=' + str(ticket.costToFix) \\\n",
    "            + ',\\n\\t\\t effortToDate=' + str(ticket.effortToDate) \\\n",
    "            + ',\\n\\t\\t percentAchieved=' + str(ticket.percentAchieved))\n",
    "                \n",
    "def format_tickets(tickets):\n",
    "    output = ''\n",
    "    for ticket in tickets:\n",
    "        output = output + '\\n\\t\\t{' + format_ticket(ticket) + '}'\n",
    "    return output\n",
    "            \n",
    "def format_uss(uss):\n",
    "    return ('\\n *** USS:' + uss.userStoryId \\\n",
    "            + '\\n\\t achieved=' + str(uss.percentAchieved) \\\n",
    "            + ',\\n\\t planned=' + str(uss.planned) \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(uss.sprintPlanned) \\\n",
    "            + ',\\n\\t tickets=' + format_tickets(uss.pendingTickets))\n",
    "\n",
    "def format_item(item, item_label, sprint, timeInSprint):\n",
    "    return ('\\n *** ' + item_label + ' at ' + timeInSprint + ' of sprint' + str(sprint) + ': ' \\\n",
    "            + '\\n\\t userStoryId=' + str(item.userStoryId) \\\n",
    "            + ',\\n\\t taskType=' + str(item.taskType) \\\n",
    "            + ',\\n\\t ticketId=' + str(item.ticketId) \\\n",
    "            + ',\\n\\t estimate=' + '{0:.2f}'.format(item.estimate) \\\n",
    "            + ',\\n\\t percentAchieved=' + str(item.percentAchieved)  \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(item.sprintPlanned))\n",
    "\n",
    "# Test logic\n",
    "def test_uss():\n",
    "    output = '' \n",
    "    repo = UserStoriesRepo([UserStory('Story A', 25, 'Joe Developer', 'Amy PM'), \\\n",
    "                            UserStory('Story B', 17, 'Alex Developer', 'Kate PM')])\n",
    "    uss = UserStoryStatus('Story B', 0.0)\n",
    "    uss.planned = True\n",
    "    uss.sprintPlanned = 1\n",
    "    output = output + (format_uss(uss))\n",
    "    item = uss.generateWorkItems(repo)[0]\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'start'))\n",
    "    item.percentAchieved = 0.7\n",
    "    newTickets = [Ticket('Bug 100','Story B', 4), Ticket('Bug 101','Story B', 1.5)]\n",
    "    bugRepo = TicketsRepo(newTickets)\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'end'))\n",
    "    uss.updateStatus([item], newTickets, bugRepo, 1)\n",
    "    uss.sprintPlanned = 2\n",
    "    output = output + (format_uss(uss))\n",
    "\n",
    "    items = uss.generateWorkItems(repo)\n",
    "    item=items[0]\n",
    "    output = output + (format_item(item, 'Item#1', 2, 'start'))\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'start'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'start'))\n",
    "    items[0].percentAchieved = 0.9\n",
    "    items[1].percentAchieved = 1.0\n",
    "    items[2].percentAchieved = 0.5\n",
    "    item=items[0]\n",
    "    item=items[0]\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'end'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'end'))\n",
    "    uss.updateStatus(items, [], bugRepo, 2)\n",
    "    output = output + (format_uss(uss))\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_uss_ACTUAL = test_uss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to print a string output one can copy and paste into test_uss_EXPECTED\n",
    "#test_uss_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Set expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_uss_EXPECTED = '\\n *** USS:Story B\\n\\t achieved=0.0,\\n\\t planned=True,\\n\\t sprintPlanned=1,\\n\\t tickets=\\n *** Item#1 at start of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=1\\n *** Item#1 at end of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.7,\\n\\t sprintPlanned=1\\n *** USS:Story B\\n\\t achieved=0.7,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 100,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=4,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n *** Item#1 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=5.10,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#3 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=1.0,\\n\\t sprintPlanned=2\\n *** Item#3 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.5,\\n\\t sprintPlanned=2\\n *** USS:Story B\\n\\t achieved=0.97,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=2.0,\\n\\t\\t percentAchieved=0.5}'\n",
    "\n",
    "tu_.EXPECTED['uss'] = test_uss_EXPECTED\n",
    "tu_.ACTUAL['uss'] = test_uss_ACTUAL\n",
    "tu_.testOK('uss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print ACTUAL output\n",
    "print(Back.BLUE + Fore.WHITE + '--------------------- ACTUAL -------------------------', \\\n",
    "      Back.RESET + Fore.BLUE + '\\n' + test_uss_ACTUAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print EXPECTED output\n",
    "print(Back.GREEN + Fore.WHITE + '--------------------- EXPECTED -----------------------', \\\n",
    "      Back.RESET + Fore.GREEN + '\\n' + test_uss_EXPECTED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_createTeams</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "teams_df = bo.createTeamsDF(tu_.DEV_DF, tu_.PM_DF)\n",
    "\n",
    "object_cols = ['Scrum Team'] # Need to drop these since they print always-chaning memory address + can't save it in EXPECTED\n",
    "test_createTeams_ACTUAL = teams_df.drop(object_cols, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "#tu_.createExpectedOutput(test_createTeams_ACTUAL, 'test_createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols = ['Developers', 'Product Managers', 'Areas of Responsibility']\n",
    "test_createTeams_EXPECTED = tu_.loadExpectedOutput('test_createTeams', list_cols)\n",
    "\n",
    "tu_.EXPECTED['createTeams'] = test_createTeams_EXPECTED\n",
    "tu_.ACTUAL['createTeams'] = test_createTeams_ACTUAL\n",
    "tu_.testOK('createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_EXPECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_userStoryCreate</h1>\n",
    "<p>This test has multiple views, each of which is checked separately</p>\n",
    "<li>test_userStoryCreate_stories\n",
    "<li>test_userStoryCreate_estimates\n",
    "<li>test_userStoryCreate_crossCheck\n",
    "<li>test_userStoryCreate_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_userStoryCreate():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "        \n",
    "    grouped_estimates_df = stories_df.groupby([ 'Scrum Team', 'Developer'])['Estimate'].sum()\n",
    "    workload_df = stories_df.groupby([ 'Scrum Team'])['User Story Id'].count()\n",
    "    #workload_count_df = workload_df.count()\n",
    "    \n",
    "    avg_estimates_df = grouped_estimates_df.unstack().apply(lambda x: x.mean(), axis='columns')\n",
    "\n",
    "    # Reset index to match the way how EXPECTED will be saved as a CSV file\n",
    "    estimates_df      = grouped_estimates_df.reset_index()\n",
    "    workload_df       = workload_df.reset_index()\n",
    "    avg_estimates_df= avg_estimates_df.reset_index()\n",
    "    \n",
    "    # The unstacking above created an column with a number as the column name, 0. That will not match once expected output is saved\n",
    "    # and reloaded, as it will come back as the string '0'. So rename that column to avoid spurious test failures\n",
    "    avg_estimates_df = avg_estimates_df.rename(index=str, columns={0: 'Avg'})\n",
    "    \n",
    "    # Because of the manipulations, the index has changed and that will cause mistaches with the EXPECTED loaded from \n",
    "    # CSV. So re-index\n",
    "    avg_estimates_df.index = pd.RangeIndex(start=0, stop=avg_estimates_df.index.size, step=1)\n",
    "    \n",
    "    crosscheck = [len(teams_df['Scrum Team'][0].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][1].backlog.pendingUserStories),\n",
    "                  len(teams_df['Scrum Team'][2].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][3].backlog.pendingUserStories)]\n",
    "    crosscheck_df = pd.DataFrame({'Team idx': [0,1,2,3], 'Backlog size': crosscheck})\n",
    "    \n",
    "    output['stories_df'] = stories_df\n",
    "    output['estimates_df'] = estimates_df\n",
    "    output['workload_df'] = workload_df\n",
    "    output['crosscheck_df'] = crosscheck_df\n",
    "    output['avg_estimates_df'] = avg_estimates_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_userStoryCreate_ACTUAL = test_userStoryCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_userStoryCreate_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['stories_df'],        'test_userStoryCreate.stories_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['estimates_df'],      'test_userStoryCreate.estimates_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['workload_df'],       'test_userStoryCreate.workload_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['crosscheck_df'],     'test_userStoryCreate.crosscheck_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['avg_estimates_df'],  'test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_userStoryCreate_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "\n",
    "test_userStoryCreate_EXPECTED = {}\n",
    "\n",
    "test_userStoryCreate_EXPECTED['stories_df']         = tu_.loadExpectedOutput('test_userStoryCreate.stories_df')\n",
    "test_userStoryCreate_EXPECTED['estimates_df']       = tu_.loadExpectedOutput('test_userStoryCreate.estimates_df')\n",
    "test_userStoryCreate_EXPECTED['workload_df']        = tu_.loadExpectedOutput('test_userStoryCreate.workload_df')\n",
    "test_userStoryCreate_EXPECTED['crosscheck_df']      = tu_.loadExpectedOutput('test_userStoryCreate.crosscheck_df')\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']   = tu_.loadExpectedOutput('test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for the sensitive fields\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Avg'],\n",
    "                                        ['avg_estimates_df'],\n",
    "                                        test_userStoryCreate_EXPECTED,\n",
    "                                        test_userStoryCreate_ACTUAL)\n",
    "\n",
    "tu_.EXPECTED['test_userStoryCreate.stories_df']         = test_userStoryCreate_EXPECTED['stories_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.estimates_df']       = test_userStoryCreate_EXPECTED['estimates_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.workload_df']        = test_userStoryCreate_EXPECTED['workload_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.crosscheck_df']      = test_userStoryCreate_EXPECTED['crosscheck_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.avg_estimates_df']   = test_userStoryCreate_EXPECTED['avg_estimates_df']\n",
    "\n",
    "tu_.ACTUAL['test_userStoryCreate.stories_df']           = test_userStoryCreate_ACTUAL['stories_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.estimates_df']         = test_userStoryCreate_ACTUAL['estimates_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.workload_df']          = test_userStoryCreate_ACTUAL['workload_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.crosscheck_df']        = test_userStoryCreate_ACTUAL['crosscheck_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.avg_estimates_df']     = test_userStoryCreate_ACTUAL['avg_estimates_df']\n",
    "\n",
    "tu_.testOK('test_userStoryCreate.stories_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.estimates_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.workload_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.crosscheck_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.avg_estimates_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_WorkAssignments</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_WorkAssignments():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    #NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    \n",
    "    #timecard.RANDOM.reset(271) # Set seed so output is the same even if logic invokes random methods\n",
    "    #teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, 125)\n",
    "\n",
    "    teamId0 = teams_df['Scrum Team'][0].teamId\n",
    "    work = WorkAssignments(teamId0, teamsRepo, storiesRepo, sprint=1)\n",
    "    initial_df = work.committedTime(10)\n",
    "    # Test re-assigning of work\n",
    "    item = work.allocations[S_.UNPLANNED][S_.OWNER_TBD][S_.UNFINISHED_STORIES][25]\n",
    "    work.reAssign(item, 'Bruno Studley', S_.CURRENT_SPRINT)\n",
    "    final_df = work.committedTime(10)\n",
    "    \n",
    "    output['Initial'] = initial_df\n",
    "    output['Final'] = final_df\n",
    "\n",
    "    return output\n",
    "    \n",
    "# Run the test\n",
    "test_WorkAssignments_ACTUAL = test_WorkAssignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_WorkAssignments_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Initial'],        'test_WorkAssignments.Initial')\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Final'],          'test_WorkAssignments.Final')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_WorkAssignments_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_WorkAssignments_EXPECTED = {}\n",
    "\n",
    "test_WorkAssignments_EXPECTED['Initial']         = tu_.loadExpectedOutput('test_WorkAssignments.Initial')\n",
    "test_WorkAssignments_EXPECTED['Final']           = tu_.loadExpectedOutput('test_WorkAssignments.Final')\n",
    "\n",
    "tu_.EXPECTED['test_WorkAssignments.Initial']         = test_WorkAssignments_EXPECTED['Initial']\n",
    "tu_.EXPECTED['test_WorkAssignments.Final']           = test_WorkAssignments_EXPECTED['Final']\n",
    "\n",
    "tu_.ACTUAL['test_WorkAssignments.Initial']           = test_WorkAssignments_ACTUAL['Initial']\n",
    "tu_.ACTUAL['test_WorkAssignments.Final']             = test_WorkAssignments_ACTUAL['Final']\n",
    "\n",
    "tu_.testOK('test_WorkAssignments.Initial'), \\\n",
    "tu_.testOK('test_WorkAssignments.Final'), \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_oneSprint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_oneSprint():    \n",
    "    output = {}\n",
    "\n",
    "    # Choose what to work on at the start of a sprint.\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    SPRINT = 1\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel())\n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "    modelsConfig.context = ReleaseCycleContext(teamId, teamsRepo, storiesRepo, ticketsRepo, SPRINT, SPRINT_DURATION)\n",
    "\n",
    "    work = timecard.chooseWhatToDoInSprint(modelsConfig)\n",
    "    start_committed_df = work.committedTime(SPRINT_DURATION)\n",
    "    start_tasks_df = work.committedTasks()\n",
    "    \n",
    "    # Deliver what the sprint actually accomplished, including ingest of defects arriving during sprint\n",
    "    timecard.deliverSprint(work, modelsConfig) # mutates work\n",
    "    inflow = timecard.inflowOfTickets(modelsConfig)\n",
    "    end_committed_df = work.committedTime(0) # Sprint is over, so sprint capacity parameter is 0\n",
    "    \n",
    "    # Test continued\n",
    "    timecard.updateBacklogAfterSprint(work, inflow, modelsConfig)\n",
    "    end_tasks_df = work.committedTasks()\n",
    "    \n",
    "    output['Start_Committed'] = start_committed_df\n",
    "    output['Start_Tasks'] = start_tasks_df\n",
    "    output['End_Committed'] = end_committed_df\n",
    "    output['End_Tasks'] = end_tasks_df\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_oneSprint_ACTUAL = test_oneSprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_oneSprint_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Committed'],    'test_oneSprint.Start_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Tasks'],        'test_oneSprint.Start_Tasks')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Committed'],      'test_oneSprint.End_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Tasks'],          'test_oneSprint.End_Tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_oneSprint_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_oneSprint_EXPECTED = {}\n",
    "\n",
    "test_oneSprint_EXPECTED['Start_Committed']      = tu_.loadExpectedOutput('test_oneSprint.Start_Committed')\n",
    "test_oneSprint_EXPECTED['Start_Tasks']          = tu_.loadExpectedOutput('test_oneSprint.Start_Tasks')\n",
    "test_oneSprint_EXPECTED['End_Committed']        = tu_.loadExpectedOutput('test_oneSprint.End_Committed')\n",
    "test_oneSprint_EXPECTED['End_Tasks']            = tu_.loadExpectedOutput('test_oneSprint.End_Tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                                          'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                        ['Start_Committed', 'End_Committed'],\n",
    "                                        test_oneSprint_EXPECTED,\n",
    "                                        test_oneSprint_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved'],\n",
    "                                        ['Start_Tasks', 'End_Tasks'],\n",
    "                                        test_oneSprint_EXPECTED,\n",
    "                                        test_oneSprint_ACTUAL)\n",
    "\n",
    "tu_.EXPECTED['test_oneSprint.Start_Committed']      = test_oneSprint_EXPECTED['Start_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.Start_Tasks']          = test_oneSprint_EXPECTED['Start_Tasks']\n",
    "tu_.EXPECTED['test_oneSprint.End_Committed']        = test_oneSprint_EXPECTED['End_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.End_Tasks']            = test_oneSprint_EXPECTED['End_Tasks']\n",
    "\n",
    "tu_.ACTUAL['test_oneSprint.Start_Committed']        = test_oneSprint_ACTUAL['Start_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.Start_Tasks']            = test_oneSprint_ACTUAL['Start_Tasks']\n",
    "tu_.ACTUAL['test_oneSprint.End_Committed']          = test_oneSprint_ACTUAL['End_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.End_Tasks']              = test_oneSprint_ACTUAL['End_Tasks']\n",
    "\n",
    "tu_.testOK('test_oneSprint.Start_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.Start_Tasks'), \\\n",
    "tu_.testOK('test_oneSprint.End_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.End_Tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_multipleSprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_multipleSprints():\n",
    "    output = {}\n",
    "    # Test many sprints into the future, to see if eventually people have extra time and start using that extra time\n",
    "    # in the current sprint to get a head start on tasks for the next sprint\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 15 \n",
    "    \n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             125, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "        \n",
    "\n",
    "    work = None\n",
    "    for i in range(NUMBER_OF_SPRINTS):\n",
    "        sprint = i+1\n",
    "        modelsConfig.context = ReleaseCycleContext(teamId, teamsRepo, storiesRepo, ticketsRepo, sprint, SPRINT_DURATION)\n",
    "        work = timecard.chooseWhatToDoInSprint(modelsConfig)\n",
    "        if (i== NUMBER_OF_SPRINTS -1):\n",
    "            break\n",
    "        timecard.deliverSprint(work, modelsConfig) # mutates 'work'\n",
    "        inflow = timecard.inflowOfTickets(modelsConfig)\n",
    "        timecard.updateBacklogAfterSprint(work, inflow, modelsConfig) # Does not mutate 'work'\n",
    "    \n",
    "    # Work Assignments at the start of the last sprint. Should see some \"looking ahead\" tasks, i.e., tasks that would\n",
    "    # normally be done in the next sprint but are started in this sprint since we have time leftover from this sprint's \n",
    "    # deliverables\n",
    "    last = work\n",
    "    start_committed_df = last.committedTime(10)\n",
    "    start_tasks_df     = last.committedTasks()\n",
    "    \n",
    "    # Finish this last sprint and confirm we spent time in some of the deliveries for the next sprint (i.e., that we\n",
    "    # were \"looking ahead\")\n",
    "    timecard.deliverSprint(last, modelsConfig)\n",
    "    end_committed_df = last.committedTime(0)\n",
    "    end_tasks_df     =last.committedTasks()\n",
    "    \n",
    "    output['start_committed']   = start_committed_df\n",
    "    output['start_tasks']       = start_tasks_df\n",
    "    output['end_committed']     = end_committed_df\n",
    "    output['end_tasks']         = end_tasks_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_multipleSprints_ACTUAL = test_multipleSprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_multipleSprints_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_committed'],    'test_multipleSprints.start_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_tasks'],        'test_multipleSprints.start_tasks')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_committed'],      'test_multipleSprints.end_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_tasks'],          'test_multipleSprints.end_tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_multipleSprints_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_multipleSprints_EXPECTED = {}\n",
    "\n",
    "test_multipleSprints_EXPECTED['start_committed']    = tu_.loadExpectedOutput('test_multipleSprints.start_committed')\n",
    "test_multipleSprints_EXPECTED['start_tasks']        = tu_.loadExpectedOutput('test_multipleSprints.start_tasks')\n",
    "test_multipleSprints_EXPECTED['end_committed']      = tu_.loadExpectedOutput('test_multipleSprints.end_committed')\n",
    "test_multipleSprints_EXPECTED['end_tasks']          = tu_.loadExpectedOutput('test_multipleSprints.end_tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                                          'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth'],\n",
    "                                        ['start_committed', 'end_committed'],\n",
    "                                        test_multipleSprints_EXPECTED,\n",
    "                                        test_multipleSprints_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved'],\n",
    "                                        ['start_tasks', 'end_tasks'],\n",
    "                                        test_multipleSprints_EXPECTED,\n",
    "                                        test_multipleSprints_ACTUAL)\n",
    "ExpectedOutputCleaner.destringify(['Delivered in Sprint'], \n",
    "                                  ['start_tasks', 'end_tasks'], \n",
    "                                  test_multipleSprints_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_multipleSprints.start_committed']    = test_multipleSprints_EXPECTED['start_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.start_tasks']        = test_multipleSprints_EXPECTED['start_tasks']\n",
    "tu_.EXPECTED['test_multipleSprints.end_committed']      = test_multipleSprints_EXPECTED['end_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.end_tasks']          = test_multipleSprints_EXPECTED['end_tasks']\n",
    "\n",
    "tu_.ACTUAL['test_multipleSprints.start_committed']      = test_multipleSprints_ACTUAL['start_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.start_tasks']          = test_multipleSprints_ACTUAL['start_tasks']\n",
    "tu_.ACTUAL['test_multipleSprints.end_committed']        = test_multipleSprints_ACTUAL['end_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.end_tasks']            = test_multipleSprints_ACTUAL['end_tasks']\n",
    "\n",
    "tu_.testOK('test_multipleSprints.start_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.start_tasks'), \\\n",
    "tu_.testOK('test_multipleSprints.end_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.end_tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_releaseCycle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "# Test logic\n",
    "def test_releaseCycle():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "\n",
    "\n",
    "    entries_df, log = timecard.runReleaseCycle(teamsRepo, ticketsRepo, storiesRepo, datetime(2018, 1, 15), \\\n",
    "                                                   SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)   \n",
    "     \n",
    "    burnout_df = timecard.releaseBurnout(entries_df)\n",
    "    \n",
    "    output['Entries'] = entries_df\n",
    "    output['Burnout'] = burnout_df\n",
    "\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        log_df = log.mergeLogs(name)\n",
    "        output[name] = log_df\n",
    "\n",
    "    output['log'] = log #Needed for visualizations\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_releaseCycle_ACTUAL = test_releaseCycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_releaseCycle_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Entries'],    'test_releaseCycle.Entries')\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Burnout'],    'test_releaseCycle.Burnout')\n",
    "    for name in ReleaseLog.SNAPSHOTS:\n",
    "        tu_.createExpectedOutput(test_releaseCycle_ACTUAL[name],    'test_releaseCycle.' + name)\n",
    "\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_releaseCycle_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_releaseCycle_EXPECTED = {}\n",
    "\n",
    "test_releaseCycle_EXPECTED['Entries']      = tu_.loadExpectedOutput('test_releaseCycle.Entries')\n",
    "test_releaseCycle_EXPECTED['Burnout']      = tu_.loadExpectedOutput('test_releaseCycle.Burnout')\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    test_releaseCycle_EXPECTED[name]       = tu_.loadExpectedOutput('test_releaseCycle.' + name)\n",
    "\n",
    "# ReleaseLog snapshots have integer-valued columns (1, 2,3, ...), and loading the EXPECTED CSV file will convert\n",
    "# them to strings ('1', '2', '3', ...), so to avoid spurious test failures rename the columns of the\n",
    "# EXPECTED data we just loaded\n",
    "\n",
    "ExpectedOutputCleaner.alignColumns(ReleaseLog.SNAPSHOTS,\n",
    "                                  test_releaseCycle_EXPECTED,\n",
    "                                  test_releaseCycle_ACTUAL)\n",
    "\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Time Spent'],\n",
    "                                        ['Entries'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Effort'],\n",
    "                                        ['Burnout'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "                                       \n",
    "testlets = ReleaseLog.SNAPSHOTS.copy()\n",
    "testlets.remove('Outcome') # All log snapshots except 'Outcome' have the same sensitive fields\n",
    "ExpectedOutputCleaner.cleanRoundingNoise([0,1,2,3,4,5,6,7,8,9,10],\n",
    "                                        testlets,\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['Original Estimate', 'Effort Spent','Effort Remaining', 'Percent Achieved'],\n",
    "                                         ['Outcome'],\n",
    "                                        test_releaseCycle_EXPECTED,\n",
    "                                        test_releaseCycle_ACTUAL)\n",
    "                                    \n",
    "\n",
    "ExpectedOutputCleaner.standardizeDates(['Date'],\n",
    "                                      ['Entries'],\n",
    "                                      test_releaseCycle_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_releaseCycle.Entries']        = test_releaseCycle_EXPECTED['Entries']\n",
    "tu_.EXPECTED['test_releaseCycle.Burnout']        = test_releaseCycle_EXPECTED['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.EXPECTED['test_releaseCycle.' + name]        = test_releaseCycle_EXPECTED[name]\n",
    "\n",
    "tu_.ACTUAL['test_releaseCycle.Entries']          = test_releaseCycle_ACTUAL['Entries']\n",
    "tu_.ACTUAL['test_releaseCycle.Burnout']          = test_releaseCycle_ACTUAL['Burnout']\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    tu_.ACTUAL['test_releaseCycle.' + name]        = test_releaseCycle_ACTUAL[name]\n",
    "\n",
    "results = []\n",
    "results.append(tu_.testOK('test_releaseCycle.Entries'))\n",
    "results.append(tu_.testOK('test_releaseCycle.Burnout'))\n",
    "for name in ReleaseLog.SNAPSHOTS:\n",
    "    results.append(tu_.testOK('test_releaseCycle.' + name))\n",
    "                   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to interactively visualize the release logs, and then comment again once interactive analysis is done. Commenting these\n",
    "# lines after interactive analysis is completed is required as test harness can't load these visualiations\n",
    "# libraries so leaving this uncommented will crash the entire test harness.\n",
    "# NB: MAY NEED TO RUN TWICE (there is a bug in Jupyter notebook, I think, so first time you call this it shows no visuals)\n",
    "#import devanalyst.simulation.visualizations.timecard_visuals as tc_visuals\n",
    "#tc_visuals.renderReleaseCycleLog(teamId = 'Team A', release_log=test_releaseCycle_ACTUAL['log'], first=1, last=17, spurious_columns=['Team Id', 'Sprint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Entries'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_Start_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_End_CURRENT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_Start_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['planned_End_NEXT_SPRINT'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['backlog'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Outcome'][0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_FOO: template for tests that produce a DataFrame output </h1>\n",
    "<li> Copy to create a new test, and replace 'FOO' by the name of your test.\n",
    "<li> Implement test logic in test_FOO()\n",
    "<li> To create expected output once test logic is producing right output, temporarily uncomment the call to createExpectedOutput, call it, and comment it again. This saves expected output as a CSV file so subsequent runs of the test can load it and verify that the test still produces the same output as before.\n",
    "<li> Uncomment lines that load expected output and remove the dummy line next to it creating a dummy expected output.\n",
    "<li> Display the outputs for convenience in debugging when test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_FOO():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    TEAM_ID = 'Team A'\n",
    "    SPRINT = 1\n",
    "    modelsConfig.context = ReleaseCycleContext(TEAM_ID, teamsRepo, storiesRepo, ticketsRepo, SPRINT, SPRINT_DURATION)\n",
    "    \n",
    "    date1 = pd.Timestamp(2019,3,28)\n",
    "    pubs1 = ['magazine', 'novel', 'textbook']\n",
    "    date2 = pd.Timestamp(2019, 1,27)\n",
    "    pubs2 = ['NY Times', 'The Atlantic', 'WSJ']\n",
    "    dummyA_df = pd.DataFrame({'A': [45.123456789, 34.123456789], 'Date': [date1, date2], 'Publications': [pubs1, pubs2]})\n",
    "    dummyB_df = pd.DataFrame({'B': [2333.123456789, 3984.123456789]})\n",
    "    \n",
    "    output['Dummy_A'] = dummyA_df\n",
    "    output['Dummy_B'] = dummyB_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_FOO_ACTUAL = test_FOO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_FOO_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_A'],    'test_FOO.Dummy_A')\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_B'],    'test_FOO.Dummy_B')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_FOO_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols_A = ['Publications'] # Lists are loaded as strings, so require special processing on load\n",
    "list_cols_B = []\n",
    "test_FOO_EXPECTED = {}\n",
    "\n",
    "test_FOO_EXPECTED['Dummy_A']      = tu_.loadExpectedOutput('test_FOO.Dummy_A', list_cols_A)\n",
    "test_FOO_EXPECTED['Dummy_B']      = tu_.loadExpectedOutput('test_FOO.Dummy_B', list_cols_B)\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['A'],\n",
    "                                        ['Dummy_A'],\n",
    "                                        test_FOO_EXPECTED,\n",
    "                                        test_FOO_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['B'],\n",
    "                                        ['Dummy_B'],\n",
    "                                        test_FOO_EXPECTED,\n",
    "                                        test_FOO_ACTUAL)\n",
    "\n",
    "\n",
    "# Dates are loaded as strings, not pd.Timestamps, so to avoid spurious mismatches between ACTUAL (which \n",
    "# represents dates as pd.Timestamps) and EXPECTED, convert the EXPECTED dates into pd.Timestamps\n",
    "ExpectedOutputCleaner.standardizeDates(['Date'],\n",
    "                                      ['Dummy_A'],\n",
    "                                      test_FOO_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_FOO.Dummy_A']        = test_FOO_EXPECTED['Dummy_A']\n",
    "tu_.EXPECTED['test_FOO.Dummy_B']        = test_FOO_EXPECTED['Dummy_B']\n",
    "\n",
    "tu_.ACTUAL['test_FOO.Dummy_A']          = test_FOO_ACTUAL['Dummy_A']\n",
    "tu_.ACTUAL['test_FOO.Dummy_B']          = test_FOO_ACTUAL['Dummy_B']\n",
    "\n",
    "tu_.testOK('test_FOO.Dummy_A'), \\\n",
    "tu_.testOK('test_FOO.Dummy_B'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run all the tests in this notebook</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all the tests defined in this notebook\n",
    "#unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
