{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "from colorama import Fore, Back, Style \n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Link Code to be Tested</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\statics.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\GenerateTimecards.ipynb\n",
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\simulationModels.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.simulation.statics as S_\n",
    "\n",
    "import devanalyst.simulation.GenerateTimecards as timecard\n",
    "from devanalyst.simulation.GenerateTimecards import UserStoriesRepo, UserStory, UserStoryStatus, \\\n",
    "Ticket, TicketsRepo, ScrumTeam, ScrumTeamsRepo, IdCounter, WorkAssignments\n",
    "\n",
    "from devanalyst.simulation.simulationModels import ModelsConfig, DefaultCostModel, GreedyAllocationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\alex\\code\\labs\\devanalyst\\devanalyst\\simulation\\tests\\test_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.simulation.tests.test_utils as tu_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_uss</h1>\n",
    "<p>Test User Story Status</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     3,
     10,
     16,
     23
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "# Helper methods to the test\n",
    "def format_ticket(ticket):\n",
    "    return ('Ticket:' + ticket.ticketId \\\n",
    "            + ',\\n\\t\\t storyId=' + ticket.userStoryId \\\n",
    "            + ',\\n\\t\\t costToFix=' + str(ticket.costToFix) \\\n",
    "            + ',\\n\\t\\t effortToDate=' + str(ticket.effortToDate) \\\n",
    "            + ',\\n\\t\\t percentAchieved=' + str(ticket.percentAchieved))\n",
    "                \n",
    "def format_tickets(tickets):\n",
    "    output = ''\n",
    "    for ticket in tickets:\n",
    "        output = output + '\\n\\t\\t{' + format_ticket(ticket) + '}'\n",
    "    return output\n",
    "            \n",
    "def format_uss(uss):\n",
    "    return ('\\n *** USS:' + uss.userStoryId \\\n",
    "            + '\\n\\t achieved=' + str(uss.percentAchieved) \\\n",
    "            + ',\\n\\t planned=' + str(uss.planned) \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(uss.sprintPlanned) \\\n",
    "            + ',\\n\\t tickets=' + format_tickets(uss.pendingTickets))\n",
    "\n",
    "def format_item(item, item_label, sprint, timeInSprint):\n",
    "    return ('\\n *** ' + item_label + ' at ' + timeInSprint + ' of sprint' + str(sprint) + ': ' \\\n",
    "            + '\\n\\t userStoryId=' + str(item.userStoryId) \\\n",
    "            + ',\\n\\t taskType=' + str(item.taskType) \\\n",
    "            + ',\\n\\t ticketId=' + str(item.ticketId) \\\n",
    "            + ',\\n\\t estimate=' + '{0:.2f}'.format(item.estimate) \\\n",
    "            + ',\\n\\t percentAchieved=' + str(item.percentAchieved)  \\\n",
    "            + ',\\n\\t sprintPlanned=' + str(item.sprintPlanned))\n",
    "\n",
    "# Test logic\n",
    "def test_uss():\n",
    "    output = '' \n",
    "    repo = UserStoriesRepo([UserStory('Story A', 25, 'Joe Developer', 'Amy PM'), \\\n",
    "                            UserStory('Story B', 17, 'Alex Developer', 'Kate PM')])\n",
    "    uss = UserStoryStatus('Story B', 0.0)\n",
    "    uss.planned = True\n",
    "    uss.sprintPlanned = 1\n",
    "    output = output + (format_uss(uss))\n",
    "    item = uss.generateWorkItems(repo)[0]\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'start'))\n",
    "    item.percentAchieved = 0.7\n",
    "    newTickets = [Ticket('Bug 100','Story B', 4), Ticket('Bug 101','Story B', 1.5)]\n",
    "    bugRepo = TicketsRepo(newTickets)\n",
    "    output = output + (format_item(item, 'Item#1', 1, 'end'))\n",
    "    uss.updateStatus([item], newTickets, bugRepo)\n",
    "    uss.sprintPlanned = 2\n",
    "    output = output + (format_uss(uss))\n",
    "\n",
    "    items = uss.generateWorkItems(repo)\n",
    "    item=items[0]\n",
    "    output = output + (format_item(item, 'Item#1', 2, 'start'))\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'start'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'start'))\n",
    "    items[0].percentAchieved = 0.9\n",
    "    items[1].percentAchieved = 1.0\n",
    "    items[2].percentAchieved = 0.5\n",
    "    item=items[0]\n",
    "    item=items[0]\n",
    "    item=items[1]\n",
    "    output = output + (format_item(item, 'Item#2', 2, 'end'))\n",
    "    item=items[2]\n",
    "    output = output + (format_item(item, 'Item#3', 2, 'end'))\n",
    "    uss.updateStatus(items, [], bugRepo)\n",
    "    output = output + (format_uss(uss))\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_uss_ACTUAL = test_uss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to print a string output one can copy and paste into test_uss_EXPECTED\n",
    "#test_uss_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_uss_EXPECTED = '\\n *** USS:Story B\\n\\t achieved=0.0,\\n\\t planned=True,\\n\\t sprintPlanned=1,\\n\\t tickets=\\n *** Item#1 at start of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=1\\n *** Item#1 at end of sprint1: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=17.00,\\n\\t percentAchieved=0.7,\\n\\t sprintPlanned=1\\n *** USS:Story B\\n\\t achieved=0.7,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 100,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=4,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=0.0,\\n\\t\\t percentAchieved=0.0}\\n *** Item#1 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=UNFINISHED_STORIES,\\n\\t ticketId=None,\\n\\t estimate=5.10,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#3 at start of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.0,\\n\\t sprintPlanned=2\\n *** Item#2 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 100,\\n\\t estimate=2.00,\\n\\t percentAchieved=1.0,\\n\\t sprintPlanned=2\\n *** Item#3 at end of sprint2: \\n\\t userStoryId=Story B,\\n\\t taskType=BUGS_ON_UNFINISHED_STORIES,\\n\\t ticketId=Bug 101,\\n\\t estimate=2.00,\\n\\t percentAchieved=0.5,\\n\\t sprintPlanned=2\\n *** USS:Story B\\n\\t achieved=0.97,\\n\\t planned=True,\\n\\t sprintPlanned=2,\\n\\t tickets=\\n\\t\\t{Ticket:Bug 101,\\n\\t\\t storyId=Story B,\\n\\t\\t costToFix=1.5,\\n\\t\\t effortToDate=2.0,\\n\\t\\t percentAchieved=0.5}'\n",
    "\n",
    "tu_.EXPECTED['uss'] = test_uss_EXPECTED\n",
    "tu_.ACTUAL['uss'] = test_uss_ACTUAL\n",
    "tu_.testOK('uss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print ACTUAL output\n",
    "print(Back.BLUE + Fore.WHITE + '--------------------- ACTUAL -------------------------', \\\n",
    "      Back.RESET + Fore.BLUE + '\\n' + test_uss_ACTUAL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Print EXPECTED output\n",
    "print(Back.GREEN + Fore.WHITE + '--------------------- EXPECTED -----------------------', \\\n",
    "      Back.RESET + Fore.GREEN + '\\n' + test_uss_EXPECTED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_createTeams</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run\n",
    "\n",
    "teams_df = timecard.createTeamsDF(tu_.DEV_DF, tu_.PM_DF)\n",
    "\n",
    "object_cols = ['Scrum Team'] # Need to drop these since they print always-chaning memory address + can't save it in EXPECTED\n",
    "test_createTeams_ACTUAL = teams_df.drop(object_cols, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "#tu_.createExpectedOutput(test_createTeams_ACTUAL, 'test_createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols = ['Developers', 'Product Managers', 'Areas of Responsibility']\n",
    "test_createTeams_EXPECTED = tu_.loadExpectedOutput('test_createTeams', list_cols)\n",
    "\n",
    "tu_.EXPECTED['createTeams'] = test_createTeams_EXPECTED\n",
    "tu_.ACTUAL['createTeams'] = test_createTeams_ACTUAL\n",
    "tu_.testOK('createTeams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_ACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_createTeams_EXPECTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_userStoryCreate</h1>\n",
    "<p>This test has multiple views, each of which is checked separately</p>\n",
    "<li>test_userStoryCreate_stories\n",
    "<li>test_userStoryCreate_estimates\n",
    "<li>test_userStoryCreate_crossCheck\n",
    "<li>test_userStoryCreate_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_userStoryCreate():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "        \n",
    "    grouped_estimates_df = stories_df.groupby([ 'Scrum Team', 'Developer'])['Estimate'].sum()\n",
    "    workload_df = stories_df.groupby([ 'Scrum Team'])['User Story Id'].count()\n",
    "    #workload_count_df = workload_df.count()\n",
    "    \n",
    "    avg_estimates_df = grouped_estimates_df.unstack().apply(lambda x: x.mean(), axis='columns')\n",
    "\n",
    "    # Reset index to match the way how EXPECTED will be saved as a CSV file\n",
    "    estimates_df      = grouped_estimates_df.reset_index()\n",
    "    workload_df       = workload_df.reset_index()\n",
    "    avg_estimates_df= avg_estimates_df.reset_index()\n",
    "    \n",
    "    # The unstacking above created an column with a number as the column name, 0. That will not match once expected output is saved\n",
    "    # and reloaded, as it will come back as the string '0'. So rename that column to avoid spurious test failures\n",
    "    avg_estimates_df = avg_estimates_df.rename(index=str, columns={0: 'Avg'})\n",
    "    \n",
    "    # Because of the manipulations, the index has changed and that will cause mistaches with the EXPECTED loaded from \n",
    "    # CSV. So re-index\n",
    "    avg_estimates_df.index = pd.RangeIndex(start=0, stop=avg_estimates_df.index.size, step=1)\n",
    "    \n",
    "    crosscheck = [len(teams_df['Scrum Team'][0].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][1].backlog.pendingUserStories),\n",
    "                  len(teams_df['Scrum Team'][2].backlog.pendingUserStories), \n",
    "                  len(teams_df['Scrum Team'][3].backlog.pendingUserStories)]\n",
    "    crosscheck_df = pd.DataFrame({'Team idx': [0,1,2,3], 'Backlog size': crosscheck})\n",
    "    \n",
    "    output['stories_df'] = stories_df\n",
    "    output['estimates_df'] = estimates_df\n",
    "    output['workload_df'] = workload_df\n",
    "    output['crosscheck_df'] = crosscheck_df\n",
    "    output['avg_estimates_df'] = avg_estimates_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_userStoryCreate_ACTUAL = test_userStoryCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_userStoryCreate_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['stories_df'],        'test_userStoryCreate.stories_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['estimates_df'],      'test_userStoryCreate.estimates_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['workload_df'],       'test_userStoryCreate.workload_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['crosscheck_df'],     'test_userStoryCreate.crosscheck_df')\n",
    "    tu_.createExpectedOutput(test_userStoryCreate_ACTUAL['avg_estimates_df'],  'test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_userStoryCreate_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "\n",
    "test_userStoryCreate_EXPECTED = {}\n",
    "\n",
    "test_userStoryCreate_EXPECTED['stories_df']         = tu_.loadExpectedOutput('test_userStoryCreate.stories_df')\n",
    "test_userStoryCreate_EXPECTED['estimates_df']       = tu_.loadExpectedOutput('test_userStoryCreate.estimates_df')\n",
    "test_userStoryCreate_EXPECTED['workload_df']        = tu_.loadExpectedOutput('test_userStoryCreate.workload_df')\n",
    "test_userStoryCreate_EXPECTED['crosscheck_df']      = tu_.loadExpectedOutput('test_userStoryCreate.crosscheck_df')\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']   = tu_.loadExpectedOutput('test_userStoryCreate.avg_estimates_df')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for the sensitive fields\n",
    "rounded = test_userStoryCreate_EXPECTED['avg_estimates_df']['Avg'].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "test_userStoryCreate_EXPECTED['avg_estimates_df']['Avg'] = rounded\n",
    "rounded = test_userStoryCreate_ACTUAL['avg_estimates_df']['Avg'].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "test_userStoryCreate_ACTUAL['avg_estimates_df']['Avg'] = rounded\n",
    "\n",
    "tu_.EXPECTED['test_userStoryCreate.stories_df']         = test_userStoryCreate_EXPECTED['stories_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.estimates_df']       = test_userStoryCreate_EXPECTED['estimates_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.workload_df']        = test_userStoryCreate_EXPECTED['workload_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.crosscheck_df']      = test_userStoryCreate_EXPECTED['crosscheck_df']\n",
    "tu_.EXPECTED['test_userStoryCreate.avg_estimates_df']   = test_userStoryCreate_EXPECTED['avg_estimates_df']\n",
    "\n",
    "tu_.ACTUAL['test_userStoryCreate.stories_df']           = test_userStoryCreate_ACTUAL['stories_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.estimates_df']         = test_userStoryCreate_ACTUAL['estimates_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.workload_df']          = test_userStoryCreate_ACTUAL['workload_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.crosscheck_df']        = test_userStoryCreate_ACTUAL['crosscheck_df']\n",
    "tu_.ACTUAL['test_userStoryCreate.avg_estimates_df']     = test_userStoryCreate_ACTUAL['avg_estimates_df']\n",
    "\n",
    "tu_.testOK('test_userStoryCreate.stories_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.estimates_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.workload_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.crosscheck_df'), \\\n",
    "tu_.testOK('test_userStoryCreate.avg_estimates_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['stories_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['workload_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['crosscheck_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_ACTUAL['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "test_userStoryCreate_EXPECTED['avg_estimates_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_WorkAssignments</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_WorkAssignments():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    #NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    \n",
    "    #timecard.RANDOM.reset(271) # Set seed so output is the same even if logic invokes random methods\n",
    "    #teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = initTestData(DEV_DF, PM_DF, 125)\n",
    "\n",
    "    teamId0 = teams_df['Scrum Team'][0].teamId\n",
    "    work = WorkAssignments(teamId0, teamsRepo, storiesRepo, sprint=1)\n",
    "    initial_df = work.committedTime(10)\n",
    "    # Test re-assigning of work\n",
    "    item = work.allocations[S_.UNPLANNED][S_.OWNER_TBD][S_.UNFINISHED_STORIES][25]\n",
    "    work.reAssign(item, 'Bruno Studley', S_.CURRENT_SPRINT)\n",
    "    final_df = work.committedTime(10)\n",
    "    \n",
    "    output['Initial'] = initial_df\n",
    "    output['Final'] = final_df\n",
    "\n",
    "    return output\n",
    "    \n",
    "# Run the test\n",
    "test_WorkAssignments_ACTUAL = test_WorkAssignments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_WorkAssignments_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Initial'],        'test_WorkAssignments.Initial')\n",
    "    tu_.createExpectedOutput(test_WorkAssignments_ACTUAL['Final'],          'test_WorkAssignments.Final')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_WorkAssignments_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_WorkAssignments_EXPECTED = {}\n",
    "\n",
    "test_WorkAssignments_EXPECTED['Initial']         = tu_.loadExpectedOutput('test_WorkAssignments.Initial')\n",
    "test_WorkAssignments_EXPECTED['Final']           = tu_.loadExpectedOutput('test_WorkAssignments.Final')\n",
    "\n",
    "tu_.EXPECTED['test_WorkAssignments.Initial']         = test_WorkAssignments_EXPECTED['Initial']\n",
    "tu_.EXPECTED['test_WorkAssignments.Final']           = test_WorkAssignments_EXPECTED['Final']\n",
    "\n",
    "tu_.ACTUAL['test_WorkAssignments.Initial']           = test_WorkAssignments_ACTUAL['Initial']\n",
    "tu_.ACTUAL['test_WorkAssignments.Final']             = test_WorkAssignments_ACTUAL['Final']\n",
    "\n",
    "tu_.testOK('test_WorkAssignments.Initial'), \\\n",
    "tu_.testOK('test_WorkAssignments.Final'), \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_ACTUAL['Final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_WorkAssignments_EXPECTED['Final']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_oneSprint</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_oneSprint():    \n",
    "    output = {}\n",
    "\n",
    "    # Choose what to work on at the start of a sprint.\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION))\n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "    work = timecard.chooseWhatToDoInSprint(teamId, teamsRepo, storiesRepo, SPRINT_DURATION, modelsConfig, sprint=1)\n",
    "    start_committed_df = work.committedTime(SPRINT_DURATION)\n",
    "    start_tasks_df = work.committedTasks()\n",
    "    \n",
    "    # Deliver what the sprint actually accomplished, including ingest of defects arriving during sprint\n",
    "    timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, SPRINT_DURATION, modelsConfig) # mutates work\n",
    "    inflow = timecard.inflowOfTickets(teamId, teamsRepo, ticketsRepo, storiesRepo)\n",
    "    end_committed_df = work.committedTime(0) # Sprint is over, so sprint capacity parameter is 0\n",
    "    \n",
    "    # Test continued\n",
    "    timecard.updateBacklogAfterSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, inflow)\n",
    "    end_tasks_df = work.committedTasks()\n",
    "    \n",
    "    output['Start_Committed'] = start_committed_df\n",
    "    output['Start_Tasks'] = start_tasks_df\n",
    "    output['End_Committed'] = end_committed_df\n",
    "    output['End_Tasks'] = end_tasks_df\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_oneSprint_ACTUAL = test_oneSprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_oneSprint_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Committed'],    'test_oneSprint.Start_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['Start_Tasks'],        'test_oneSprint.Start_Tasks')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Committed'],      'test_oneSprint.End_Committed')\n",
    "    tu_.createExpectedOutput(test_oneSprint_ACTUAL['End_Tasks'],          'test_oneSprint.End_Tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_oneSprint_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_oneSprint_EXPECTED = {}\n",
    "\n",
    "test_oneSprint_EXPECTED['Start_Committed']      = tu_.loadExpectedOutput('test_oneSprint.Start_Committed')\n",
    "test_oneSprint_EXPECTED['Start_Tasks']          = tu_.loadExpectedOutput('test_oneSprint.Start_Tasks')\n",
    "test_oneSprint_EXPECTED['End_Committed']        = tu_.loadExpectedOutput('test_oneSprint.End_Committed')\n",
    "test_oneSprint_EXPECTED['End_Tasks']            = tu_.loadExpectedOutput('test_oneSprint.End_Tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                    'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth']\n",
    "testlets = ['Start_Committed', 'End_Committed']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_oneSprint_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_oneSprint_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved']\n",
    "testlets = ['Start_Tasks', 'End_Tasks']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_oneSprint_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_oneSprint_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_oneSprint_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "tu_.EXPECTED['test_oneSprint.Start_Committed']      = test_oneSprint_EXPECTED['Start_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.Start_Tasks']          = test_oneSprint_EXPECTED['Start_Tasks']\n",
    "tu_.EXPECTED['test_oneSprint.End_Committed']        = test_oneSprint_EXPECTED['End_Committed']\n",
    "tu_.EXPECTED['test_oneSprint.End_Tasks']            = test_oneSprint_EXPECTED['End_Tasks']\n",
    "\n",
    "tu_.ACTUAL['test_oneSprint.Start_Committed']        = test_oneSprint_ACTUAL['Start_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.Start_Tasks']            = test_oneSprint_ACTUAL['Start_Tasks']\n",
    "tu_.ACTUAL['test_oneSprint.End_Committed']          = test_oneSprint_ACTUAL['End_Committed']\n",
    "tu_.ACTUAL['test_oneSprint.End_Tasks']              = test_oneSprint_ACTUAL['End_Tasks']\n",
    "\n",
    "tu_.testOK('test_oneSprint.Start_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.Start_Tasks'), \\\n",
    "tu_.testOK('test_oneSprint.End_Committed'), \\\n",
    "tu_.testOK('test_oneSprint.End_Tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['Start_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_ACTUAL['End_Tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneSprint_EXPECTED['End_Tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_multipleSprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_multipleSprints():\n",
    "    output = {}\n",
    "    # Test many sprints into the future, to see if eventually people have extra time and start using that extra time\n",
    "    # in the current sprint to get a head start on tasks for the next sprint\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 15 \n",
    "    \n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "    \n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             125, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "    # Select a team\n",
    "    teamId = teams_df['Scrum Team'][0].teamId\n",
    "    \n",
    "\n",
    "\n",
    "    work = None\n",
    "    for i in range(NUMBER_OF_SPRINTS):\n",
    "        work = timecard.chooseWhatToDoInSprint(teamId, teamsRepo, storiesRepo, SPRINT_DURATION, modelsConfig, sprint=i+1)\n",
    "        if (i== NUMBER_OF_SPRINTS -1):\n",
    "            break\n",
    "        timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, SPRINT_DURATION, \\\n",
    "                              modelsConfig) # mutates 'work'\n",
    "        inflow = timecard.inflowOfTickets(teamId, teamsRepo, ticketsRepo, storiesRepo)\n",
    "        timecard.updateBacklogAfterSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, work, inflow) # Does not mutate 'work'\n",
    "    \n",
    "    # Work Assignments at the start of the last sprint. Should see some \"looking ahead\" tasks, i.e., tasks that would\n",
    "    # normally be done in the next sprint but are started in this sprint since we have time leftover from this sprint's \n",
    "    # deliverables\n",
    "    last = work\n",
    "    start_committed_df = last.committedTime(10)\n",
    "    start_tasks_df     = last.committedTasks()\n",
    "    \n",
    "    # Finish this last sprint and confirm we spent time in some of the deliveries for the next sprint (i.e., that we\n",
    "    # were \"looking ahead\")\n",
    "    timecard.deliverSprint(teamId, teamsRepo, ticketsRepo, storiesRepo, last, 10, modelsConfig)\n",
    "    end_committed_df = last.committedTime(0)\n",
    "    end_tasks_df     =last.committedTasks()\n",
    "    \n",
    "    output['start_committed']   = start_committed_df\n",
    "    output['start_tasks']       = start_tasks_df\n",
    "    output['end_committed']     = end_committed_df\n",
    "    output['end_tasks']         = end_tasks_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_multipleSprints_ACTUAL = test_multipleSprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_multipleSprints_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_committed'],    'test_multipleSprints.start_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['start_tasks'],        'test_multipleSprints.start_tasks')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_committed'],      'test_multipleSprints.end_committed')\n",
    "    tu_.createExpectedOutput(test_multipleSprints_ACTUAL['end_tasks'],          'test_multipleSprints.end_tasks')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_multipleSprints_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_multipleSprints_EXPECTED = {}\n",
    "\n",
    "test_multipleSprints_EXPECTED['start_committed']    = tu_.loadExpectedOutput('test_multipleSprints.start_committed')\n",
    "test_multipleSprints_EXPECTED['start_tasks']        = tu_.loadExpectedOutput('test_multipleSprints.start_tasks')\n",
    "test_multipleSprints_EXPECTED['end_committed']      = tu_.loadExpectedOutput('test_multipleSprints.end_committed')\n",
    "test_multipleSprints_EXPECTED['end_tasks']          = tu_.loadExpectedOutput('test_multipleSprints.end_tasks')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Rejects (days)', 'Debugging (days)', 'Implementation (days)', 'Bandwidth',\\\n",
    "                    'NEXT SPRINT (days)', 'NEXT SPRINT Bandwidth']\n",
    "testlets = ['start_committed', 'end_committed']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_multipleSprints_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_multipleSprints_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Original Estimate', 'Effort Spent', 'Effort Remaining', 'Percent Achieved']\n",
    "testlets = ['start_tasks', 'end_tasks']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_multipleSprints_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_multipleSprints_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_multipleSprints_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "tu_.EXPECTED['test_multipleSprints.start_committed']    = test_multipleSprints_EXPECTED['start_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.start_tasks']        = test_multipleSprints_EXPECTED['start_tasks']\n",
    "tu_.EXPECTED['test_multipleSprints.end_committed']      = test_multipleSprints_EXPECTED['end_committed']\n",
    "tu_.EXPECTED['test_multipleSprints.end_tasks']          = test_multipleSprints_EXPECTED['end_tasks']\n",
    "\n",
    "tu_.ACTUAL['test_multipleSprints.start_committed']      = test_multipleSprints_ACTUAL['start_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.start_tasks']          = test_multipleSprints_ACTUAL['start_tasks']\n",
    "tu_.ACTUAL['test_multipleSprints.end_committed']        = test_multipleSprints_ACTUAL['end_committed']\n",
    "tu_.ACTUAL['test_multipleSprints.end_tasks']            = test_multipleSprints_ACTUAL['end_tasks']\n",
    "\n",
    "tu_.testOK('test_multipleSprints.start_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.start_tasks'), \\\n",
    "tu_.testOK('test_multipleSprints.end_committed'), \\\n",
    "tu_.testOK('test_multipleSprints.end_tasks'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['start_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_committed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_ACTUAL['end_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multipleSprints_EXPECTED['end_tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_releaseCycle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "# Test logic\n",
    "def test_releaseCycle():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "    NUMBER_OF_SPRINTS = 25\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    \n",
    "\n",
    "\n",
    "    entries_df, worksheets = timecard.runReleaseCycle(teamsRepo, ticketsRepo, storiesRepo, datetime(2018, 1, 15), \\\n",
    "                                                   SPRINT_DURATION, NUMBER_OF_SPRINTS, modelsConfig)    \n",
    "    \n",
    "    anton_df = entries_df[entries_df['Developer'] == 'Anton Easterday']\n",
    "    glenna_df = entries_df[entries_df['Developer'] == 'Glenna Mcghie']\n",
    "        \n",
    "    burnout_df = timecard.releaseBurnout(entries_df)\n",
    "    \n",
    "    output['Entries'] = entries_df\n",
    "    output['Anton'] = anton_df\n",
    "    output['Glenna'] = glenna_df\n",
    "    output['Burnout'] = burnout_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_releaseCycle_ACTUAL = test_releaseCycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_releaseCycle_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Entries'],    'test_releaseCycle.Entries')\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Anton'],      'test_releaseCycle.Anton')\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Glenna'],     'test_releaseCycle.Glenna')\n",
    "    tu_.createExpectedOutput(test_releaseCycle_ACTUAL['Burnout'],    'test_releaseCycle.Burnout')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_releaseCycle_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_releaseCycle_EXPECTED = {}\n",
    "\n",
    "test_releaseCycle_EXPECTED['Entries']      = tu_.loadExpectedOutput('test_releaseCycle.Entries')\n",
    "test_releaseCycle_EXPECTED['Anton']        = tu_.loadExpectedOutput('test_releaseCycle.Anton')\n",
    "test_releaseCycle_EXPECTED['Glenna']       = tu_.loadExpectedOutput('test_releaseCycle.Glenna')\n",
    "test_releaseCycle_EXPECTED['Burnout']      = tu_.loadExpectedOutput('test_releaseCycle.Burnout')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['Time Spent']\n",
    "testlets = ['Entries', 'Anton', 'Glenna']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_releaseCycle_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_releaseCycle_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['Effort']\n",
    "testlets = ['Burnout']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_releaseCycle_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_releaseCycle_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_releaseCycle_ACTUAL[testlet][field] = rounded\n",
    "        \n",
    "# Dates are loaded as strings, not pd.Timestamps, so to avoid spurious mismatches between ACTUAL (which represents dates\n",
    "# as pd.Timestamps) and EXPECTED, convert the EXPECTED dates into pd.Timestamps\n",
    "d = test_releaseCycle_EXPECTED['Entries']['Date']\n",
    "test_releaseCycle_EXPECTED['Entries']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "d = test_releaseCycle_EXPECTED['Anton']['Date']\n",
    "test_releaseCycle_EXPECTED['Anton']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "d = test_releaseCycle_EXPECTED['Glenna']['Date']\n",
    "test_releaseCycle_EXPECTED['Glenna']['Date'] = d.apply(lambda x: pd.Timestamp(datetime.strptime(x, '%Y-%m-%d')))\n",
    "\n",
    "# For output that was filtered (i.e., a subset of rows from a dataframe), the CSV files messes up the index so fix this \n",
    "# to avoid spurious mismatches between ACTUAL and EXPECTED\n",
    "test_releaseCycle_EXPECTED['Anton'].index = test_releaseCycle_ACTUAL['Anton'].index\n",
    "test_releaseCycle_EXPECTED['Glenna'].index = test_releaseCycle_ACTUAL['Glenna'].index\n",
    "\n",
    "tu_.EXPECTED['test_releaseCycle.Entries']        = test_releaseCycle_EXPECTED['Entries']\n",
    "tu_.EXPECTED['test_releaseCycle.Anton']          = test_releaseCycle_EXPECTED['Anton']\n",
    "tu_.EXPECTED['test_releaseCycle.Glenna']         = test_releaseCycle_EXPECTED['Glenna']\n",
    "tu_.EXPECTED['test_releaseCycle.Burnout']        = test_releaseCycle_EXPECTED['Burnout']\n",
    "\n",
    "tu_.ACTUAL['test_releaseCycle.Entries']          = test_releaseCycle_ACTUAL['Entries']\n",
    "tu_.ACTUAL['test_releaseCycle.Anton']            = test_releaseCycle_ACTUAL['Anton']\n",
    "tu_.ACTUAL['test_releaseCycle.Glenna']           = test_releaseCycle_ACTUAL['Glenna']\n",
    "tu_.ACTUAL['test_releaseCycle.Burnout']          = test_releaseCycle_ACTUAL['Burnout']\n",
    "\n",
    "tu_.testOK('test_releaseCycle.Entries'), \\\n",
    "tu_.testOK('test_releaseCycle.Anton'), \\\n",
    "tu_.testOK('test_releaseCycle.Glenna'), \\\n",
    "tu_.testOK('test_releaseCycle.Burnout'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Anton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Anton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Glenna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Glenna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_ACTUAL['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_releaseCycle_EXPECTED['Burnout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_FOO: template for tests that produce a DataFrame output </h1>\n",
    "<li> Copy to create a new test, and replace 'FOO' by the name of your test.\n",
    "<li> Implement test logic in test_FOO()\n",
    "<li> To create expected output once test logic is producing right output, temporarily uncomment the call to createExpectedOutput, call it, and comment it again. This saves expected output as a CSV file so subsequent runs of the test can load it and verify that the test still produces the same output as before.\n",
    "<li> Uncomment lines that load expected output and remove the dummy line next to it creating a dummy expected output.\n",
    "<li> Display the outputs for convenience in debugging when test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_FOO():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel(SPRINT_DURATION)) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, teamsRepo, storiesRepo, ticketsRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                                             RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "\n",
    "    dummyA_df = pd.DataFrame({'A': [45.123456789, 34.123456789]})\n",
    "    dummyB_df = pd.DataFrame({'B': [2333.123456789, 3984.123456789]})\n",
    "    \n",
    "    output['Dummy_A'] = dummyA_df\n",
    "    output['Dummy_B'] = dummyB_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_FOO_ACTUAL = test_FOO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_FOO_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_A'],    'test_FOO.Dummy_A')\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_B'],    'test_FOO.Dummy_B')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_FOO_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "test_FOO_EXPECTED = {}\n",
    "\n",
    "test_FOO_EXPECTED['Dummy_A']      = tu_.loadExpectedOutput('test_FOO.Dummy_A')\n",
    "test_FOO_EXPECTED['Dummy_B']      = tu_.loadExpectedOutput('test_FOO.Dummy_B')\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "sensitive_fields = ['A']\n",
    "testlets = ['Dummy_A']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_FOO_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_FOO_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_ACTUAL[testlet][field] = rounded\n",
    "sensitive_fields = ['B']\n",
    "testlets = ['Dummy_B']\n",
    "for field in sensitive_fields:\n",
    "    for testlet in testlets:\n",
    "        rounded = test_FOO_EXPECTED[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_EXPECTED[testlet][field] = rounded\n",
    "        rounded = test_FOO_ACTUAL[testlet][field].apply(lambda x: round(x, 6)) # Round to 6 decimal places\n",
    "        test_FOO_ACTUAL[testlet][field] = rounded\n",
    "\n",
    "\n",
    "tu_.EXPECTED['test_FOO.Dummy_A']        = test_FOO_EXPECTED['Dummy_A']\n",
    "tu_.EXPECTED['test_FOO.Dummy_B']        = test_FOO_EXPECTED['Dummy_B']\n",
    "\n",
    "tu_.ACTUAL['test_FOO.Dummy_A']          = test_FOO_ACTUAL['Dummy_A']\n",
    "tu_.ACTUAL['test_FOO.Dummy_B']          = test_FOO_ACTUAL['Dummy_B']\n",
    "\n",
    "tu_.testOK('test_FOO.Dummy_A'), \\\n",
    "tu_.testOK('test_FOO.Dummy_B'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run all the tests in this notebook</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all the tests defined in this notebook\n",
    "#unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
