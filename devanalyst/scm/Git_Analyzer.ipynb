{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Low-level utilities</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Saves the 'dataset_df' dataframe to a CSV file under the given 'directory'. We treat the persisted data as a\n",
    "# snapshot as of a point in time because the CSV filename is built out of a timestamp (today) and the given \n",
    "# 'snapshot_name'.\n",
    "# Example: if today is August 5, 2019 and the 'snaphost_name' parameter is \"ticket_enriched_commits\", then the\n",
    "# dataframe is saved to \"<directory>/190805 ticket_enriched_commits.csv\"\n",
    "def snapshot_dataset(dataset_df, directory, snapshot_name):\n",
    "    d = datetime.today()\n",
    "    date_tag = d.strftime('%y%m%d') # We prefix filenames with strings like '190628' for June 28, 2019 (date of extract)\n",
    "    if dataset_df is not None:\n",
    "        dataset_df.to_csv(directory + '/' + date_tag + ' ' + snapshot_name + '.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "# Retuns a dataframe corresponding to a dataset snapshot that is presumed to exist in the given directory\n",
    "# and be time-stamped with one of the date prefixes in the 'dates' list.\n",
    "#\n",
    "# -directory: a string, for the full path for the directory where the dataset was previously saved as a snapshot\n",
    "# -dates: a list of date prefixes (a prefix is a string like '190805' for an August 5, 2019 timestamp). These \n",
    "#         defines the acceptable timestamps for the data set we week. Only one data set is returned, so if there\n",
    "#         are multiple datasets in the directories all for the given 'snapshot_name', then the one that has the\n",
    "#         latest timestamp in 'dates' is chosen to be returned.\n",
    "# -snapshot_name: a string, corresponding to the logical name of the snapshot we seek.\n",
    "def load_dataset_snapshot(directory, dates, snapshot_name):\n",
    "    for date_prefix in dates:\n",
    "        filename = directory + '/' + date_prefix + ' ' + snapshot_name +'.csv'\n",
    "        if os.path.isfile(filename):\n",
    "            # When reading CSV file, explicitly state dtype for columns where the act of reading the\n",
    "            # CSV file might trigger a warning of \"mixed types\". Each time we get such a warning,\n",
    "            # add one more column here where we specify the dtype for that column so warning is avoided\n",
    "            df                    = pd.read_csv(filename, dtype={'Body':str, \n",
    "                                                                 'Symbolic Link':str})\n",
    "            df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "            foundData             = True\n",
    "            return df # Found a date for which we had data, so no need to look at more dates\n",
    "    # If we get this far, we didnt find a snapshot for any of the date_prefixes\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "# Displays a barchart plot. The x-axis is taken from the first column of 'commits_df' parameter, and the y-axis is\n",
    "# taken from the values in the second column of the 'commits_df'. However, there is a twist: the x-values are \n",
    "# sorted in order of decreasing y-values, and then rendered. Thus the bar chart is decreasing unless the \n",
    "# 'cumulative' parameter is set to True.\n",
    "#\n",
    "# -commits_df: a dataframe with two columns: developer (a string), and an integer column for the number of commits \n",
    "#  done by that developer.\n",
    "# -cumulative: boolean to determine whether the y-axis represents the cumulative number of commits\n",
    "def chartCommits(commits_df, cumulative=False):\n",
    "    col = list(commits_df.columns)[1]\n",
    "    sorted_series = commits_df[col].sort_values(ascending=False)\n",
    "    if (not cumulative):\n",
    "        sorted_series.T.plot(kind='bar', figsize=[8, 4])\n",
    "    else:\n",
    "        cum_df = sorted_series.cumsum() /sorted_series.sum()\n",
    "        cum_df.T.plot(kind= 'bar', figsize=[8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# -df: dataframe holding the data. The column and index headers will be used as the x and y axis values for \n",
    "#      the heatmap.\n",
    "# -bounds: an np.array of numbers, marking the levels around which a particular color should be centered in the heatmap\n",
    "def plotHeatmap(df, title, bounds):\n",
    "    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    hm = sns.heatmap(df, annot=True, ax=ax,fmt='.0f',\n",
    "                     linewidths=.05, norm=norm)\n",
    "    f.subplots_adjust(top=0.93)\n",
    "    t= f.suptitle(title, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "# Searches whether a string appears in the requirements behind a dataframe of commits. Specifically, this\n",
    "# returns two lists and a dataframe: \n",
    "#    1) a list of the 'Subject' fields for rows in 'commits_df' that contains the given 'word'\n",
    "#    2) a list of the 'TicketIds' involved\n",
    "#    3) a dataframe obtained by filtering 'commits_df' by filtering for the rows for the 'TicketIds' involved\n",
    "\n",
    "# The lists may have different lengths since a given ticket may appear in the subject of multiple commits\n",
    "#\n",
    "# -commits_df: a dataframe with commit information per row, including columns called 'Subject' and 'Ticket(s)'\n",
    "# -word: a string that we are searching for in the 'commits_df'\n",
    "def searchInReqs(commits_df, word):\n",
    "    subjects = set()\n",
    "    tickets = set()\n",
    "    included_rows = []\n",
    "    for index, row in commits_df.iterrows():\n",
    "        req = row['Subject']\n",
    "        \n",
    "        ticket = row['Ticket(s)']\n",
    "        if type(ticket) == float and math.isnan(ticket):\n",
    "            #Skip this row, it is not a real ticket\n",
    "            included_rows.append(False)\n",
    "            continue\n",
    "        \n",
    "        if word in req:\n",
    "            subjects.add(req)\n",
    "            tickets.add(row['Ticket(s)'])\n",
    "            included_rows.append(True)\n",
    "        else:\n",
    "            included_rows.append(False)\n",
    "\n",
    "    filtered_df = commits_df[included_rows]\n",
    "    \n",
    "    #Sort tickets list, easier to cross-reference\n",
    "    tickets_list = list(tickets)\n",
    "    tickets_list.sort()\n",
    "    \n",
    "    return list(subjects), tickets_list, filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Library to ingest data from GIT repos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class GitLogStatics:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    START_COMMIT_RECORD = '______START_COMMIT_RECORD______'\n",
    "    COMMITID            = 'CommitId'\n",
    "    SUBJECT             = 'Subject'\n",
    "    AUTHOR              = 'Author'\n",
    "    AUTHOR_EMAIL        = 'Author_email'\n",
    "    AUTHOR_DATE         = 'Author_date'\n",
    "    COMMITTER           = 'Committer'\n",
    "    COMMITTER_EMAIL     = 'Committer_email'\n",
    "    COMMITTER_DATE      = 'Committer_date'\n",
    "    BODY_STARTS         = 'Body_Starts'\n",
    "    END_BODY            = 'End_Body'\n",
    "    FILES_CHANGED       = 'Files_changed'\n",
    "    \n",
    "    # It is important the the 'FORMAT' string contain no spaces. Otherwise the subprocess.communicate() call will fail\n",
    "    # silently and produce no output. For that reason we use a '.' (dot) in lieu of ' ' (a space)\n",
    "    FORMAT = '' \\\n",
    "    + '%Cred______START_COMMIT_RECORD______%Creset' \\\n",
    "    + '%n' + COMMITID + '..........%H' \\\n",
    "    + '%n' + SUBJECT + '...........%s' \\\n",
    "    + '%n' + AUTHOR + '............%an' \\\n",
    "    + '%n' + AUTHOR_EMAIL + '......%ae' \\\n",
    "    + '%n' + AUTHOR_DATE + '.......%ad' \\\n",
    "    + '%n' + COMMITTER + '.........%cn' \\\n",
    "    + '%n' + COMMITTER_EMAIL + '...%ce' \\\n",
    "    + '%n' + COMMITTER_DATE + '....%cd' \\\n",
    "    + '%n' + BODY_STARTS + '.......%b' \\\n",
    "    + '%n' + END_BODY \\\n",
    "    + '%n' + FILES_CHANGED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ChangedLine():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.filename    = None\n",
    "        self.loc         = None\n",
    "        self.loc_added   = None\n",
    "        self.loc_removed = None\n",
    "        self.loc_changed = None\n",
    "        self.other       = None\n",
    "        \n",
    "    def buildUp(self, filename, raw_loc_changes): \n",
    "        self.filename = filename.strip()\n",
    "        \n",
    "        self.loc, self.loc_added, self.loc_removed, self.loc_changed, self.other = self._parseLocChanges(raw_loc_changes)\n",
    "        \n",
    "    def _parseLocChanges(self, raw): # raw is like '109 +++++--?' or 'Bin 0 -> 5190 bytes'\n",
    "        REGEX = '^[0-9]+'\n",
    "        m = re.search(REGEX, raw)\n",
    "        if m != None:\n",
    "            loc = int(m.group(0))\n",
    "            # Ratios among '+', '-', '?' indicate what proportion of the 'loc'-many lines were added, removed, or changed\n",
    "            p = raw.count('+')\n",
    "            m = raw.count('-')\n",
    "            q = raw.count('?')\n",
    "            t = p + m + q\n",
    "            added = 0\n",
    "            removed = 0\n",
    "            changed = 0\n",
    "            if t != 0:\n",
    "                added = loc * p/t\n",
    "                removed = loc* m/t\n",
    "                changed = loc* q/t\n",
    "            return loc, added, removed, changed, None\n",
    "        else:\n",
    "            return 0,0,0,0, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     1,
     13,
     27,
     41,
     54,
     61
    ]
   },
   "outputs": [],
   "source": [
    "class CommitParcel():\n",
    "    def __init__(self):\n",
    "        self.commitId        = None\n",
    "        self.subject         = None\n",
    "        self.author          = None\n",
    "        self.author_email    = None\n",
    "        self.author_date     = None\n",
    "        self.committer       = None\n",
    "        self.committer_email = None\n",
    "        self.committer_date  = None\n",
    "        self.body            = None\n",
    "        self.files_changed   = []\n",
    "        \n",
    "    def buildUp(self, commit_lines):\n",
    "        \n",
    "        self.commitId        = self._stripLabel(commit_lines[1], GitLogStatics.COMMITID)\n",
    "        self.subject         = self._stripLabel(commit_lines[2], GitLogStatics.SUBJECT)\n",
    "        self.author          = self._stripLabel(commit_lines[3], GitLogStatics.AUTHOR)\n",
    "        self.author_email    = self._stripLabel(commit_lines[4], GitLogStatics.AUTHOR_EMAIL)\n",
    "        self.author_date     = self._stripLabel(commit_lines[5], GitLogStatics.AUTHOR_DATE)\n",
    "        self.committer       = self._stripLabel(commit_lines[6], GitLogStatics.COMMITTER)\n",
    "        self.committer_email = self._stripLabel(commit_lines[7], GitLogStatics.COMMITTER_EMAIL)\n",
    "        self.committer_date  = self._stripLabel(commit_lines[8], GitLogStatics.COMMITTER_DATE)\n",
    "        \n",
    "        next_idx = self._readBody(commit_lines)\n",
    "        self._readFilesChanged(commit_lines, next_idx)\n",
    "        \n",
    "    def _readBody(self, commit_lines):       \n",
    "            idx = 9\n",
    "            \n",
    "            result = self._stripLabel(commit_lines[idx], GitLogStatics.BODY_STARTS)\n",
    "\n",
    "            idx += 1\n",
    "            while idx < len(commit_lines):\n",
    "                if self._hasLabel(commit_lines[idx], GitLogStatics.END_BODY): # Done with body\n",
    "                    break\n",
    "                result += '\\n' + commit_lines[idx]\n",
    "                idx += 1\n",
    "            self.body = result\n",
    "            return idx\n",
    "        \n",
    "    def _readFilesChanged(self, commit_lines, start_idx):\n",
    "        #Label indicating when this beings is GitLogStatics.FILES_CHANGED\n",
    "        idx = start_idx\n",
    "        while idx < len(commit_lines):\n",
    "            line = commit_lines[idx]\n",
    "            if line.find('|') != -1:\n",
    "                tokens = line.split('|')\n",
    "                assert len(tokens)== 2, tokens\n",
    "                change = ChangedLine()\n",
    "                change.buildUp(tokens[0].strip(), tokens[1].strip())  \n",
    "                self.files_changed.append(change)\n",
    "            idx += 1\n",
    "            \n",
    "    def _stripLabel(self, line, label): \n",
    "            REGEX  = '^' + label + '[.]*' # like 'Subject...........'\n",
    "            m = re.search(REGEX, line)\n",
    "            assert m!=None, line\n",
    "            prefix = m.group(0)  \n",
    "            return line[len(prefix):].strip()\n",
    "        \n",
    "    def _hasLabel(self, line, label):\n",
    "            REGEX  = '^' + label + '[.]*' # like 'Subject...........'\n",
    "            m = re.search(REGEX, line)\n",
    "            if m==None:\n",
    "                return False\n",
    "            else:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     27,
     85,
     104,
     137,
     148
    ]
   },
   "outputs": [],
   "source": [
    "class FilenameParser:   \n",
    "    # -filename: a string, for the full path in a GIT repo for an artifact, with '/' separators. Might look like:\n",
    "    #\n",
    "    #     'MisysPD/UniversalBanking/BFUBInfrastructure/src/com/misys/ub/fatoms/batch/common/uxEnhancement/BatchUXEnhancementAccumulator.java'\n",
    "    #\n",
    "    # -layer_depth: integer stating how many folders in the filename's path correspond to the highest level (self.layer)\n",
    "    #               as apposed to a submodule. In the above example a 'layer_depth' of 2 results in\n",
    "    #\n",
    "    #           self.layer     = 'MisysPD/UniversalBanking'\n",
    "    #           self.submodule = 'BFUBInfrastructure'\n",
    "    #\n",
    "    # -package_folders: a list of strings, each of them being the name used for folders containing packages.\n",
    "    #              For example, for Java the 'package_folder' is usually '[src]' or possibly '[src, src-gen, lib, dist]'\n",
    "    #              if there are generated Java source files as well as other artifacts besides Java files\n",
    "    # -artifact_families: Used to produce a more meaningful artifact_type and artifact_family in cases where:\n",
    "    #                     1) an additional suffixed appended at the end which might be spurious, like a '.bak' suffix.\n",
    "    #                     2) or we want to explicitly say what name to use for the artifact_family of some artifact_types\n",
    "    #       More formally: parameter is a dictionary whose values are lists of strings, such as:\n",
    "    #\n",
    "    #                          {'java':         ['java'],\n",
    "    #                           'BF Artefact':  ['bfg', 'ast', 'bod']]\n",
    "    #\n",
    "    #                     In this example, any file that ends in 'java', 'java.bak', 'java.foo.bar' will thake those values\n",
    "    #                     as the 'artifact_type', but their 'artifact_family' woulbe 'java.'\n",
    "    #                     Also in this example, any file finishing in 'bfg', 'bfg.store', 'bfg.mo', 'ast', 'bod.bak' would\n",
    "    #                     take those values as the 'artifact_type', and the 'artifact_family' would be 'BF Artefact'\n",
    "    #                     \n",
    "    def __init__(self, filename, layer_depth=2, package_folders=['src'], artifact_families=[], package_prefixes=[]):\n",
    "        self.filename               = filename\n",
    "        self.layer_depth            = layer_depth\n",
    "        self.package_folders        = package_folders\n",
    "        self.artifact_families      = artifact_families\n",
    "        self.package_prefixes       = package_prefixes\n",
    "        \n",
    "        self.artifact_type          = None\n",
    "        self.artifact_family        = None\n",
    "        self.layer                  = None\n",
    "        self.submodule              = None \n",
    "        self.package_type           = None\n",
    "        self.package                = None\n",
    "        self.classname              = None\n",
    "        self.symbolic_link          = None\n",
    "        \n",
    "    def parse(self):\n",
    "        \n",
    "        #  Comments below are with regards to this example for 'self.filename':\n",
    "        #\n",
    "        # 'MisysPD/UniversalBanking/BFUBInfrastructure/src/com/misys/ub/fatoms/batch/common/\n",
    "        #                                                 uxEnhancement/BatchUXEnhancementAccumulator.java'\n",
    "        #\n",
    "        # If 'self.filename' does not include a 'src' folder in its path, then the 'self.package' is the full path\n",
    "        \n",
    "        massaged_filename, link = self._stripSymbolicLinks()\n",
    "        self.symbolic_link      = link\n",
    "        tokens                  = massaged_filename.split('/')\n",
    "\n",
    "        self._extractArtifactType(tokens) # 'java' for both self.artifact_type and self.artifact_family\n",
    "\n",
    "        idx_module_ends         = len(tokens)-2 #Default value, may be changed in loop below if we have a source file\n",
    "        idx_package_starts      = idx_module_ends # By default we assume there is no 'package_type', so skipping it\n",
    "        self.package_type       = ''            #Default value, may be changed in loop below if we have a source file\n",
    "        self.package            = ''            #Default value, may be changed in loop below if we have a source file\n",
    "        \n",
    "        src = self._searchPackageDivider(tokens)\n",
    "        if src != None: \n",
    "            idx_module_ends    = tokens.index(src)\n",
    "            idx_package_starts = idx_module_ends + 1 # Shift when package starts by 1, to make room for a package_type\n",
    "        else: # No package divider, but check if we have a match of package prefixes\n",
    "            idx = self._findWherePrefixStarts(tokens)\n",
    "            if idx != None:\n",
    "                idx_package_starts = idx\n",
    "                idx_module_ends    = idx\n",
    "\n",
    "        self.package      = '.'.join(tokens[idx_package_starts:-1]) # like 'com.misys.ub.fatoms.batch.common.uxEnhancement'\n",
    "        if idx_module_ends < idx_package_starts: # There is room for a package_type\n",
    "            self.package_type = tokens[idx_module_ends] # like 'src'\n",
    "\n",
    "\n",
    "        module_tokens       = tokens[:idx_module_ends] # like '[MisysPD, UniversalBanking, BFUBInfrastructure]\n",
    "        DEPTH               = self.layer_depth\n",
    "        self.layer          = '/'.join(module_tokens[:DEPTH]) # like 'MisysPD/UniversalBanking'\n",
    "        self.submodule      = '/'.join(module_tokens[DEPTH:]) # like 'BFUBInfrastructure'\n",
    "                                          \n",
    "        self.classname      = tokens[-1:][0]  # like 'BatchUXEnhancementAccumulator.java'\n",
    "        \n",
    "    def _searchPackageDivider(self, tokens):\n",
    "        best_idx = len(tokens) #deliberately out of bounds, signifying we don't have any matches yet\n",
    "        for src in self.package_folders:\n",
    "            # Match things like 'src', 'src-test', 'test-src', or 'api-src'. Priority for first match          \n",
    "            REGEX = '(^' + src + '$)|(^' + src + '-[a-zA-Z]+$)|(^[a-zA-Z]+-' + src + '$)'\n",
    "            for idx in range(len(tokens)):\n",
    "                tok = tokens[idx]\n",
    "                m = re.search(REGEX, tok)\n",
    "                if m!=None: # Found a match for a divider\n",
    "                    if idx < best_idx: # This match is better than what we had so far, so make it the front runner\n",
    "                        best_idx = idx\n",
    "                else:\n",
    "                    continue # Try next match\n",
    "        # Return the best match, if there is one. Else null\n",
    "        if best_idx < len(tokens):\n",
    "            return tokens[best_idx]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _findWherePrefixStarts(self, tokens): #find where in tokens a prefix like 'com/misys' occurs, if at all\n",
    "        result_idx = None\n",
    "        for prefix in self.package_prefixes: # prefix is like 'com/misys'\n",
    "            prefix_tokens = prefix.split('/')\n",
    "            if len(prefix_tokens)==0: #boundary case - ignore empty prefixes\n",
    "                continue\n",
    "            for idx in range(len(tokens)): #Attempt to match starting at each idx, until we succeed or exhaust options\n",
    "                tok = tokens[idx]\n",
    "                if prefix_tokens[0]==tok: #This may be the start of a match. See if all other prefix tokens match as well\n",
    "                    if len(tokens[idx:]) < len(prefix_tokens):\n",
    "                        continue # Can't match if the full prefix won't fit in the space of the remaining tokens\n",
    "                    else:\n",
    "                        so_far_so_good = True\n",
    "                        for prefix_idx in range(len(prefix_tokens)): #match all the prefix tokens\n",
    "                            if prefix_tokens[prefix_idx] != tokens[idx + prefix_idx]:\n",
    "                                so_far_so_good = False\n",
    "                                break #no match, so give up, and try next tok\n",
    "                        if so_far_so_good==True: # We didn't invalidate this, so a match was found\n",
    "                            result_idx = idx\n",
    "                            return result_idx\n",
    "                        else:\n",
    "                            continue # try another tok to start the attempt to match prefix\n",
    "                            \n",
    "            # We got this far without matching any prefix to any consecutive subset of the tokens. So no match\n",
    "            result_idx = None\n",
    "            return result_idx\n",
    "                                        \n",
    "    # Used to massage filenames that contain symbolic links, by replacing the link by the actual file. For example,\n",
    "    # if 'self.filename' is:\n",
    "    # \n",
    "    #     'MisysPD/UniversalBanking/{BFUBRetail/src-gen => ReferencedBOs/src}/com/trapedza/bankfusion/bo/refimpl/IBOUB_BLK_StaticAccountBlock.java'\n",
    "    #\n",
    "    # then the symbolic link portion '{BFUBRetail/src-gen => ReferencedBOs/src}' gets replaced by 'ReferencedBOs/src'\n",
    "    def _stripSymbolicLinks(self):\n",
    "        REGEX = '{[ _.0-9a-zA-Z/-]* => [ _.0-9a-zA-Z/-]+}'\n",
    "        m = re.search(REGEX, self.filename)\n",
    "        if m != None:\n",
    "            link = m.group(0) # Like '{BFUBRetail/src-gen => ReferencedBOs/src}'\n",
    "            link_tokens = link[1:-1].split(' => ') # like '[BFUBRetail/src-gen, ReferencedBOs/src]'\n",
    "            massaged_filename = self.filename.replace(link, link_tokens[1])\n",
    "            return massaged_filename, link\n",
    "        else:\n",
    "            return self.filename, ''\n",
    "\n",
    "    def _extractArtifactType(self, tokens):\n",
    "        pathless_filename = tokens[-1:][0] # Strips the pathname, leaving like BatchUXEnhancementAccumulator.java'\n",
    "        \n",
    "        words             = pathless_filename.split('.')\n",
    "\n",
    "        if len(words) < 2: # There is no sufix to the file\n",
    "            self.artifact_type   = ''\n",
    "            self.artifact_family = ''\n",
    "            return\n",
    "\n",
    "        # If we get this far, a suffix does exist. First test if this filename is in designated artifact family\n",
    "        for key in self.artifact_families.keys():\n",
    "            possible_types = self.artifact_families[key]\n",
    "            for candidate in possible_types:\n",
    "                if candidate in words:\n",
    "                    idx_candidate        = words.index(candidate)\n",
    "                    self.artifact_type   = '.'.join(words[idx_candidate:])\n",
    "                    self.artifact_family = key\n",
    "                    return\n",
    "            \n",
    "        # If we get this far, there are no special artifact families. Use the default implementation\n",
    "        self.artifact_type   = words[-1:][0] # like 'java' or, if multiple suffixes are used, e.g. like 'properties.bak', returns 'bak'\n",
    "        self.artifact_family = self.artifact_type\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     3,
     29,
     98
    ]
   },
   "outputs": [],
   "source": [
    "# Catalogues all the contents of a GIT repo into a dataframe, one row per artifact\n",
    "class GitRepoCataloguer:\n",
    "    \n",
    "    def __init__(self, repo_directory, layer_depth, package_folders, artifact_families, package_prefixes):\n",
    "        self.repo_directory    = repo_directory\n",
    "        \n",
    "        self.layer_depth       = layer_depth\n",
    "        self.package_folders   = package_folders\n",
    "        self.artifact_families = artifact_families\n",
    "        self.package_prefixes  = package_prefixes\n",
    "        self.repo_df           = None # Populated when self.load is called\n",
    "\n",
    "   \n",
    "    # Populates and returns self.repo_df, with each row containing information about each artifact in self.repo, \n",
    "    # such as the filename, loc, artifact type, and some module classification based on the filename\n",
    "    #\n",
    "    # It also returns an array of the entries that produced errorsand an array of ignored entries\n",
    "    def catalogueRepoContents(self):\n",
    "        result_dict = {'Filename': [], 'LOC': []}\n",
    "        DIR = self.repo_directory\n",
    "\n",
    "        # Include the -print0 and -0 options to ensure filenames with spaces don't cause problems\n",
    "        # DOES NOT WORK: raw_catalogue = subprocess.getoutput('find ' + DIR + ' -type f | xargs wc -l').split('\\n')\n",
    "        raw_catalogue = subprocess.getoutput('find ' + DIR + ' -type f -print0 | xargs -0 wc -l').split('\\n')\n",
    "            \n",
    "        df, ERRORS, IGNORED = self._build_df(raw_catalogue)\n",
    "        self.repo_df = df\n",
    "        return self.repo_df, ERRORS, IGNORED\n",
    "    \n",
    "    def _build_df(self, raw_catalogue):\n",
    "\n",
    "        DIR = self.repo_directory\n",
    "        ERRORS  = []\n",
    "        IGNORED = []\n",
    "\n",
    "        repo_list            = []\n",
    "        artifact_type_list   = []\n",
    "        artifact_family_list = [] \n",
    "        layer_list           = []\n",
    "        submodule_list       = []\n",
    "        package_list         = []\n",
    "        package_type_list    = []\n",
    "        classname_list       = []\n",
    "        symbolic_link_list   = []\n",
    "        filename_list        = []\n",
    "        loc_list             = []\n",
    "\n",
    "        repo_name = DIR.split('/')[-1]\n",
    "        filename = None\n",
    "        for entry in raw_catalogue:\n",
    "            \n",
    "            filename, loc, status_code = self._extractFilenameAndLoc(entry)\n",
    "            \n",
    "            if status_code == 1: # this tells us its an error\n",
    "                ERRORS.append(entry)\n",
    "                continue\n",
    "            if status_code == 2: # this entry can be ignored\n",
    "                IGNORED.append(entry)\n",
    "                continue\n",
    "\n",
    "            # If we get this far, filename should be good\n",
    "            assert(filename != None)\n",
    "\n",
    "            repo_list            .append(repo_name)\n",
    "            fp = FilenameParser(filename, \n",
    "                                layer_depth         = self.layer_depth, \n",
    "                                package_folders     = self.package_folders,\n",
    "                                artifact_families   = self.artifact_families,\n",
    "                                package_prefixes    = self.package_prefixes)\n",
    "            fp.parse()\n",
    "\n",
    "            artifact_type_list    .append(fp.artifact_type)\n",
    "            artifact_family_list  .append(fp.artifact_family)\n",
    "            layer_list            .append(fp.layer)\n",
    "            submodule_list        .append(fp.submodule)\n",
    "            package_type_list     .append(fp.package_type)\n",
    "            package_list          .append(fp.package)\n",
    "            classname_list        .append(fp.classname)\n",
    "            symbolic_link_list    .append(fp.symbolic_link)\n",
    "            filename_list         .append(filename)\n",
    "            loc_list              .append(loc)\n",
    "\n",
    "        result_dict = {'Repo': repo_list, 'Artifact Family': artifact_family_list, \n",
    "                       'Artifact Type': artifact_type_list,\n",
    "                       'Submodule': submodule_list, 'Package Type': package_type_list,\n",
    "                       'Package': package_list, 'Classname': classname_list,\n",
    "                       'Layer': layer_list, 'Loc': loc_list,'Symbolic Link': symbolic_link_list,'Filename': filename_list, \n",
    "                      }\n",
    "        df = pd.DataFrame(result_dict)\n",
    "        return df, ERRORS, IGNORED\n",
    "    \n",
    "    # Returns a filename (string), a loc (int), and a status (int). \n",
    "    # Status values are: \n",
    "    #     0: processing successful, and filename and loc have valid values\n",
    "    #     1: processing erroneous, and filename and loc are null\n",
    "    #     2: processing ignored, since the input 'entry' is ignorable noise. Returned filename and loc are null.\n",
    "    #\n",
    "    # -entry: a string, a line in the 'raw catalogue' produced by the shell command 'find DIR -type f | xargs wc -l'\n",
    "    def _extractFilenameAndLoc(self, entry): \n",
    "        DIR = self.repo_directory\n",
    "\n",
    "        tokens          = entry.split() # pair like ['1104', 'C:/Alex/Code/Essence/ansible/Essence/roles/oradb-create/templates/dbca-create-db.rsp.11.2.0.4.j2']\n",
    "        if len(tokens) < 2:\n",
    "            return None, None, 1\n",
    "\n",
    "        if tokens[0].isdigit():\n",
    "            loc           = int(tokens[0])\n",
    "        else: #this is an error, a bad one\n",
    "            return None, None, 1\n",
    "\n",
    "        #full_filename = tokens[1] \n",
    "        #full_filename = entry.strip().split(tokens[0])[1].strip() \n",
    "\n",
    "        full_filename = entry.strip()[len(tokens[0]):].strip() # strip *original* entry to preserve spaces in filename, if any\n",
    "\n",
    "        if full_filename.startswith(DIR +'/.git'): #Ignore the underlying .git directories\n",
    "            return None, None, 2\n",
    "        \n",
    "        if full_filename == 'total': #ignore spurious line added by the linux 'find' command\n",
    "            return None, None, 2\n",
    "        \n",
    "        split_filename = full_filename.split(DIR)\n",
    "        if len(split_filename) < 2:\n",
    "            return None, None, 1          \n",
    "\n",
    "        # This is the normal case and if get this far things should work\n",
    "        filename      = full_filename.split(DIR)[1] \n",
    "        \n",
    "        return filename, loc, 0\n",
    "    \n",
    "    # Saves the self.repo_df if it has been loaded already\n",
    "    def save(self, directory):\n",
    "        '''\n",
    "        d = datetime.today()\n",
    "        date_tag = d.strftime('%y%m%d') # We prefix filenames with strings like '190628' for June 28, 2019 (date of extract)\n",
    "        if self.repo_df is not None:\n",
    "            self.repo_df.to_csv(directory + '/' + date_tag + ' Repo Catalogue.csv') \n",
    "        '''\n",
    "        snapshot_dataset(self.repo_df, directory, 'Repo Catalogue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Catalogues indicative information about artifacts in all repos, such as filenames, and stores it in a CSV file in the\n",
    "# 'root_data_dir' with today's timestamp as a prefix to the CSV file's name.\n",
    "def catalogueAllRepos(repos, root_data_dir, root_git_dir,\n",
    "                         package_folders, artifact_families, package_prefixes):\n",
    "    ERRORS_DICT = {}\n",
    "    IGNORED_DICT = {}\n",
    "    print('*** Cataloguing repos under ' + root_git_dir + '***')\n",
    "    for repo in repos:\n",
    "        REPO_DIR = root_git_dir + '/' + repo\n",
    "        DATA_DIR = root_data_dir + '/' + repo\n",
    "        if not os.path.isdir(DATA_DIR):\n",
    "            os.mkdir(DATA_DIR)\n",
    "        print('Cataloguing ' + repo + '...')\n",
    "        cataloguer = GitRepoCataloguer(REPO_DIR, layer_depth=2, package_folders=package_folders,\n",
    "                             artifact_families=artifact_families, package_prefixes=package_prefixes)\n",
    "        df, ERRORS, IGNORED = cataloguer.catalogueRepoContents()\n",
    "        ERRORS_DICT[repo] = ERRORS\n",
    "        IGNORED_DICT[repo] = IGNORED\n",
    "\n",
    "        if df.index.size==0:\n",
    "            print('...' + repo + ' is empty; Nothing will be saved. Errors=' + str(len(ERRORS)) \n",
    "                  + ', Ignored=' + str(len(IGNORED)))\n",
    "        else:\n",
    "            cataloguer.save(DATA_DIR)\n",
    "            print('...done cataloguing ' + repo +'; found ' + str(len(df.index)) \n",
    "                  + ' artifacts, ' + str(len(ERRORS)) + ' ERRORS, and another ' + str(len(IGNORED)) + ' where safely ignored')\n",
    "            \n",
    "    return ERRORS_DICT, IGNORED_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     67,
     143,
     201,
     214
    ]
   },
   "outputs": [],
   "source": [
    "class GitLogParser:\n",
    "    # -git_directory: a string with the full path to the directory in which the git project exists and git commands\n",
    "    #                 can be run successfully. For example, c:/Alex/Code/Essence/ubrepos'\n",
    "    # -after_date: either None, or must be a string in format MM/DD/YY, such as '06/01/18' for the 1st of June, 2018.\n",
    "    #              Used to filter log entries to only include commits after that date.\n",
    "    # -before_date: similar to 'after_date', but excluding commits before the date.\n",
    "    # -max_commits: integer used to restrict how many commits to retrieve. If set to 'None' then no restriction is imposed.\n",
    "    # -diff_line_width: integer to determine how many characters to include in the diff lines product by \n",
    "    #                   'git log --stat'. Should be big enough that paths of changed files are not truncated, since\n",
    "    #                   the 'GitLogParser' needs the full paths to accurately match changed files to entries\n",
    "    #                   in the git repo. The default value of 450 should usually be enough.\n",
    "    # -layer_depth: integer stating how many folders in the filename's path correspond to the highest level (self.layer)\n",
    "    #               as apposed to a submodule. In the above example a 'layer_depth' of 2 results in\n",
    "    #\n",
    "    #           self.layer     = 'MisysPD/UniversalBanking'\n",
    "    #           self.submodule = 'BFUBInfrastructure'\n",
    "    #\n",
    "    # -package_folders, artifact_families, package_prefixes: see documentation of the FilenameParser class.\n",
    "    def __init__(self, git_directory, repo, after_date, before_date=None, max_commits=None, \n",
    "                 diff_line_width   =450,\n",
    "                 layer_depth       =2, \n",
    "                 package_folders   = ['src'],\n",
    "                 artifact_families =[],\n",
    "                 package_prefixes  =[]):\n",
    "        self.git_directory     = git_directory\n",
    "        self.repo              = repo\n",
    "        self.after_date        = after_date\n",
    "        self.before_date       = before_date\n",
    "        self.max_commits       = max_commits\n",
    "        self.diff_line_width   = diff_line_width\n",
    "        self.layer_depth       = layer_depth\n",
    "        self.package_folders   = package_folders\n",
    "        self.artifact_families = artifact_families\n",
    "        self.package_prefixes = package_prefixes\n",
    "        self.log_tokens      = None\n",
    "        self.git_command     = None\n",
    "        self.parcels         = None\n",
    "\n",
    "    def _buildGitCommand(self):\n",
    "        \n",
    "        FILTERS = ''\n",
    "        if self.after_date != None:  \n",
    "            FILTERS += ' --after=' + self.after_date\n",
    "        if self.before_date != None:\n",
    "            FILTERS += ' --before=' + self.before_date\n",
    "        if self.max_commits != None:\n",
    "            FILTERS += ' -n ' + str(self.max_commits)\n",
    "\n",
    "        GIT_CMD = 'git log --date=short --format=' + GitLogStatics.FORMAT \\\n",
    "        + ' --stat --stat-width=' + str(self.diff_line_width) \\\n",
    "        + FILTERS\n",
    "        \n",
    "        return GIT_CMD\n",
    "    \n",
    "    def _generateLogTokens(self):\n",
    "        self.git_command = self._buildGitCommand()\n",
    "        process          = subprocess.Popen(self.git_command.split(), cwd=self.git_directory, stdout=subprocess.PIPE)\n",
    "        output, error    = process.communicate()\n",
    "        s                = output.decode('utf-8')\n",
    "        self.log_tokens  = s.split('\\n')\n",
    "        \n",
    "    def parse(self):\n",
    "        \n",
    "        self._generateLogTokens()\n",
    "        self.parcels = self._parcelOutCommits(self.log_tokens)\n",
    "        return self._build_raw_df(), self._build_commits_df()\n",
    "        \n",
    "    def _build_raw_df(self):\n",
    "        repo_list            = []\n",
    "        commitId_list        = []\n",
    "        subject_list         = []\n",
    "        author_list          = []\n",
    "        author_email_list    = []\n",
    "        author_date_list     = []\n",
    "        committer_list       = []\n",
    "        committer_email_list = []\n",
    "        committer_date_list  = []\n",
    "        body_list            = []\n",
    "        filename_list        = []\n",
    "        loc_list             = []\n",
    "        loc_added_list       = []\n",
    "        loc_removed_list     = []\n",
    "        loc_changed_list     = []\n",
    "        other_list           = []\n",
    "        artifact_type_list   = []\n",
    "        artifact_family_list = [] \n",
    "        layer_list           = []\n",
    "        submodule_list       = []\n",
    "        package_list         = []\n",
    "        package_type_list    = []\n",
    "        classname_list       = []\n",
    "        symbolic_link_list   = []\n",
    "\n",
    "        for p in self.parcels:\n",
    "            for f in p.files_changed:\n",
    "                \n",
    "                repo_list            .append(self.repo)\n",
    "                commitId_list        .append(p.commitId)\n",
    "                subject_list         .append(p.subject)\n",
    "                author_list          .append(p.author) \n",
    "                author_email_list    .append(p.author_email)\n",
    "                author_date_list     .append(p.author_date)\n",
    "                committer_list       .append(p.committer)\n",
    "                committer_email_list .append(p.committer_email)\n",
    "                committer_date_list  .append(p.committer_date)\n",
    "                body_list            .append(p.body)\n",
    "                filename_list        .append(f.filename)\n",
    "                loc_list             .append(f.loc)\n",
    "                loc_added_list       .append(f.loc_added)\n",
    "                loc_removed_list     .append(f.loc_removed)\n",
    "                loc_changed_list     .append(f.loc_changed)\n",
    "                other_list           .append(f.other)\n",
    "\n",
    "                fp = FilenameParser(f.filename, \n",
    "                                    layer_depth         = self.layer_depth, \n",
    "                                    package_folders     = self.package_folders,\n",
    "                                    artifact_families   = self.artifact_families,\n",
    "                                    package_prefixes    = self.package_prefixes)\n",
    "                fp.parse()\n",
    "                \n",
    "                artifact_type_list    .append(fp.artifact_type)\n",
    "                artifact_family_list  .append(fp.artifact_family)\n",
    "                layer_list            .append(fp.layer)\n",
    "                submodule_list        .append(fp.submodule)\n",
    "                package_type_list     .append(fp.package_type)\n",
    "                package_list          .append(fp.package)\n",
    "                classname_list        .append(fp.classname)\n",
    "                symbolic_link_list    .append(fp.symbolic_link)\n",
    "\n",
    "        result_dict = {'Repo': repo_list, 'CommitId(s)': commitId_list, 'Artifact Family': artifact_family_list, \n",
    "                       'Artifact Type': artifact_type_list,\n",
    "                       'Submodule': submodule_list, 'Package Type': package_type_list,\n",
    "                       'Package': package_list, 'Classname': classname_list,\n",
    "                       'Loc': loc_list, 'Loc+': loc_added_list, 'Loc-': loc_removed_list, 'Loc?': loc_changed_list,\n",
    "                       'Loc other': other_list, 'Subject': subject_list, 'Body': body_list,\n",
    "                       'Layer': layer_list, 'Symbolic Link': symbolic_link_list,'Filename': filename_list, \n",
    "                      'Author(s)': author_list, 'Author(s) e-mail': author_email_list, 'Author(s) date': author_date_list,\n",
    "                      'Committer(s)': committer_list, 'Committer(s) e-mail': committer_email_list, \n",
    "                       'Comitter(s) date': committer_date_list,\n",
    "                      }\n",
    "        df = pd.DataFrame(result_dict)\n",
    "        return df\n",
    "    \n",
    "    def _build_commits_df(self):\n",
    "        repo_list            = []\n",
    "        commitId_list        = []\n",
    "        subject_list         = []\n",
    "        author_list          = []\n",
    "        author_email_list    = []\n",
    "        author_date_list     = []\n",
    "        committer_list       = []\n",
    "        committer_email_list = []\n",
    "        committer_date_list  = []\n",
    "        body_list            = []\n",
    "        \n",
    "        files_count_list     = []\n",
    "        loc_list             = []\n",
    "        loc_added_list       = []\n",
    "        loc_removed_list     = []\n",
    "        loc_changed_list     = []\n",
    "        \n",
    "        for p in self.parcels:\n",
    "                \n",
    "            repo_list            .append(self.repo)\n",
    "            commitId_list        .append(p.commitId)\n",
    "            subject_list         .append(p.subject)\n",
    "            author_list          .append(p.author) \n",
    "            author_email_list    .append(p.author_email)\n",
    "            author_date_list     .append(p.author_date)\n",
    "            committer_list       .append(p.committer)\n",
    "            committer_email_list .append(p.committer_email)\n",
    "            committer_date_list  .append(p.committer_date)\n",
    "            body_list            .append(p.body)\n",
    "            \n",
    "            files_count_list     .append(len(p.files_changed))\n",
    "\n",
    "            loc         = 0\n",
    "            loc_added   = 0\n",
    "            loc_removed = 0\n",
    "            loc_changed = 0\n",
    "            for f in p.files_changed:\n",
    "                loc         += f.loc\n",
    "                loc_added   += f.loc_added\n",
    "                loc_removed += f.loc_removed\n",
    "                loc_changed += f.loc_changed\n",
    "                \n",
    "            loc_list             .append(loc)\n",
    "            loc_added_list       .append(loc_added)\n",
    "            loc_removed_list     .append(loc_removed)\n",
    "            loc_changed_list     .append(loc_changed)\n",
    "\n",
    "        result_dict = {'Repo': repo_list, 'CommitId(s)': commitId_list, '# files changed': files_count_list,\n",
    "                       'Loc': loc_list, 'Loc+': loc_added_list, 'Loc-': loc_removed_list, 'Loc?': loc_changed_list,\n",
    "                       'Subject': subject_list, 'Body': body_list,\n",
    "                      'Author(s)': author_list, 'Author(s) e-mail': author_email_list, 'Author(s) date': author_date_list,\n",
    "                      'Committer(s)': committer_list, 'Committer(s) e-mail': committer_email_list, \n",
    "                       'Comitter(s) date': committer_date_list,\n",
    "                      }\n",
    "        df = pd.DataFrame(result_dict)\n",
    "        return df\n",
    "        \n",
    "    def _parcelOutCommits(self, tokens):\n",
    "        cursor = 0\n",
    "        parcelled_commits= []\n",
    "        while cursor < len(tokens):\n",
    "            n = self._getNextCommitLines(tokens, cursor)\n",
    "            if n == None:\n",
    "                break # We are done, didn't find a parcel\n",
    "            parcel = CommitParcel()\n",
    "            parcel.buildUp(n[0])\n",
    "            parcelled_commits.append(parcel)\n",
    "            cursor = n[1]\n",
    "        return parcelled_commits\n",
    "\n",
    "    def _getNextCommitLines(self, tokens, cursor):\n",
    "        while tokens[cursor] != GitLogStatics.START_COMMIT_RECORD:\n",
    "            cursor += 1\n",
    "            if len(tokens) <= cursor: #No more tokens to see\n",
    "                return None\n",
    "        #Found where next commit starts\n",
    "        commit_start_idx = cursor\n",
    "\n",
    "        cursor += 1\n",
    "        #search for where commit ends\n",
    "        while tokens[cursor] != GitLogStatics.START_COMMIT_RECORD:\n",
    "            cursor += 1\n",
    "            if len(tokens) <= cursor: #No more tokens to see\n",
    "                break\n",
    "\n",
    "        commit_end_idx = cursor\n",
    "        return tokens[commit_start_idx: commit_end_idx], cursor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     4,
     34,
     53,
     93,
     109,
     131,
     158
    ]
   },
   "outputs": [],
   "source": [
    "class GitLogAggregationEngine():\n",
    "    \n",
    "    #\n",
    "    # -glogdf: a DataFrame, as built by the GitLogParser after doing a full parsing run\n",
    "    def __init__(self, raw_df, commits_df):\n",
    "        self.raw_df       = raw_df\n",
    "        self.commits_df   = commits_df\n",
    "       \n",
    "        self.artifacts_df = None\n",
    "        self.modules_df   = None\n",
    "        self.vol_df       = None\n",
    "        \n",
    "    # Saves all the dataframes in that are part of the state of 'self'\n",
    "    def save_all(self, directory):\n",
    "        '''\n",
    "        d = datetime.today()\n",
    "        date_tag = d.strftime('%y%m%d') # We prefix filenames with strings like '190628' for June 28, 2019 (date of extract)\n",
    "        if self.commits_df is not None:\n",
    "            self.commits_df.to_csv(directory + '/' + date_tag + ' commits_parsed_git_log.csv')\n",
    "        if self.raw_df is not None:\n",
    "            self.raw_df.to_csv(directory + '/' + date_tag + ' raw_parsed_git_log.csv')\n",
    "        if self.artifacts_df is not None:\n",
    "            self.artifacts_df.to_csv(directory + '/' + date_tag + ' by_artifact_parsed_git_log.csv')\n",
    "        if self.modules_df is not None:\n",
    "            self.modules_df.to_csv(directory + '/' + date_tag + ' by_module_parsed_git_log.csv')\n",
    "        if self.vol_df is not None:\n",
    "            self.vol_df.to_csv(directory + '/' + date_tag + ' by_volatility_parsed_git_log.csv')\n",
    "        '''\n",
    "        snapshot_dataset(self.commits_df,   directory, 'commits_parsed_git_log')\n",
    "        snapshot_dataset(self.raw_df,       directory, 'raw_parsed_git_log')\n",
    "        snapshot_dataset(self.artifacts_df, directory, 'by_artifact_parsed_git_log')\n",
    "        snapshot_dataset(self.modules_df,   directory, 'by_module_parsed_git_log')\n",
    "        snapshot_dataset(self.vol_df,       directory, 'by_volatility_parsed_git_log')\n",
    "    \n",
    "    def aggregateByArtifact(self):\n",
    "        \n",
    "        # DataFrame.groupby will drop rows for which the grouping dimension is null. So pad up null 'Package Type'\n",
    "        # with empty strings to avoid dropping data\n",
    "        self.raw_df['Repo'].fillna('', inplace=True)\n",
    "        self.raw_df['Filename'].fillna('', inplace=True)\n",
    "        self.raw_df['Package Type'].fillna('', inplace=True)\n",
    "                \n",
    "        artifacts_df      = self.raw_df.groupby(['Repo', 'Filename', 'Package Type']).apply(self._collapseFilenameMultiplicities)\n",
    "        artifacts_df.sort_values(by=['# commits'], ascending=False, inplace=True)\n",
    "        artifacts_df      = artifacts_df.reset_index()\n",
    "        cols              = list(artifacts_df.columns)\n",
    "        cols.remove('level_3')\n",
    "        cols              = cols[1:] + cols[:1]\n",
    "        artifacts_df      = artifacts_df[cols]\n",
    "        self.artifacts_df = artifacts_df\n",
    "        \n",
    "        return self.artifacts_df\n",
    "    \n",
    "    def aggregateByModule(self):\n",
    "        if self.artifacts_df is not None: # Optimization: re-use partial aggregation already done to file level\n",
    "            input_df = self.artifacts_df\n",
    "        else:\n",
    "            input_df = self.raw_df # Do from scratch: aggregate from each <commit, file> pair\n",
    "\n",
    "        # DataFrame.groupby will drop rows for which the grouping dimension is null. So before grouping pad up null\n",
    "        # with empty strings to avoid dropping data\n",
    "        input_df['Repo'].fillna('', inplace=True)\n",
    "        input_df['Layer'].fillna('', inplace=True)\n",
    "        input_df['Submodule'].fillna('', inplace=True)\n",
    "        input_df['Package Type'].fillna('', inplace=True)\n",
    "            \n",
    "        modules_df = input_df.groupby(['Repo', 'Layer', 'Submodule', 'Package Type']).apply(self._collapseModuleMultiplicities)\n",
    "        modules_df = modules_df.reset_index()\n",
    "        cols = list(modules_df.columns)\n",
    "        cols.remove('level_4')\n",
    "        modules_df = modules_df[cols]\n",
    "        modules_df.sort_values(by=['# commits'], ascending=False, inplace=True)\n",
    "        \n",
    "        self.modules_df = modules_df\n",
    "        \n",
    "        return self.modules_df\n",
    "    \n",
    "    # Returns a dataframe, 1 row per module, of all the modules where the changed loc exceeds the given\n",
    "    # 'loc_limit'\n",
    "    # Assumes that self.modules_df has already been computed (for example, by calling self.aggregateByModule)\n",
    "    #\n",
    "    # -loc_limit: an integer stating the lower bound for how many lines of code (loc) must have changed for\n",
    "    #             a module to quality as volatile\n",
    "    def buildVolatility_df(self, loc_limit):\n",
    "        m_df = self.modules_df\n",
    "        if m_df is None:\n",
    "            return None\n",
    "        vol_df = m_df[m_df['Loc'] > loc_limit]\n",
    "        vol_df = vol_df.sort_values(by=['Loc'], ascending=False)\n",
    "        \n",
    "        self.vol_df = vol_df\n",
    "        return self.vol_df\n",
    "\n",
    "    def _collapseFilenameMultiplicities(self, df):\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        # For any columns that have a unique value, take that value\n",
    "        for col in df.columns:            \n",
    "            # !! Ignore the columns by which we are grouping by, lest they appear twice post-aggregation and cause trouble\n",
    "            if col in ['Repo', 'Filename', 'Package Type']:\n",
    "                continue\n",
    "            if (df[col].unique().size==1):\n",
    "                result_df[col] = [df[col].iloc[0]]\n",
    "                \n",
    "        # For other columns, we must aggregate the math.\n",
    "        self._aggregateMetrics(df, result_df)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def _collapseModuleMultiplicities(self, df):\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "        result_df['Artifact Types'] = [list(df['Artifact Type'].unique())]\n",
    "        result_df['# files changed'] = [df['Filename'].unique().size]\n",
    "        result_df['files changed'] = [list(df['Filename'])]\n",
    "                \n",
    "        '''\n",
    "        Commented generic code causes an exception because df[col].unique() crashes if a value is not hashable, like a list\n",
    "        # For any columns that have a unique value, take that value\n",
    "        for col in df.columns:            \n",
    "            # !! Ignore the columns by which we are grouping by, lest they appear twice post-aggregation and cause trouble\n",
    "            if col in ['Layer', 'Submodule', 'Package Type']:\n",
    "                continue\n",
    "            if (df[col].unique().size==1):\n",
    "                result_df[col] = [df[col].iloc[0]]\n",
    "        '''\n",
    "        \n",
    "        self._aggregateMetrics(df, result_df)\n",
    "\n",
    "        return result_df\n",
    "    \n",
    "    def _aggregateMetrics(self, input_df, result_df):\n",
    "\n",
    "        author_list                   = self._mergeLists(list(input_df['Author(s)'])) #Avoid duplicates\n",
    "        commit_ids                    = self._mergeLists(list(input_df['CommitId(s)']))\n",
    "        if type(commit_ids) is not list:\n",
    "            result_df['# commits']        = [1]                \n",
    "        else:    \n",
    "            result_df['# commits']        = [len(commit_ids)]                \n",
    "\n",
    "        result_df['# authors']        = [len(author_list)]\n",
    "        result_df['Loc']              = [input_df['Loc'].sum()]\n",
    "        result_df['Loc+']             = [input_df['Loc+'].sum()]\n",
    "        result_df['Loc-']             = [input_df['Loc-'].sum()]\n",
    "        result_df['Loc?']             = [input_df['Loc?'].sum()]\n",
    "        \n",
    "        result_df['Loc other']        = [self._mergeLists(list(input_df['Loc other']))] #Avoid duplicates\n",
    "        result_df['CommitId(s)']      = [commit_ids]\n",
    "        result_df['Author(s)']        = [author_list]\n",
    "        result_df['Author(s) e-mail'] = [self._mergeLists(list(input_df['Author(s) e-mail']))] #Avoid duplicates\n",
    "        \n",
    "    # Merges a list of elements under an agreed approach of defaulting to scalars for empty or singleton lists. \n",
    "    # Thus, elements which are themselves lists are concatenated with the result, whereas elements that are not lists\n",
    "    # are treated as scalars and inserted to the resulting list.\n",
    "    # Duplicates are avoided, and if the resulting list is a singleton then the unique element of the resulting\n",
    "    # list is returned. Otherwise the resulting list is returned.\n",
    "    #\n",
    "    # -list_of_elts: a list where each element is either a string or another list\n",
    "    def _mergeLists(self, list_of_elts):\n",
    "        raw_merge = []\n",
    "        for elt in list_of_elts:\n",
    "            if type(elt)==list:\n",
    "                raw_merge.extend(elt)\n",
    "            else:\n",
    "                raw_merge.append(elt)\n",
    "        # Now eliminate duplicates\n",
    "        no_duplicates_merge = list(set(raw_merge))\n",
    "        if len(no_duplicates_merge)==1:\n",
    "            return no_duplicates_merge[0]\n",
    "        else:\n",
    "            if len(no_duplicates_merge)==0:\n",
    "                return ''\n",
    "            else:\n",
    "                return no_duplicates_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def parseLogsFromAllRepos(repos, root_data_dir, root_git_dir, after_date, max_commits,\n",
    "                         package_folders, artifact_families, package_prefixes, volatility_threshold):\n",
    "    # Per repo, accesses the GIT logs and produces and saves 4 dataframes (dataframes are not returned, just saved for \n",
    "    # later use). What is returned is a GitLogParser instance that was used to do the parsing. This GitLogParser instance\n",
    "    # contains some state that may be of interest when debugging.\n",
    "    # The 4 frames that are produced and saved are:\n",
    "    #  1) a 'raw' dataframe that has one row for each <commit, filename> pair in the GIT logs\n",
    "    #  2) an 'artifacts' dataframe that has one row per filename, aggregating the 'raw' dataframe\n",
    "    #  3) a 'modules' dataframe with one row per 'submodule', where 'submodule' is inferred during parsing time based\n",
    "    #     on the filename, by stripping the portion of the filename prior to the classname-related portions at the end of the\n",
    "    #     filename, and the first two directories in the front of the filename, considered a 'layer'\n",
    "    #  4) a 'volatility' dataframe, obtained by filtering the 'modules' dataframe based on an threshold of how many LOC changed\n",
    "    #     in the submodule in question\n",
    "    #\n",
    "    # Parameters:\n",
    "    # -repos: a list of strings, each corresponding to the name of a directory for a GIT repo.\n",
    "    # -root-dat_dir: a string corresponding to the parent directory under which the produced dataframes should be saved,\n",
    "    #                organized in subfolders given by the repo names\n",
    "    # -root_git_dir: the absolute path for a directory under which the repos exist. So GIT repos are subdirectories of this one.\n",
    "    # -after_date: a string in format MM/DD/YY, such as '06/01/18' for the 1st of June, 2018.\n",
    "    #              Used to filter log entries to only include commits after that date.\n",
    "    # -max_commits: this is either None (no effect), or an integer. When it is an integer, then it sets the maximum number\n",
    "    #          of commits to be retrieved from GIT logs. This is useful to limit the amount of information received and\n",
    "    #          speed up performance, as for example when testing this function.\n",
    "    # -package_folders, artifact_families, package_prefixes: data structures used when parsing filenames to infer what \n",
    "    #           is the classname, submodule, artifact family, etc. for each. Refer to the documentation of the\n",
    "    #           FilenameParser class.\n",
    "    # -volatility_threshold: an int, defining the level of LOC that must have changed in a module for it to be considered\n",
    "    #                        volatile\n",
    "    print('Parsing logs for repos under ' + root_git_dir + '...')\n",
    "    for repo in repos:\n",
    "        GIT_DIR = root_git_dir + '/' + repo\n",
    "        DATA_DIR = root_data_dir + '/' + repo\n",
    "        print('Processing ' + repo + '...')\n",
    "        glog   = GitLogParser(GIT_DIR, repo, after_date, max_commits=max_commits, \n",
    "                              package_folders   =package_folders,\n",
    "                              artifact_families =artifact_families,\n",
    "                              package_prefixes  =package_prefixes\n",
    "                             )\n",
    "        raw_df, commits_df  = glog.parse()\n",
    "        if len(commits_df.index)==0:\n",
    "            print('...there are no commits for ' + repo + '; Nothing will be saved')\n",
    "        else:\n",
    "            if len(raw_df.index)==0:\n",
    "                print('...there are no files in the commits for ' + repo + '; only the commits dataframe will be saved')\n",
    "                agg    = GitLogAggregationEngine(raw_df, commits_df)\n",
    "                agg.save_all(DATA_DIR)\n",
    "            else:\n",
    "                agg    = GitLogAggregationEngine(raw_df, commits_df)\n",
    "                a_df   = agg.aggregateByArtifact()\n",
    "                m_df   = agg.aggregateByModule()\n",
    "                vol_df = agg.buildVolatility_df(volatility_threshold) # TODO: make the limit a parameter, don't hardcode 10k\n",
    "                agg.save_all(DATA_DIR)\n",
    "                print('...done processing ' + repo)\n",
    "            \n",
    "    return glog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Library to enrich ingested GIT data with ticket information</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# -dates: list of date_prefix like '190626' when csv files are time-tagged, listed in order in which to search first for\n",
    "#         existance of a data file with that tag\n",
    "def loadLogDataForAllRepos(repos, root_data_dir, dates):\n",
    "    result_dict = {}\n",
    "    snapshot_types = ['raw_parsed_git_log', \n",
    "                  'by_artifact_parsed_git_log', \n",
    "                  'by_volatility_parsed_git_log', \n",
    "                  'by_module_parsed_git_log', \n",
    "                  'Repo Catalogue' ]\n",
    "    for snapshot_name in snapshot_types:\n",
    "        result_dict[snapshot_name] = {}\n",
    "        for repo in repos:\n",
    "            DATA_DIR = root_data_dir + '/' + repo\n",
    "            foundData = False\n",
    "            '''\n",
    "            for date_prefix in dates:\n",
    "                filename = DATA_DIR + '/' + date_prefix + ' ' + dt +'.csv'\n",
    "                if os.path.isfile(filename):\n",
    "                    # When reading CSV file, explicitly state dtype for columns where the act of reading the\n",
    "                    # CSV file might trigger a warning of \"mixed types\". Each time we get such a warning,\n",
    "                    # add one more column here where we specify the dtype for that column so warning is avoided\n",
    "                    df                    = pd.read_csv(filename, dtype={'Body':str, \n",
    "                                                                         'Symbolic Link':str})\n",
    "                    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "                    result_dict[dt][repo] = df\n",
    "                    foundData             = True\n",
    "                    break # Found a date for which we had data, so no need to look at more dates\n",
    "            '''\n",
    "            df = load_dataset_snapshot(DATA_DIR, dates, snapshot_name)\n",
    "            #if not foundData:\n",
    "            if df is None:\n",
    "                print('*** WARNING: FOUND NO \\'' + snapshot_name + '\\' DATA FOR REPO \\'' + repo + '\\'')\n",
    "            else:\n",
    "                result_dict[snapshot_name][repo] = df\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def mergeDataframesAcrossRepos(dataframe_dict):\n",
    "    repos = dataframe_dict.keys()\n",
    "    df_list = []\n",
    "    for repo in repos:\n",
    "        df = dataframe_dict[repo]\n",
    "        df['Repo'] = repo\n",
    "        df_list.append(df)\n",
    "    merged_df = pd.concat(df_list, sort=True) #sort=True set to avoid a warning when non-concat axis don't align\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     22,
     97
    ]
   },
   "outputs": [],
   "source": [
    "# Returns two arrays of strings. First array is th ticket ids,  such as: ['FBP-48732', 'FBE 4.2.6.1']\n",
    "# Second array is the ticket family, such as ['FBP', 'FBE']\n",
    "#\n",
    "# -bad_families: an array of string. Used to flag strings that are not valid ticket families, but which the algorithm\n",
    "#               can't help itself to think it is the family token of a ticket id. For example, consider this\n",
    "#               commit subject: \n",
    "#                                     '[CMFBE-1563]-Updating scripts for 5315 HF1 builds'\n",
    "# \n",
    "#               The algorithm will think there are two ticket ids ('CMFBE-1563' and 'for 5315'), but only the first\n",
    "#               is valid. So to help the algorithm in a case like this, include the string 'for' in the 'bad_families'\n",
    "#               list parameter.\n",
    "#\n",
    "# -treat_as_equal_families: dictionary, mapping strings to strings. It is used to replace a family for a user\n",
    "#               ticket when the family is a tweak on what the family should really be. For example, consider this\n",
    "#               commit subject:\n",
    "#\n",
    "#                                   'FBE PaymentsFBPY-4087 User Exit in Inward Payments processing'\n",
    "#\n",
    "#               The algorithm will think that 'PaymentsFBPY' is the family for ticket 'PaymentsFBPY-4087', when\n",
    "#               in reality the ticket should be 'FBPY-4087' in family 'FBPY'. To hit it on the algorithm that this\n",
    "#               is what it should be doing, add an entry in treat_as_equal_families where 'PaymentsFBPY' is the key and\n",
    "#               'FBPY' is the value\n",
    "def extractTickets(subject, bad_families, treat_as_equal_families):\n",
    "    END_DELIM     = '[\\]\\s,-:]+'\n",
    "    START_DELIM   = '[^\\[\\s,/{]+'\n",
    "    \n",
    "    SPACES        = '\\s*'\n",
    "    HYPHEN        = SPACES + '-' + SPACES\n",
    "    COLON         = SPACES + ':' + SPACES\n",
    "    MIDDLE_DELIM  = '[:\\s-]+'\n",
    "    #MIDDLE_DELIM  = HYPHEN + '|' + COLON + '|' + SPACES\n",
    "    \n",
    "    REGEX = '(' + START_DELIM + '[a-zA-Z]{2,6}' + MIDDLE_DELIM + '[.0-9]{2,12})' + END_DELIM\n",
    "    \n",
    "    tentative_tickets = re.findall(REGEX, subject)\n",
    "    \n",
    "    # First need to loop through the tickets to 'clean them up' Sometimes the algorithm above picks up an\n",
    "    # alleged 'ticket' that is a big string, bigger than the actual ticket. So we loop through the tickets to\n",
    "    # see if they contain an 'inner ticket', in which case we consider them to be real. As an example, if the\n",
    "    # subject is \n",
    "    #             '[FBP-46915]FBP-46915-multi-entity-entity-based-permission'\n",
    "    #\n",
    "    # the algorithm will think that the ticket is 'FBP-46915]FBP-46915'. The following loop will clean this\n",
    "    # up by replacing the ticket with the inner ticket 'FBP-46915'\n",
    "    unwrapped_tickets = []\n",
    "    for ticket in tentative_tickets:\n",
    "        INNER_REGEX = '[a-zA-Z]+-[0-9]+'\n",
    "        inners = re.findall(INNER_REGEX, ticket)\n",
    "        if len(inners) > 0 and len(inners[0]) < len(ticket): # Found a smaller 'inner ticket', so must be the real one\n",
    "            unwrapped_tickets.append(inners[0])\n",
    "        else:\n",
    "            unwrapped_tickets.append(ticket)\n",
    "   \n",
    "    validated_tickets = []\n",
    "    families = []\n",
    "    for ticket in unwrapped_tickets:\n",
    "        family = _extractTicketFamily(ticket)\n",
    "        \n",
    "        if family in bad_families: #this is not a real ticket, we were fooled. Refer to above documentation\n",
    "            continue # Ignore this ticket, it is bad\n",
    "\n",
    "        if family in treat_as_equal_families: # this family is a tweak of the real family, so fix that\n",
    "            corrected_family = treat_as_equal_families[family] \n",
    "            MAX_REPLACE = 1\n",
    "            corrected_ticket  = ticket.replace(family, corrected_family, MAX_REPLACE)\n",
    "\n",
    "            ticket    = corrected_ticket\n",
    "            family   = corrected_family\n",
    "\n",
    "                \n",
    "        if family == '':\n",
    "            # Didn't get a good family, and perhaps we are in a spurious case. If we already added this row\n",
    "            # then we can ignore this ticket\n",
    "            if len(validated_tickets) > 0: # Have a prior good ticket that included this row already, so skip spurious\n",
    "                continue                \n",
    "                \n",
    "        # There are cases where the above will result in spurious tickets. For example, if the subject is\n",
    "        #\n",
    "        #   '[FBLE-9658]:Yoma - 5.3.1 - Lending Non-Paged Queries.',\n",
    "        #\n",
    "        # then the algorithm will think that 'FBLE-9658]:Yoma - 5.3.1' is the ticket and that 'FBLE-9658]' is the family.\n",
    "        # To fix this, check if the alleged 'family' is really the ticket\n",
    "        #\n",
    "        suspected_tickets = re.findall(REGEX, family)\n",
    "        if len(suspected_tickets) == 1: # The suspect is guilty as charged\n",
    "            corrected_ticket  = suspected_tickets[0]\n",
    "            corrected_family = _extractTicketFamily(corrected_ticket)\n",
    "            families          .append(corrected_family)\n",
    "            validated_tickets .append(corrected_ticket)\n",
    "            \n",
    "        else:\n",
    "            families          .append(family)\n",
    "            validated_tickets .append(ticket)\n",
    "            \n",
    "    \n",
    "    return validated_tickets, families\n",
    "\n",
    "def _extractTicketFamily(ticket):\n",
    "        #Try different middle delimeters, until success, or leave family not set\n",
    "        tokens = ticket.strip().split('-')\n",
    "        if len(tokens)==2:\n",
    "            return tokens[0].strip()\n",
    "        tokens = ticket.strip().split(':')\n",
    "        if len(tokens)==2:\n",
    "            return tokens[0].strip()\n",
    "        tokens = ticket.strip().split(' ')\n",
    "        if len(tokens)==2:\n",
    "            return tokens[0].strip()\n",
    "        \n",
    "        # If we get this far there was no match.\n",
    "        return ''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a dataframe. Input is supposed to be the raw dataframe produced by GitLogParser.parse\n",
    "# The 'primary key' is the triple of columns <Filename, commitId, ticket>\n",
    "#\n",
    "# -bad_families, treat_as_equal_families: these parameters are simply passed to the extractTickets function.\n",
    "#            Refer to that function's documentation for information and examples on how to set them.\n",
    "def build_ticket_df(raw_df, bad_families, treat_as_equal_families):\n",
    "    \n",
    "    input_columns = list(raw_df.columns)\n",
    "    \n",
    "    result_dict = {}\n",
    "    for col in input_columns:\n",
    "        result_dict[col] = []\n",
    "        \n",
    "    ticket_list        = []\n",
    "    ticket_family_list = []\n",
    "        \n",
    "    for index, row in raw_df.iterrows():\n",
    "        stories, stories_family = extractTickets(row['Subject'],  bad_families, treat_as_equal_families)\n",
    "        \n",
    "        found_valid_story = False\n",
    "        for storyidx in range(len(stories)):\n",
    "            story = stories[storyidx]\n",
    "            family = stories_family[storyidx]\n",
    "                        \n",
    "            # Normal case - add a row per ticket\n",
    "            for col in input_columns:\n",
    "                result_dict[col].append(row[col])\n",
    "            ticket_list         .append(story)\n",
    "            ticket_family_list  .append(family)\n",
    "            found_valid_story = True\n",
    "                \n",
    "        #Boundary case - add a single row with no ticket to avoid losing this row \n",
    "        if found_valid_story==False:\n",
    "            for col in input_columns:\n",
    "                result_dict[col].append(row[col])\n",
    "            ticket_list         .append(None)\n",
    "            ticket_family_list  .append(None)\n",
    "\n",
    "                \n",
    "    result_dict['Ticket(s)']         = ticket_list\n",
    "    result_dict['Ticket(s) Family']  = ticket_family_list\n",
    "\n",
    "    df = pd.DataFrame(result_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Filter tickets to those for a particular release</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a dictionary, where for the keys are the various Jira extract names and the values are a list of the Jira tickets\n",
    "# identifiers (strings like 'IBF_17223')\n",
    "#\n",
    "# -root_data_dir: a string, for the root directory where data lives, as an absolute path\n",
    "# -jira_extracts_folder: a string, for the folder under 'root_data_dir' where the Jira extract text files reside.\n",
    "# -jira_extracts: list of the names of the Jira extract text files to be parsed.\n",
    "def _parseJiraExtracts(root_data_dir, jira_extracts_folder, jira__extracts):\n",
    "    result_dict = {}\n",
    "    \n",
    "    for extract_filename in jira__extracts:\n",
    "        text_file = open(root_data_dir + '/' + jira_extracts_folder + '/' + extract_filename,'r')\n",
    "        lines = text_file.readlines()\n",
    "        text_file.close()\n",
    "        # Should be 5 lines:\n",
    "        #     lines[0] has the Jira URL for the dashboard. \n",
    "        #     lines[1] has the number of user stories. This is manually entered from the dashboard by author of extract file.\n",
    "        #              This is used to reconcile that we correctly parse the extract and don't miss any Jira tickets.\n",
    "        #     lines[2] has  different Jira URL that is reached from the dashboard, by selecting the hypelink in the dashboard\n",
    "        #              that has the total number of closed tickets/stories\n",
    "        #     lines[3] is empty, for readability so prior lines are segregated from the blob of text that follows\n",
    "        #     lines[4] is the 'extract' from the web page whose URL is in lines[2]. This extract was obtained by doing\n",
    "        #              'view source' on the web page and copying-and-pasting a section that is delineated by square brackes\n",
    "        #              which corresponds to the list of closed Jira tickets displayed on the webpage'\n",
    "        #              It is a long string that starts like\n",
    "        #\n",
    "        #                   '[&quot;IBF-17223&quot;,&quot;IBF-17222&quot; ....' \n",
    "        #\n",
    "        #              and finishes like \n",
    "        #\n",
    "        #                   '... &quot;FBBT-19990&quot;,&quot;FBBT-19970&quot;]'\n",
    "        #        \n",
    "        # Extract the user stories and reconcile. \n",
    "        REGEX = '[a-zA-Z]+-[0-9]+'\n",
    "        jira_tickets = re.findall(REGEX, lines[4])\n",
    "        expected_number_of_tickets = re.findall('[0-9]+', lines[1])\n",
    "        \n",
    "        # Test we parsed the expected number of ticketss\n",
    "        assert(len(jira_tickets) == int(expected_number_of_tickets[0]))\n",
    "        result_dict[extract_filename] = jira_tickets\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "# Returns a dataframe whose primary key is the triple <commitId, ticketId, filename>, corresponding to all the \n",
    "# commits and tickets for a given release. This is done by filtering the 'global_tickets_df' input to only those\n",
    "# tickets that are pertinent to a release, as identified from the jira extract text files provided as inputs: these\n",
    "# files contain the tickets that apply to a the the release of interest.\n",
    "#\n",
    "# -global_tickets_df: a dataframe built from the function 'build_ticket_df', i.e., it has all the tickets for all the\n",
    "#                    work present in the data extracted from GIT logs in the current global context of analysis\n",
    "# -root_data_dir: a string, for the root directory where data lives, as an absolute path\n",
    "# -jira_extracts_folder: a string, for the folder under 'root_data_dir' where the Jira extract text files reside.\n",
    "# -jira_extracts: list of the names of the Jira extract text files to be parsed.\n",
    "def build_release_df(global_tickets_df, root_data_dir, jira_extracts_folder, jira_story_extracts, jira_bug_extracts):\n",
    "    all_extracts = list(set(jira_story_extracts).union(set(jira_bug_extracts)))\n",
    "    jira_tickets_dict = _parseJiraExtracts(root_data_dir, jira_extracts_folder, all_extracts)\n",
    "    all_jira_tickets = []\n",
    "    for key in jira_tickets_dict.keys():\n",
    "        all_jira_tickets.extend(jira_tickets_dict[key]) \n",
    "    release_df = global_tickets_df[global_tickets_df['Ticket(s)'].isin(all_jira_tickets)]\n",
    "    release_df = release_df.reset_index(drop=True)\n",
    "\n",
    "    # Now lok at each row in 'release-df', and figure out if the 'Ticket(s)' is a story or a bug, to\n",
    "    # set up the 'Ticket Type column'. Logic requires we first prepare the list of tickets that are either\n",
    "    # stories or bugs, to then be able to search in them.\n",
    "    release_df.loc[:, 'Ticket Type']  = None\n",
    "    \n",
    "    stories = []\n",
    "    for extract_key in jira_story_extracts:\n",
    "        stories.extend(jira_tickets_dict[extract_key])\n",
    "    bugs = []\n",
    "    for extract_key in jira_bug_extracts:\n",
    "        bugs.extend(jira_tickets_dict[extract_key])\n",
    "\n",
    "    for index, row in release_df.iterrows():\n",
    "        ticket = row['Ticket(s)']\n",
    "        if ticket in stories:\n",
    "            release_df.loc[index, 'Ticket Type'] = 'User Story'\n",
    "        else:\n",
    "            if ticket in bugs:\n",
    "                release_df.loc[index, 'Ticket Type'] = 'Defect'\n",
    "    return release_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>[DEPRECATED - SAVE THE USEFUL SOMEWHERE] Library to analyze data</h1>\n",
    "These routines won't access GIT. Insteady they will rely on loading CSV files previously saved by the other library that does access GIT and parses the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
