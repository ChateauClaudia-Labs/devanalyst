{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\businessObjects.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\statics.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\simulationModels.ipynb\n"
     ]
    }
   ],
   "source": [
    "from devanalyst.simulation.businessObjects import ReleaseCycleContext\n",
    "\n",
    "from devanalyst.simulation.simulationModels import ModelsConfig, DefaultCostModel, GreedyAllocationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\test_utils\\test_utils.ipynb\n",
      "importing Jupyter notebook from c:\\users\\aleja\\documents\\code\\chateauclaudia-labs\\devanalyst\\devanalyst\\simulation\\generateTimecards.ipynb\n"
     ]
    }
   ],
   "source": [
    "import devanalyst.test_utils.test_utils as tu_\n",
    "from devanalyst.test_utils.test_utils import ExpectedOutputCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>test_FOO: template for tests that produce a DataFrame output </h1>\n",
    "<li> Copy to create a new test, and replace 'FOO' by the name of your test.\n",
    "<li> Implement test logic in test_FOO()\n",
    "<li> To create expected output once test logic is producing right output, temporarily uncomment the call to createExpectedOutput, call it, and comment it again. This saves expected output as a CSV file so subsequent runs of the test can load it and verify that the test still produces the same output as before.\n",
    "<li> Uncomment lines that load expected output and remove the dummy line next to it creating a dummy expected output.\n",
    "<li> Display the outputs for convenience in debugging when test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Implement test logic, and run it\n",
    "\n",
    "#Test logic\n",
    "def test_FOO():\n",
    "    output = {}\n",
    "    RELEASE_DURATION = 125\n",
    "    SPRINT_DURATION = 10\n",
    "\n",
    "    # Configure models\n",
    "    modelsConfig = ModelsConfig([DefaultCostModel()], [], GreedyAllocationModel()) \n",
    "    modelsConfig.random.reset(271)\n",
    "\n",
    "    teams_df, stories_df, globalRepo = tu_.initTestData(tu_.DEV_DF, tu_.PM_DF, \\\n",
    "                                                        RELEASE_DURATION, SPRINT_DURATION, modelsConfig)\n",
    "    modelsConfig.globalRepo = globalRepo\n",
    "    TEAM_ID = 'Team A'\n",
    "    SPRINT = 1\n",
    "    modelsConfig.context = ReleaseCycleContext(TEAM_ID, SPRINT, SPRINT_DURATION)\n",
    "    \n",
    "    date1 = pd.Timestamp(2019,3,28)\n",
    "    pubs1 = ['magazine', 'novel', 'textbook']\n",
    "    date2 = pd.Timestamp(2019, 1,27)\n",
    "    pubs2 = ['NY Times', 'The Atlantic', 'WSJ']\n",
    "    dummyA_df = pd.DataFrame({'A': [45.123456789, 34.123456789], 'Date': [date1, date2], 'Publications': [pubs1, pubs2]})\n",
    "    dummyB_df = pd.DataFrame({'B': [2333.123456789, 3984.123456789]})\n",
    "    \n",
    "    output['Dummy_A'] = dummyA_df\n",
    "    output['Dummy_B'] = dummyB_df\n",
    "\n",
    "    return output\n",
    "\n",
    "# Run the test\n",
    "test_FOO_ACTUAL = test_FOO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to update expected output to match the actual one\n",
    "\n",
    "# Helper method\n",
    "def create_FOO_EXPECTED():\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_A'],    'test_FOO.Dummy_A')\n",
    "    tu_.createExpectedOutput(test_FOO_ACTUAL['Dummy_B'],    'test_FOO.Dummy_B')\n",
    "\n",
    "# Uncomment to update expected output to match the actual one, and then put the comment back\n",
    "#create_FOO_EXPECTED()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load expected output, update the EXPECTED and ACTUAL dictionaries, and check test is OK\n",
    "list_cols_A = ['Publications'] # Lists are loaded as strings, so require special processing on load\n",
    "list_cols_B = []\n",
    "test_FOO_EXPECTED = {}\n",
    "\n",
    "test_FOO_EXPECTED['Dummy_A']      = tu_.loadExpectedOutput('test_FOO.Dummy_A', list_cols_A)\n",
    "test_FOO_EXPECTED['Dummy_B']      = tu_.loadExpectedOutput('test_FOO.Dummy_B', list_cols_B)\n",
    "\n",
    "# Rounding inaccuracies in saving and loading CSV will create an artificial mismatch between ACTUAL and EXPECTED\n",
    "# So round EXPECTED and ACTUAL to 6 decimal places for sensitive fields (any float)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['A'],\n",
    "                                        ['Dummy_A'],\n",
    "                                        test_FOO_EXPECTED,\n",
    "                                        test_FOO_ACTUAL)\n",
    "ExpectedOutputCleaner.cleanRoundingNoise(['B'],\n",
    "                                        ['Dummy_B'],\n",
    "                                        test_FOO_EXPECTED,\n",
    "                                        test_FOO_ACTUAL)\n",
    "\n",
    "\n",
    "# Dates are loaded as strings, not pd.Timestamps, so to avoid spurious mismatches between ACTUAL (which \n",
    "# represents dates as pd.Timestamps) and EXPECTED, convert the EXPECTED dates into pd.Timestamps\n",
    "ExpectedOutputCleaner.standardizeDates(['Date'],\n",
    "                                      ['Dummy_A'],\n",
    "                                      test_FOO_EXPECTED)\n",
    "\n",
    "tu_.EXPECTED['test_FOO.Dummy_A']        = test_FOO_EXPECTED['Dummy_A']\n",
    "tu_.EXPECTED['test_FOO.Dummy_B']        = test_FOO_EXPECTED['Dummy_B']\n",
    "\n",
    "tu_.ACTUAL['test_FOO.Dummy_A']          = test_FOO_ACTUAL['Dummy_A']\n",
    "tu_.ACTUAL['test_FOO.Dummy_B']          = test_FOO_ACTUAL['Dummy_B']\n",
    "\n",
    "tu_.testOK('test_FOO.Dummy_A'), \\\n",
    "tu_.testOK('test_FOO.Dummy_B'), \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_ACTUAL['Dummy_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_FOO_EXPECTED['Dummy_B']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Run all the tests in this notebook</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run all the tests defined in this notebook\n",
    "#unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
